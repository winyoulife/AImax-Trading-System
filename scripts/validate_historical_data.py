#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MAXæ­·å²è³‡æ–™åº«å®Œæ•´æ€§å’Œæ•¸æ“šå“è³ªé©—è­‰è…³æœ¬
ç¢ºä¿å¯¦ç›¤æ¸¬è©¦å‰æ•¸æ“šå®Œå…¨å¯é 
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import asyncio
import logging
from typing import Dict, List, Tuple, Any

from data.historical_data_manager import create_historical_manager
from data.technical_indicators import TechnicalIndicatorCalculator

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class HistoricalDataValidator:
    """æ­·å²æ•¸æ“šé©—è­‰å™¨"""
    
    def __init__(self, db_path: str = "data/market_history.db"):
        self.db_path = db_path
        self.manager = create_historical_manager(db_path)
        self.tech_indicators = TechnicalIndicatorCalculator()
        
    async def run_full_validation(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„æ•¸æ“šé©—è­‰"""
        print("ğŸ” é–‹å§‹MAXæ­·å²è³‡æ–™åº«å®Œæ•´æ€§é©—è­‰...")
        
        validation_results = {
            'database_info': {},
            'data_integrity': {},
            'data_quality': {},
            'continuity_check': {},
            'technical_indicators': {},
            'auto_update_test': {},
            'summary': {}
        }
        
        try:
            # 1. æ•¸æ“šåº«åŸºæœ¬ä¿¡æ¯
            validation_results['database_info'] = await self._check_database_info()
            
            # 2. æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥
            validation_results['data_integrity'] = await self._check_data_integrity()
            
            # 3. æ•¸æ“šå“è³ªæª¢æŸ¥
            validation_results['data_quality'] = await self._check_data_quality()
            
            # 4. æ•¸æ“šé€£çºŒæ€§æª¢æŸ¥
            validation_results['continuity_check'] = await self._check_data_continuity()
            
            # 5. æŠ€è¡“æŒ‡æ¨™è¨ˆç®—æ¸¬è©¦
            validation_results['technical_indicators'] = await self._test_technical_indicators()
            
            # 6. è‡ªå‹•æ›´æ–°åŠŸèƒ½æ¸¬è©¦
            validation_results['auto_update_test'] = await self._test_auto_update()
            
            # 7. ç”Ÿæˆç¸½çµå ±å‘Š
            validation_results['summary'] = self._generate_summary(validation_results)
            
            return validation_results
            
        except Exception as e:
            logger.error(f"âŒ é©—è­‰éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            validation_results['error'] = str(e)
            return validation_results
    
    async def _check_database_info(self) -> Dict[str, Any]:
        """æª¢æŸ¥æ•¸æ“šåº«åŸºæœ¬ä¿¡æ¯"""
        print("\nğŸ“Š æª¢æŸ¥æ•¸æ“šåº«åŸºæœ¬ä¿¡æ¯...")
        
        try:
            stats = self.manager.get_data_statistics("btctwd")
            
            info = {
                'database_path': stats.get('database_path', ''),
                'database_size_mb': stats.get('database_size_mb', 0),
                'timeframe_stats': stats.get('timeframe_stats', {}),
                'update_stats': stats.get('update_stats', {}),
                'total_records': sum(tf['count'] for tf in stats.get('timeframe_stats', {}).values())
            }
            
            print(f"   ğŸ“ æ•¸æ“šåº«è·¯å¾‘: {info['database_path']}")
            print(f"   ğŸ’¾ æ•¸æ“šåº«å¤§å°: {info['database_size_mb']:.2f} MB")
            print(f"   ğŸ“ˆ ç¸½è¨˜éŒ„æ•¸: {info['total_records']}")
            
            for timeframe, tf_stats in info['timeframe_stats'].items():
                print(f"   â° {timeframe}: {tf_stats['count']}æ¢è¨˜éŒ„ï¼Œè¦†è“‹{tf_stats['coverage_hours']:.1f}å°æ™‚")
            
            return info
            
        except Exception as e:
            logger.error(f"âŒ æª¢æŸ¥æ•¸æ“šåº«ä¿¡æ¯å¤±æ•—: {e}")
            return {'error': str(e)}
    
    async def _check_data_integrity(self) -> Dict[str, Any]:
        """æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§"""
        print("\nğŸ” æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§...")
        
        integrity_results = {}
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # æª¢æŸ¥é‡è¤‡è¨˜éŒ„
                cursor.execute('''
                    SELECT market, timeframe, timestamp, COUNT(*) as count
                    FROM klines 
                    GROUP BY market, timeframe, timestamp
                    HAVING COUNT(*) > 1
                ''')
                duplicates = cursor.fetchall()
                integrity_results['duplicates'] = len(duplicates)
                
                # æª¢æŸ¥NULLå€¼
                cursor.execute('''
                    SELECT COUNT(*) FROM klines 
                    WHERE open IS NULL OR high IS NULL OR low IS NULL 
                       OR close IS NULL OR volume IS NULL
                ''')
                null_count = cursor.fetchone()[0]
                integrity_results['null_values'] = null_count
                
                # æª¢æŸ¥ç•°å¸¸åƒ¹æ ¼æ•¸æ“š
                cursor.execute('''
                    SELECT COUNT(*) FROM klines 
                    WHERE high < low OR open < 0 OR close < 0 
                       OR high < 0 OR low < 0 OR volume < 0
                ''')
                invalid_prices = cursor.fetchone()[0]
                integrity_results['invalid_prices'] = invalid_prices
                
                # æª¢æŸ¥åƒ¹æ ¼åˆç†æ€§ (æ¥µç«¯å€¼)
                for timeframe in ['1m', '5m', '1h']:
                    cursor.execute('''
                        SELECT MIN(close), MAX(close), AVG(close), 
                               MIN(volume), MAX(volume), AVG(volume)
                        FROM klines 
                        WHERE market = 'btctwd' AND timeframe = ?
                    ''', (timeframe,))
                    
                    result = cursor.fetchone()
                    if result:
                        min_price, max_price, avg_price, min_vol, max_vol, avg_vol = result
                        integrity_results[f'{timeframe}_price_range'] = {
                            'min': min_price, 'max': max_price, 'avg': avg_price,
                            'price_volatility': (max_price - min_price) / avg_price if avg_price > 0 else 0
                        }
                        integrity_results[f'{timeframe}_volume_range'] = {
                            'min': min_vol, 'max': max_vol, 'avg': avg_vol
                        }
                
                print(f"   ğŸ”„ é‡è¤‡è¨˜éŒ„: {integrity_results['duplicates']}æ¢")
                print(f"   âŒ NULLå€¼: {integrity_results['null_values']}æ¢")
                print(f"   âš ï¸ ç•°å¸¸åƒ¹æ ¼: {integrity_results['invalid_prices']}æ¢")
                
                return integrity_results
                
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥å¤±æ•—: {e}")
            return {'error': str(e)}
    
    async def _check_data_quality(self) -> Dict[str, Any]:
        """æª¢æŸ¥æ•¸æ“šå“è³ª"""
        print("\nğŸ“ˆ æª¢æŸ¥æ•¸æ“šå“è³ª...")
        
        quality_results = {}
        
        try:
            for timeframe in ['1m', '5m', '1h']:
                df = self.manager.get_historical_data("btctwd", timeframe, 100)
                
                if df is not None and not df.empty:
                    # è¨ˆç®—åŸºæœ¬çµ±è¨ˆ
                    quality_results[f'{timeframe}_stats'] = {
                        'count': len(df),
                        'price_std': df['close'].std(),
                        'volume_std': df['volume'].std(),
                        'price_mean': df['close'].mean(),
                        'volume_mean': df['volume'].mean()
                    }
                    
                    # æª¢æŸ¥åƒ¹æ ¼è·³èº
                    df['price_change'] = df['close'].pct_change()
                    extreme_changes = (abs(df['price_change']) > 0.1).sum()  # 10%ä»¥ä¸Šè®ŠåŒ–
                    quality_results[f'{timeframe}_extreme_changes'] = extreme_changes
                    
                    # æª¢æŸ¥æˆäº¤é‡ç•°å¸¸
                    volume_median = df['volume'].median()
                    volume_outliers = (df['volume'] > volume_median * 10).sum()
                    quality_results[f'{timeframe}_volume_outliers'] = volume_outliers
                    
                    print(f"   â° {timeframe}: {len(df)}æ¢è¨˜éŒ„ï¼Œæ¥µç«¯è®ŠåŒ–{extreme_changes}æ¬¡ï¼Œæˆäº¤é‡ç•°å¸¸{volume_outliers}æ¬¡")
                
            return quality_results
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šå“è³ªæª¢æŸ¥å¤±æ•—: {e}")
            return {'error': str(e)}
    
    async def _check_data_continuity(self) -> Dict[str, Any]:
        """æª¢æŸ¥æ•¸æ“šé€£çºŒæ€§"""
        print("\nğŸ”— æª¢æŸ¥æ•¸æ“šé€£çºŒæ€§...")
        
        continuity_results = {}
        
        try:
            for timeframe in ['1m', '5m', '1h']:
                df = self.manager.get_historical_data("btctwd", timeframe, 500)
                
                if df is not None and not df.empty:
                    # è¨ˆç®—æ™‚é–“é–“éš”
                    df = df.sort_values('timestamp')
                    time_diffs = df['timestamp'].diff()
                    
                    # é æœŸé–“éš”
                    expected_interval = {'1m': 60, '5m': 300, '1h': 3600}[timeframe]
                    expected_timedelta = timedelta(seconds=expected_interval)
                    
                    # æ‰¾å‡ºé–“éš”ç•°å¸¸çš„è¨˜éŒ„
                    normal_intervals = (time_diffs == expected_timedelta).sum()
                    total_intervals = len(time_diffs) - 1  # æ’é™¤ç¬¬ä¸€å€‹NaN
                    
                    if total_intervals > 0:
                        continuity_rate = normal_intervals / total_intervals
                        gaps = total_intervals - normal_intervals
                        
                        continuity_results[f'{timeframe}_continuity'] = {
                            'total_intervals': total_intervals,
                            'normal_intervals': normal_intervals,
                            'gaps': gaps,
                            'continuity_rate': continuity_rate,
                            'time_range': {
                                'start': df['timestamp'].min().isoformat(),
                                'end': df['timestamp'].max().isoformat()
                            }
                        }
                        
                        print(f"   â° {timeframe}: é€£çºŒæ€§{continuity_rate:.1%}ï¼Œé–“éš”ç•°å¸¸{gaps}æ¬¡")
                
            return continuity_results
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šé€£çºŒæ€§æª¢æŸ¥å¤±æ•—: {e}")
            return {'error': str(e)}
    
    async def _test_technical_indicators(self) -> Dict[str, Any]:
        """æ¸¬è©¦æŠ€è¡“æŒ‡æ¨™è¨ˆç®—"""
        print("\nğŸ“Š æ¸¬è©¦æŠ€è¡“æŒ‡æ¨™è¨ˆç®—...")
        
        indicator_results = {}
        
        try:
            # ç²å–å¤šæ™‚é–“æ¡†æ¶æ¸¬è©¦æ•¸æ“š
            data_1m = self.manager.get_historical_data("btctwd", "1m", 100)
            data_5m = self.manager.get_historical_data("btctwd", "5m", 100)
            data_1h = self.manager.get_historical_data("btctwd", "1h", 100)
            
            klines_data = {}
            if data_1m is not None and not data_1m.empty:
                klines_data['1m'] = data_1m
            if data_5m is not None and not data_5m.empty:
                klines_data['5m'] = data_5m
            if data_1h is not None and not data_1h.empty:
                klines_data['1h'] = data_1h
            
            if klines_data:
                # æ¸¬è©¦æŠ€è¡“æŒ‡æ¨™è¨ˆç®—
                indicators = self.tech_indicators.calculate_comprehensive_indicators(klines_data)
                
                indicator_results['total_indicators'] = len(indicators)
                indicator_results['calculated_indicators'] = list(indicators.keys())
                
                # æª¢æŸ¥é—œéµæŒ‡æ¨™
                key_indicators = ['short_rsi', 'medium_macd_signal', 'long_sma_20', 'trend_consistency']
                successful_key = 0
                
                for key_indicator in key_indicators:
                    if key_indicator in indicators:
                        successful_key += 1
                        print(f"   âœ… {key_indicator}: {indicators[key_indicator]}")
                    else:
                        print(f"   âŒ {key_indicator}: æœªè¨ˆç®—")
                
                indicator_results['key_indicators_success'] = successful_key
                indicator_results['key_indicators_total'] = len(key_indicators)
                indicator_results['success_rate'] = successful_key / len(key_indicators)
                
                print(f"   ğŸ“Š ç¸½è¨ˆç®—æŒ‡æ¨™: {len(indicators)}å€‹")
                print(f"   ğŸ¯ é—œéµæŒ‡æ¨™æˆåŠŸç‡: {successful_key}/{len(key_indicators)} ({indicator_results['success_rate']:.1%})")
                
            else:
                indicator_results['error'] = "ç„¡æ³•ç²å–æ¸¬è©¦æ•¸æ“š"
                
            return indicator_results
            
        except Exception as e:
            logger.error(f"âŒ æŠ€è¡“æŒ‡æ¨™æ¸¬è©¦å¤±æ•—: {e}")
            return {'error': str(e)}
    
    async def _test_auto_update(self) -> Dict[str, Any]:
        """æ¸¬è©¦è‡ªå‹•æ›´æ–°åŠŸèƒ½"""
        print("\nğŸ”„ æ¸¬è©¦è‡ªå‹•æ›´æ–°åŠŸèƒ½...")
        
        update_results = {}
        
        try:
            # æ¸¬è©¦æ•¸æ“šæ–°é®®åº¦æª¢æŸ¥
            print("   ğŸ” æ¸¬è©¦æ•¸æ“šæ–°é®®åº¦æª¢æŸ¥...")
            
            for timeframe in ['1m', '5m', '1h']:
                needs_update, reason = self.manager._check_data_freshness("btctwd", timeframe)
                update_results[f'{timeframe}_freshness'] = {
                    'needs_update': needs_update,
                    'reason': reason
                }
                print(f"     â° {timeframe}: {'éœ€è¦æ›´æ–°' if needs_update else 'æ•¸æ“šæ–°é®®'} - {reason}")
            
            # æ¸¬è©¦æ•¸æ“šç¢ºä¿åŠŸèƒ½
            print("   ğŸ“¥ æ¸¬è©¦æ•¸æ“šç¢ºä¿åŠŸèƒ½...")
            ensure_results = await self.manager.ensure_historical_data("btctwd", ['1m', '5m', '1h'])
            
            update_results['ensure_results'] = ensure_results
            successful_updates = sum(1 for success in ensure_results.values() if success)
            
            print(f"   âœ… æˆåŠŸæ›´æ–°: {successful_updates}/3 å€‹æ™‚é–“æ¡†æ¶")
            
            return update_results
            
        except Exception as e:
            logger.error(f"âŒ è‡ªå‹•æ›´æ–°æ¸¬è©¦å¤±æ•—: {e}")
            return {'error': str(e)}
    
    def _generate_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆé©—è­‰ç¸½çµå ±å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆé©—è­‰ç¸½çµå ±å‘Š...")
        
        summary = {
            'overall_status': 'PASS',
            'critical_issues': [],
            'warnings': [],
            'recommendations': [],
            'data_readiness': True
        }
        
        try:
            # æª¢æŸ¥é—œéµå•é¡Œ
            integrity = results.get('data_integrity', {})
            
            if integrity.get('duplicates', 0) > 0:
                summary['critical_issues'].append(f"ç™¼ç¾{integrity['duplicates']}æ¢é‡è¤‡è¨˜éŒ„")
                summary['overall_status'] = 'FAIL'
                summary['data_readiness'] = False
            
            if integrity.get('null_values', 0) > 0:
                summary['critical_issues'].append(f"ç™¼ç¾{integrity['null_values']}æ¢NULLå€¼è¨˜éŒ„")
                summary['overall_status'] = 'FAIL'
                summary['data_readiness'] = False
            
            if integrity.get('invalid_prices', 0) > 0:
                summary['critical_issues'].append(f"ç™¼ç¾{integrity['invalid_prices']}æ¢ç•°å¸¸åƒ¹æ ¼è¨˜éŒ„")
                summary['overall_status'] = 'FAIL'
                summary['data_readiness'] = False
            
            # æª¢æŸ¥è­¦å‘Š
            continuity = results.get('continuity_check', {})
            for timeframe in ['1m', '5m', '1h']:
                continuity_info = continuity.get(f'{timeframe}_continuity', {})
                continuity_rate = continuity_info.get('continuity_rate', 1.0)
                
                if continuity_rate < 0.95:  # 95%é€£çºŒæ€§é–¾å€¼
                    summary['warnings'].append(f"{timeframe}æ•¸æ“šé€£çºŒæ€§åƒ…{continuity_rate:.1%}")
                    if summary['overall_status'] == 'PASS':
                        summary['overall_status'] = 'WARNING'
            
            # ç”Ÿæˆå»ºè­°
            db_info = results.get('database_info', {})
            total_records = db_info.get('total_records', 0)
            
            if total_records < 1000:
                summary['recommendations'].append("å»ºè­°å¢åŠ æ­·å²æ•¸æ“šé‡ä»¥æé«˜AIæ±ºç­–æº–ç¢ºæ€§")
            
            if summary['overall_status'] == 'PASS':
                summary['recommendations'].append("æ•¸æ“šå“è³ªè‰¯å¥½ï¼Œå¯ä»¥é–‹å§‹å¯¦ç›¤æ¸¬è©¦")
            
            # è¼¸å‡ºç¸½çµ
            print(f"\nğŸ¯ é©—è­‰ç¸½çµ:")
            print(f"   ğŸ“Š æ•´é«”ç‹€æ…‹: {summary['overall_status']}")
            print(f"   ğŸ“ˆ ç¸½è¨˜éŒ„æ•¸: {total_records}")
            print(f"   ğŸš€ å¯¦ç›¤æº–å‚™: {'âœ… å°±ç·’' if summary['data_readiness'] else 'âŒ æœªå°±ç·’'}")
            
            if summary['critical_issues']:
                print(f"   ğŸš¨ é—œéµå•é¡Œ:")
                for issue in summary['critical_issues']:
                    print(f"     - {issue}")
            
            if summary['warnings']:
                print(f"   âš ï¸ è­¦å‘Š:")
                for warning in summary['warnings']:
                    print(f"     - {warning}")
            
            if summary['recommendations']:
                print(f"   ğŸ’¡ å»ºè­°:")
                for rec in summary['recommendations']:
                    print(f"     - {rec}")
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç¸½çµå ±å‘Šå¤±æ•—: {e}")
            summary['error'] = str(e)
            summary['overall_status'] = 'ERROR'
            summary['data_readiness'] = False
            return summary
    
    async def close(self):
        """é—œé–‰é€£æ¥"""
        await self.manager.close()


async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ MAXæ­·å²è³‡æ–™åº«å®Œæ•´æ€§å’Œæ•¸æ“šå“è³ªé©—è­‰")
    print("=" * 60)
    
    validator = HistoricalDataValidator()
    
    try:
        # åŸ·è¡Œå®Œæ•´é©—è­‰
        results = await validator.run_full_validation()
        
        # ä¿å­˜é©—è­‰çµæœ
        import json
        with open('AImax/logs/data_validation_report.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: AImax/logs/data_validation_report.json")
        
        # è¿”å›é©—è­‰ç‹€æ…‹
        summary = results.get('summary', {})
        if summary.get('data_readiness', False):
            print("\nğŸ‰ æ•¸æ“šé©—è­‰é€šéï¼å¯ä»¥é–‹å§‹å¯¦ç›¤æ¸¬è©¦ï¼")
            return True
        else:
            print("\nâš ï¸ æ•¸æ“šé©—è­‰æœªé€šéï¼Œéœ€è¦ä¿®å¾©å•é¡Œå¾Œå†é€²è¡Œå¯¦ç›¤æ¸¬è©¦")
            return False
            
    except Exception as e:
        logger.error(f"âŒ é©—è­‰éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False
        
    finally:
        await validator.close()


if __name__ == "__main__":
    # å‰µå»ºæ—¥èªŒç›®éŒ„
    os.makedirs('AImax/logs', exist_ok=True)
    
    # é‹è¡Œé©—è­‰
    success = asyncio.run(main())
    
    if success:
        print("\nâœ… é©—è­‰å®Œæˆï¼Œç³»çµ±æº–å‚™å°±ç·’ï¼")
    else:
        print("\nâŒ é©—è­‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥å•é¡Œä¸¦ä¿®å¾©")