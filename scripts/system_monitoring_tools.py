#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç³»çµ±ç›£æ§å’Œç¶­è­·å·¥å…· - ä»»å‹™8.3å¯¦ç¾
å»ºç«‹ç³»çµ±ç›£æ§å’Œç¶­è­·å·¥å…·
"""

import sys
import os
import json
import time
import psutil
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
import logging
from pathlib import Path
import sqlite3
from collections import deque, defaultdict

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

logger = logging.getLogger(__name__)

@dataclass
class SystemMetrics:
    """ç³»çµ±æŒ‡æ¨™"""
    timestamp: datetime
    cpu_usage: float
    memory_usage: float
    disk_usage: float
    network_io: Dict[str, int]
    process_count: int
    thread_count: int
    uptime: float

@dataclass
class Alert:
    """è­¦å ±"""
    id: str
    type: str
    severity: str  # LOW, MEDIUM, HIGH, CRITICAL
    message: str
    timestamp: datetime
    resolved: bool = False
    resolution_time: Optional[datetime] = None

class MetricsCollector:
    """æŒ‡æ¨™æ”¶é›†å™¨"""
    
    def __init__(self, db_path: str = None):
        self.db_path = db_path or "AImax/logs/system_metrics.db"
        self.setup_database()
        self.collecting = False
        self.collection_thread = None
        
    def setup_database(self):
        """è¨­ç½®æ•¸æ“šåº«"""
        try:
            os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
            
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS system_metrics (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp TEXT NOT NULL,
                        cpu_usage REAL,
                        memory_usage REAL,
                        disk_usage REAL,
                        network_sent INTEGER,
                        network_recv INTEGER,
                        process_count INTEGER,
                        thread_count INTEGER,
                        uptime REAL
                    )
                """)
                
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS alerts (
                        id TEXT PRIMARY KEY,
                        type TEXT NOT NULL,
                        severity TEXT NOT NULL,
                        message TEXT NOT NULL,
                        timestamp TEXT NOT NULL,
                        resolved INTEGER DEFAULT 0,
                        resolution_time TEXT
                    )
                """)
                
                conn.commit()
                
            logger.info(f"âœ… æŒ‡æ¨™æ•¸æ“šåº«åˆå§‹åŒ–å®Œæˆ: {self.db_path}")
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šåº«è¨­ç½®å¤±æ•—: {e}")
            raise
    
    def collect_metrics(self) -> SystemMetrics:
        """æ”¶é›†ç³»çµ±æŒ‡æ¨™"""
        try:
            # CPUä½¿ç”¨ç‡
            cpu_usage = psutil.cpu_percent(interval=1)
            
            # å…§å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            memory_usage = memory.percent
            
            # ç£ç›¤ä½¿ç”¨ç‡
            disk = psutil.disk_usage('/')
            disk_usage = (disk.used / disk.total) * 100
            
            # ç¶²çµ¡IO
            net_io = psutil.net_io_counters()
            network_io = {
                'bytes_sent': net_io.bytes_sent,
                'bytes_recv': net_io.bytes_recv
            }
            
            # é€²ç¨‹å’Œç·šç¨‹æ•¸
            process_count = len(psutil.pids())
            
            # ç•¶å‰é€²ç¨‹çš„ç·šç¨‹æ•¸
            current_process = psutil.Process()
            thread_count = current_process.num_threads()
            
            # ç³»çµ±é‹è¡Œæ™‚é–“
            uptime = time.time() - psutil.boot_time()
            
            metrics = SystemMetrics(
                timestamp=datetime.now(),
                cpu_usage=cpu_usage,
                memory_usage=memory_usage,
                disk_usage=disk_usage,
                network_io=network_io,
                process_count=process_count,
                thread_count=thread_count,
                uptime=uptime
            )
            
            return metrics
            
        except Exception as e:
            logger.error(f"âŒ æŒ‡æ¨™æ”¶é›†å¤±æ•—: {e}")
            raise
    
    def save_metrics(self, metrics: SystemMetrics):
        """ä¿å­˜æŒ‡æ¨™åˆ°æ•¸æ“šåº«"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    INSERT INTO system_metrics 
                    (timestamp, cpu_usage, memory_usage, disk_usage, 
                     network_sent, network_recv, process_count, thread_count, uptime)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    metrics.timestamp.isoformat(),
                    metrics.cpu_usage,
                    metrics.memory_usage,
                    metrics.disk_usage,
                    metrics.network_io['bytes_sent'],
                    metrics.network_io['bytes_recv'],
                    metrics.process_count,
                    metrics.thread_count,
                    metrics.uptime
                ))
                conn.commit()
                
        except Exception as e:
            logger.error(f"âŒ æŒ‡æ¨™ä¿å­˜å¤±æ•—: {e}")
    
    def start_collection(self, interval: int = 60):
        """é–‹å§‹æŒ‡æ¨™æ”¶é›†"""
        if self.collecting:
            return
        
        self.collecting = True
        self.collection_thread = threading.Thread(
            target=self._collection_loop,
            args=(interval,),
            daemon=True
        )
        self.collection_thread.start()
        
        logger.info(f"ğŸ“Š é–‹å§‹æŒ‡æ¨™æ”¶é›† (é–“éš”: {interval}ç§’)")
    
    def stop_collection(self):
        """åœæ­¢æŒ‡æ¨™æ”¶é›†"""
        self.collecting = False
        if self.collection_thread:
            self.collection_thread.join(timeout=5)
        
        logger.info("â¹ï¸ åœæ­¢æŒ‡æ¨™æ”¶é›†")
    
    def _collection_loop(self, interval: int):
        """æŒ‡æ¨™æ”¶é›†å¾ªç’°"""
        while self.collecting:
            try:
                metrics = self.collect_metrics()
                self.save_metrics(metrics)
                time.sleep(interval)
                
            except Exception as e:
                logger.error(f"âŒ æŒ‡æ¨™æ”¶é›†å¾ªç’°éŒ¯èª¤: {e}")
                time.sleep(interval)
    
    def get_recent_metrics(self, hours: int = 24) -> List[Dict[str, Any]]:
        """ç²å–æœ€è¿‘çš„æŒ‡æ¨™"""
        try:
            since = datetime.now() - timedelta(hours=hours)
            
            with sqlite3.connect(self.db_path) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.execute("""
                    SELECT * FROM system_metrics 
                    WHERE timestamp > ? 
                    ORDER BY timestamp DESC
                """, (since.isoformat(),))
                
                metrics = []
                for row in cursor.fetchall():
                    metrics.append(dict(row))
                
                return metrics
                
        except Exception as e:
            logger.error(f"âŒ ç²å–æŒ‡æ¨™å¤±æ•—: {e}")
            return []
    
    def get_metrics_summary(self, hours: int = 24) -> Dict[str, Any]:
        """ç²å–æŒ‡æ¨™æ‘˜è¦"""
        try:
            metrics = self.get_recent_metrics(hours)
            
            if not metrics:
                return {}
            
            # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
            cpu_values = [m['cpu_usage'] for m in metrics if m['cpu_usage'] is not None]
            memory_values = [m['memory_usage'] for m in metrics if m['memory_usage'] is not None]
            disk_values = [m['disk_usage'] for m in metrics if m['disk_usage'] is not None]
            
            summary = {
                "time_range_hours": hours,
                "total_records": len(metrics),
                "cpu_stats": {
                    "avg": sum(cpu_values) / len(cpu_values) if cpu_values else 0,
                    "max": max(cpu_values) if cpu_values else 0,
                    "min": min(cpu_values) if cpu_values else 0
                },
                "memory_stats": {
                    "avg": sum(memory_values) / len(memory_values) if memory_values else 0,
                    "max": max(memory_values) if memory_values else 0,
                    "min": min(memory_values) if memory_values else 0
                },
                "disk_stats": {
                    "avg": sum(disk_values) / len(disk_values) if disk_values else 0,
                    "max": max(disk_values) if disk_values else 0,
                    "min": min(disk_values) if disk_values else 0
                }
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ æŒ‡æ¨™æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}")
            return {}

class AlertManager:
    """è­¦å ±ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = None):
        self.db_path = db_path or "AImax/logs/system_metrics.db"
        self.alert_rules = []
        self.setup_alert_rules()
        
    def setup_alert_rules(self):
        """è¨­ç½®è­¦å ±è¦å‰‡"""
        self.alert_rules = [
            {
                "name": "é«˜CPUä½¿ç”¨ç‡",
                "condition": lambda metrics: metrics.cpu_usage > 80,
                "severity": "HIGH",
                "message": lambda metrics: f"CPUä½¿ç”¨ç‡éé«˜: {metrics.cpu_usage:.1f}%"
            },
            {
                "name": "é«˜å…§å­˜ä½¿ç”¨ç‡",
                "condition": lambda metrics: metrics.memory_usage > 85,
                "severity": "HIGH", 
                "message": lambda metrics: f"å…§å­˜ä½¿ç”¨ç‡éé«˜: {metrics.memory_usage:.1f}%"
            },
            {
                "name": "ç£ç›¤ç©ºé–“ä¸è¶³",
                "condition": lambda metrics: metrics.disk_usage > 90,
                "severity": "CRITICAL",
                "message": lambda metrics: f"ç£ç›¤ä½¿ç”¨ç‡éé«˜: {metrics.disk_usage:.1f}%"
            },
            {
                "name": "é€²ç¨‹æ•¸éå¤š",
                "condition": lambda metrics: metrics.process_count > 500,
                "severity": "MEDIUM",
                "message": lambda metrics: f"é€²ç¨‹æ•¸éå¤š: {metrics.process_count}"
            }
        ]
    
    def check_alerts(self, metrics: SystemMetrics) -> List[Alert]:
        """æª¢æŸ¥è­¦å ±"""
        alerts = []
        
        for rule in self.alert_rules:
            try:
                if rule["condition"](metrics):
                    alert_id = f"{rule['name']}_{metrics.timestamp.strftime('%Y%m%d_%H%M%S')}"
                    
                    alert = Alert(
                        id=alert_id,
                        type=rule["name"],
                        severity=rule["severity"],
                        message=rule["message"](metrics),
                        timestamp=metrics.timestamp
                    )
                    
                    alerts.append(alert)
                    
            except Exception as e:
                logger.error(f"âŒ è­¦å ±è¦å‰‡æª¢æŸ¥å¤±æ•—: {rule['name']}, {e}")
        
        return alerts
    
    def save_alert(self, alert: Alert):
        """ä¿å­˜è­¦å ±"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO alerts 
                    (id, type, severity, message, timestamp, resolved, resolution_time)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    alert.id,
                    alert.type,
                    alert.severity,
                    alert.message,
                    alert.timestamp.isoformat(),
                    1 if alert.resolved else 0,
                    alert.resolution_time.isoformat() if alert.resolution_time else None
                ))
                conn.commit()
                
        except Exception as e:
            logger.error(f"âŒ è­¦å ±ä¿å­˜å¤±æ•—: {e}")
    
    def get_active_alerts(self) -> List[Dict[str, Any]]:
        """ç²å–æ´»èºè­¦å ±"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.execute("""
                    SELECT * FROM alerts 
                    WHERE resolved = 0 
                    ORDER BY timestamp DESC
                """)
                
                alerts = []
                for row in cursor.fetchall():
                    alerts.append(dict(row))
                
                return alerts
                
        except Exception as e:
            logger.error(f"âŒ ç²å–æ´»èºè­¦å ±å¤±æ•—: {e}")
            return []
    
    def resolve_alert(self, alert_id: str):
        """è§£æ±ºè­¦å ±"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    UPDATE alerts 
                    SET resolved = 1, resolution_time = ?
                    WHERE id = ?
                """, (datetime.now().isoformat(), alert_id))
                conn.commit()
                
            logger.info(f"âœ… è­¦å ±å·²è§£æ±º: {alert_id}")
            
        except Exception as e:
            logger.error(f"âŒ è§£æ±ºè­¦å ±å¤±æ•—: {e}")

class SystemMaintenance:
    """ç³»çµ±ç¶­è­·å·¥å…·"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root or os.getcwd())
        
    def cleanup_logs(self, days_to_keep: int = 30) -> Dict[str, Any]:
        """æ¸…ç†èˆŠæ—¥èªŒ"""
        try:
            logger.info(f"ğŸ§¹ æ¸…ç† {days_to_keep} å¤©å‰çš„æ—¥èªŒ")
            
            logs_dir = self.project_root / "AImax" / "logs"
            if not logs_dir.exists():
                return {"success": True, "message": "æ—¥èªŒç›®éŒ„ä¸å­˜åœ¨", "cleaned_files": 0}
            
            cutoff_date = datetime.now() - timedelta(days=days_to_keep)
            cleaned_files = 0
            total_size_cleaned = 0
            
            for log_file in logs_dir.rglob("*.log"):
                try:
                    file_mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
                    if file_mtime < cutoff_date:
                        file_size = log_file.stat().st_size
                        log_file.unlink()
                        cleaned_files += 1
                        total_size_cleaned += file_size
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†æ—¥èªŒæ–‡ä»¶å¤±æ•—: {log_file}, {e}")
            
            return {
                "success": True,
                "message": f"æ¸…ç†å®Œæˆ: {cleaned_files} å€‹æ–‡ä»¶, {total_size_cleaned / 1024 / 1024:.1f} MB",
                "cleaned_files": cleaned_files,
                "size_cleaned_mb": total_size_cleaned / 1024 / 1024
            }
            
        except Exception as e:
            logger.error(f"âŒ æ—¥èªŒæ¸…ç†å¤±æ•—: {e}")
            return {"success": False, "error": str(e)}
    
    def optimize_database(self) -> Dict[str, Any]:
        """å„ªåŒ–æ•¸æ“šåº«"""
        try:
            logger.info("ğŸ”§ å„ªåŒ–ç³»çµ±æ•¸æ“šåº«")
            
            db_files = [
                "AImax/logs/system_metrics.db",
                "data/kline_data.db",
                "data/market_history.db",
                "data/multi_pair_market.db"
            ]
            
            optimized_dbs = []
            
            for db_file in db_files:
                db_path = self.project_root / db_file
                if db_path.exists():
                    try:
                        with sqlite3.connect(str(db_path)) as conn:
                            # åŸ·è¡ŒVACUUMå„ªåŒ–
                            conn.execute("VACUUM")
                            # é‡å»ºç´¢å¼•
                            conn.execute("REINDEX")
                            conn.commit()
                        
                        optimized_dbs.append(db_file)
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ•¸æ“šåº«å„ªåŒ–å¤±æ•—: {db_file}, {e}")
            
            return {
                "success": True,
                "message": f"å„ªåŒ–äº† {len(optimized_dbs)} å€‹æ•¸æ“šåº«",
                "optimized_databases": optimized_dbs
            }
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šåº«å„ªåŒ–å¤±æ•—: {e}")
            return {"success": False, "error": str(e)}
    
    def check_disk_space(self) -> Dict[str, Any]:
        """æª¢æŸ¥ç£ç›¤ç©ºé–“"""
        try:
            disk_usage = psutil.disk_usage(str(self.project_root))
            
            total_gb = disk_usage.total / (1024**3)
            used_gb = disk_usage.used / (1024**3)
            free_gb = disk_usage.free / (1024**3)
            usage_percent = (used_gb / total_gb) * 100
            
            status = "OK"
            if usage_percent > 90:
                status = "CRITICAL"
            elif usage_percent > 80:
                status = "WARNING"
            
            return {
                "success": True,
                "status": status,
                "total_gb": round(total_gb, 2),
                "used_gb": round(used_gb, 2),
                "free_gb": round(free_gb, 2),
                "usage_percent": round(usage_percent, 1)
            }
            
        except Exception as e:
            logger.error(f"âŒ ç£ç›¤ç©ºé–“æª¢æŸ¥å¤±æ•—: {e}")
            return {"success": False, "error": str(e)}
    
    def system_health_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç³»çµ±å¥åº·å ±å‘Š"""
        try:
            logger.info("ğŸ“‹ ç”Ÿæˆç³»çµ±å¥åº·å ±å‘Š")
            
            # æ”¶é›†å„ç¨®ç³»çµ±ä¿¡æ¯
            metrics_collector = MetricsCollector()
            current_metrics = metrics_collector.collect_metrics()
            metrics_summary = metrics_collector.get_metrics_summary(24)
            
            alert_manager = AlertManager()
            active_alerts = alert_manager.get_active_alerts()
            
            disk_info = self.check_disk_space()
            
            # é€²ç¨‹ä¿¡æ¯
            current_process = psutil.Process()
            process_info = {
                "pid": current_process.pid,
                "memory_mb": current_process.memory_info().rss / 1024 / 1024,
                "cpu_percent": current_process.cpu_percent(),
                "num_threads": current_process.num_threads(),
                "create_time": datetime.fromtimestamp(current_process.create_time()).isoformat()
            }
            
            health_report = {
                "timestamp": datetime.now().isoformat(),
                "current_metrics": {
                    "cpu_usage": current_metrics.cpu_usage,
                    "memory_usage": current_metrics.memory_usage,
                    "disk_usage": current_metrics.disk_usage,
                    "process_count": current_metrics.process_count,
                    "thread_count": current_metrics.thread_count,
                    "uptime_hours": current_metrics.uptime / 3600
                },
                "metrics_summary_24h": metrics_summary,
                "active_alerts": len(active_alerts),
                "alert_details": active_alerts,
                "disk_info": disk_info,
                "process_info": process_info,
                "system_status": self._determine_system_status(current_metrics, active_alerts, disk_info)
            }
            
            return health_report
            
        except Exception as e:
            logger.error(f"âŒ å¥åº·å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
            return {"error": str(e), "timestamp": datetime.now().isoformat()}
    
    def _determine_system_status(self, metrics: SystemMetrics, alerts: List, disk_info: Dict) -> str:
        """ç¢ºå®šç³»çµ±ç‹€æ…‹"""
        try:
            # æª¢æŸ¥é—œéµæŒ‡æ¨™
            critical_issues = []
            
            if metrics.cpu_usage > 90:
                critical_issues.append("CPUä½¿ç”¨ç‡éé«˜")
            
            if metrics.memory_usage > 90:
                critical_issues.append("å…§å­˜ä½¿ç”¨ç‡éé«˜")
            
            if disk_info.get("usage_percent", 0) > 95:
                critical_issues.append("ç£ç›¤ç©ºé–“ä¸è¶³")
            
            # æª¢æŸ¥é—œéµè­¦å ±
            critical_alerts = [a for a in alerts if a.get("severity") == "CRITICAL"]
            
            if critical_issues or critical_alerts:
                return "CRITICAL"
            elif metrics.cpu_usage > 80 or metrics.memory_usage > 80 or len(alerts) > 0:
                return "WARNING"
            else:
                return "HEALTHY"
                
        except Exception as e:
            logger.error(f"âŒ ç³»çµ±ç‹€æ…‹åˆ¤æ–·å¤±æ•—: {e}")
            return "UNKNOWN"

class SystemMonitor:
    """ç³»çµ±ç›£æ§ä¸»é¡"""
    
    def __init__(self, project_root: str = None):
        self.project_root = project_root
        self.metrics_collector = MetricsCollector()
        self.alert_manager = AlertManager()
        self.maintenance = SystemMaintenance(project_root)
        self.monitoring = False
        
    def start_monitoring(self, interval: int = 60):
        """é–‹å§‹ç³»çµ±ç›£æ§"""
        try:
            logger.info("ğŸ” å•Ÿå‹•ç³»çµ±ç›£æ§")
            
            # å•Ÿå‹•æŒ‡æ¨™æ”¶é›†
            self.metrics_collector.start_collection(interval)
            
            # å•Ÿå‹•è­¦å ±ç›£æ§
            self.monitoring = True
            monitor_thread = threading.Thread(
                target=self._monitoring_loop,
                args=(interval,),
                daemon=True
            )
            monitor_thread.start()
            
            logger.info(f"âœ… ç³»çµ±ç›£æ§å·²å•Ÿå‹• (é–“éš”: {interval}ç§’)")
            
        except Exception as e:
            logger.error(f"âŒ ç³»çµ±ç›£æ§å•Ÿå‹•å¤±æ•—: {e}")
    
    def stop_monitoring(self):
        """åœæ­¢ç³»çµ±ç›£æ§"""
        try:
            logger.info("â¹ï¸ åœæ­¢ç³»çµ±ç›£æ§")
            
            self.monitoring = False
            self.metrics_collector.stop_collection()
            
            logger.info("âœ… ç³»çµ±ç›£æ§å·²åœæ­¢")
            
        except Exception as e:
            logger.error(f"âŒ ç³»çµ±ç›£æ§åœæ­¢å¤±æ•—: {e}")
    
    def _monitoring_loop(self, interval: int):
        """ç›£æ§å¾ªç’°"""
        while self.monitoring:
            try:
                # æ”¶é›†ç•¶å‰æŒ‡æ¨™
                metrics = self.metrics_collector.collect_metrics()
                
                # æª¢æŸ¥è­¦å ±
                alerts = self.alert_manager.check_alerts(metrics)
                
                # ä¿å­˜æ–°è­¦å ±
                for alert in alerts:
                    self.alert_manager.save_alert(alert)
                    logger.warning(f"âš ï¸ æ–°è­¦å ±: {alert.message}")
                
                time.sleep(interval)
                
            except Exception as e:
                logger.error(f"âŒ ç›£æ§å¾ªç’°éŒ¯èª¤: {e}")
                time.sleep(interval)
    
    def get_dashboard_data(self) -> Dict[str, Any]:
        """ç²å–ç›£æ§å„€è¡¨æ¿æ•¸æ“š"""
        try:
            health_report = self.maintenance.system_health_report()
            active_alerts = self.alert_manager.get_active_alerts()
            
            dashboard_data = {
                "timestamp": datetime.now().isoformat(),
                "system_status": health_report.get("system_status", "UNKNOWN"),
                "current_metrics": health_report.get("current_metrics", {}),
                "active_alerts_count": len(active_alerts),
                "recent_alerts": active_alerts[:5],  # æœ€è¿‘5å€‹è­¦å ±
                "disk_info": health_report.get("disk_info", {}),
                "process_info": health_report.get("process_info", {})
            }
            
            return dashboard_data
            
        except Exception as e:
            logger.error(f"âŒ å„€è¡¨æ¿æ•¸æ“šç²å–å¤±æ•—: {e}")
            return {"error": str(e)}

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ” AImax ç³»çµ±ç›£æ§å·¥å…·")
    print("=" * 40)
    
    try:
        # å‰µå»ºç³»çµ±ç›£æ§
        monitor = SystemMonitor()
        
        # å•Ÿå‹•ç›£æ§
        monitor.start_monitoring(interval=30)  # 30ç§’é–“éš”
        
        # é‹è¡Œä¸€æ®µæ™‚é–“æ”¶é›†æ•¸æ“š
        print("ğŸ“Š æ”¶é›†ç³»çµ±æŒ‡æ¨™ä¸­...")
        time.sleep(60)  # é‹è¡Œ1åˆ†é˜
        
        # ç²å–å„€è¡¨æ¿æ•¸æ“š
        dashboard_data = monitor.get_dashboard_data()
        
        print(f"\nğŸ“‹ ç³»çµ±ç‹€æ…‹: {dashboard_data.get('system_status', 'UNKNOWN')}")
        
        current_metrics = dashboard_data.get('current_metrics', {})
        print(f"CPUä½¿ç”¨ç‡: {current_metrics.get('cpu_usage', 0):.1f}%")
        print(f"å…§å­˜ä½¿ç”¨ç‡: {current_metrics.get('memory_usage', 0):.1f}%")
        print(f"ç£ç›¤ä½¿ç”¨ç‡: {current_metrics.get('disk_usage', 0):.1f}%")
        print(f"æ´»èºè­¦å ±: {dashboard_data.get('active_alerts_count', 0)} å€‹")
        
        # åŸ·è¡Œç¶­è­·ä»»å‹™
        print(f"\nğŸ§¹ åŸ·è¡Œç³»çµ±ç¶­è­·...")
        cleanup_result = monitor.maintenance.cleanup_logs(days_to_keep=7)
        print(f"æ—¥èªŒæ¸…ç†: {cleanup_result.get('message', 'å¤±æ•—')}")
        
        optimize_result = monitor.maintenance.optimize_database()
        print(f"æ•¸æ“šåº«å„ªåŒ–: {optimize_result.get('message', 'å¤±æ•—')}")
        
        # åœæ­¢ç›£æ§
        monitor.stop_monitoring()
        
        # ä¿å­˜ç›£æ§å ±å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_dir = Path("AImax/logs/monitoring_reports")
        report_dir.mkdir(parents=True, exist_ok=True)
        
        report_file = report_dir / f"monitoring_report_{timestamp}.json"
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump({
                "dashboard_data": dashboard_data,
                "maintenance_results": {
                    "cleanup": cleanup_result,
                    "optimize": optimize_result
                }
            }, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ“„ ç›£æ§å ±å‘Šå·²ä¿å­˜: {report_file}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ ç³»çµ±ç›£æ§åŸ·è¡Œå¤±æ•—: {e}")
        return 1

if __name__ == "__main__":
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    exit_code = main()
    sys.exit(exit_code)