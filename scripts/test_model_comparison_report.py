#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦æ¨¡å‹æ¯”è¼ƒå ±å‘Šç³»çµ±
"""

import sys
import os
import asyncio
import logging
from datetime import datetime
import numpy as np
import json

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.core.model_validation_report import create_model_validation_report_generator

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def generate_comprehensive_test_data(num_samples: int = 30) -> tuple:
    """ç”Ÿæˆç¶œåˆæ¸¬è©¦æ•¸æ“š"""
    test_data = []
    ground_truth = []
    
    np.random.seed(42)  # ç¢ºä¿çµæœå¯é‡ç¾
    
    for i in range(num_samples):
        # ç”Ÿæˆå¤šæ¨£åŒ–çš„å¸‚å ´æ•¸æ“š
        base_price = 1500000 + np.random.normal(0, 200000)
        price_change = np.random.normal(0, 3)  # æ›´å¤§çš„åƒ¹æ ¼è®ŠåŒ–ç¯„åœ
        volume_ratio = max(0.3, np.random.lognormal(0, 0.5))  # æ›´å¤§çš„æˆäº¤é‡è®ŠåŒ–
        
        # æ ¹æ“šå¸‚å ´æ¢ä»¶ç”Ÿæˆç›¸æ‡‰çš„AIæ•¸æ“šå’ŒçœŸå¯¦æ¨™ç±¤
        if price_change > 2:
            ai_data = "å¼·å‹¢ä¸Šæ¼²ä¿¡è™Ÿï¼Œå¤šé …æŠ€è¡“æŒ‡æ¨™ç¢ºèªçœ‹æ¼²è¶¨å‹¢"
            true_signal = "BUY"
        elif price_change > 0.5:
            ai_data = "æº«å’Œä¸Šæ¼²ä¿¡è™Ÿï¼Œå¸‚å ´æƒ…ç·’åæ¨‚è§€"
            true_signal = "BUY" if np.random.random() > 0.3 else "HOLD"
        elif price_change < -2:
            ai_data = "å¼·å‹¢ä¸‹è·Œä¿¡è™Ÿï¼ŒæŠ€è¡“æŒ‡æ¨™é¡¯ç¤ºçœ‹è·Œ"
            true_signal = "SELL"
        elif price_change < -0.5:
            ai_data = "æº«å’Œä¸‹è·Œä¿¡è™Ÿï¼Œå¸‚å ´æƒ…ç·’åæ‚²è§€"
            true_signal = "SELL" if np.random.random() > 0.3 else "HOLD"
        else:
            ai_data = "å¸‚å ´æ©«ç›¤æ•´ç†ï¼ŒæŠ€è¡“æŒ‡æ¨™ä¸­æ€§"
            true_signal = "HOLD"
        
        # æ·»åŠ ä¸€äº›å™ªéŸ³ä½¿æ•¸æ“šæ›´çœŸå¯¦
        if np.random.random() < 0.1:  # 10%çš„å™ªéŸ³
            true_signal = np.random.choice(['BUY', 'SELL', 'HOLD'])
        
        market_data = {
            'current_price': base_price,
            'price_change_1m': price_change,
            'volume_ratio': volume_ratio,
            'ai_formatted_data': ai_data,
            'volume': max(100, np.random.poisson(1000)),
            'timestamp': datetime.now()
        }
        
        test_data.append(market_data)
        ground_truth.append(true_signal)
    
    return test_data, ground_truth

async def test_model_comparison_report():
    """æ¸¬è©¦æ¨¡å‹æ¯”è¼ƒå ±å‘Šç³»çµ±"""
    print("ğŸ§ª æ¸¬è©¦æ¨¡å‹æ¯”è¼ƒå ±å‘Šç³»çµ±...")
    print("=" * 60)
    
    # å‰µå»ºå ±å‘Šç”Ÿæˆå™¨
    print("\nğŸ“Š åˆå§‹åŒ–å ±å‘Šç”Ÿæˆå™¨...")
    report_generator = create_model_validation_report_generator("AImax/reports")
    
    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
    print("\nğŸ“Š ç”Ÿæˆæ¸¬è©¦æ•¸æ“š...")
    test_data, ground_truth = generate_comprehensive_test_data(30)
    print(f"   ç”Ÿæˆäº† {len(test_data)} å€‹æ¸¬è©¦æ¨£æœ¬")
    print(f"   çœŸå¯¦æ¨™ç±¤åˆ†ä½ˆ: {dict(zip(*np.unique(ground_truth, return_counts=True)))}")
    
    # é¡¯ç¤ºéƒ¨åˆ†æ¸¬è©¦æ•¸æ“š
    print(f"\nğŸ“‹ æ¸¬è©¦æ•¸æ“šæ¨£æœ¬:")
    for i in range(3):
        data = test_data[i]
        truth = ground_truth[i]
        print(f"   æ¨£æœ¬ {i+1}: åƒ¹æ ¼è®ŠåŒ– {data['price_change_1m']:+.2f}%, çœŸå¯¦æ¨™ç±¤ {truth}")
    
    # åŸ·è¡Œæ¨¡å‹é©—è­‰å’Œæ¯”è¼ƒ
    print(f"\nğŸ” åŸ·è¡Œæ¨¡å‹é©—è­‰å’Œæ¯”è¼ƒ...")
    validation_result = await report_generator.validate_all_models(test_data, ground_truth)
    
    if not validation_result['success']:
        print(f"âŒ æ¨¡å‹é©—è­‰å¤±æ•—: {validation_result.get('error', 'Unknown error')}")
        return {
            'test_successful': False,
            'models_validated': 0,
            'best_model': 'none',
            'average_accuracy': 0.0,
            'reports_saved': 0,
            'test_success_rate': 0.0,
            'system_health': 'å¤±æ•—'
        }
    
    print(f"âœ… æ¨¡å‹é©—è­‰æˆåŠŸ")
    
    # é¡¯ç¤ºé©—è­‰çµæœæ‘˜è¦
    print(f"\nğŸ“ˆ é©—è­‰çµæœæ‘˜è¦:")
    print("-" * 40)
    models_validated = validation_result['models_validated']
    print(f"   æˆåŠŸé©—è­‰çš„æ¨¡å‹æ•¸: {len(models_validated)}")
    print(f"   é©—è­‰çš„æ¨¡å‹: {', '.join(models_validated)}")
    
    # é¡¯ç¤ºæ¯”è¼ƒåˆ†æçµæœ
    comparison_analysis = validation_result['comparison_analysis']
    print(f"\nğŸ† æ¨¡å‹æ¯”è¼ƒåˆ†æ:")
    print("-" * 40)
    print(f"   æœ€ä½³ç¶œåˆæ¨¡å‹: {comparison_analysis.best_overall_model}")
    print(f"   æœ€ä½³æº–ç¢ºç‡æ¨¡å‹: {comparison_analysis.best_accuracy_model}")
    print(f"   æœ€ä½³ç©©å®šæ€§æ¨¡å‹: {comparison_analysis.best_stability_model}")
    print(f"   æœ€ä½³é€Ÿåº¦æ¨¡å‹: {comparison_analysis.best_speed_model}")
    
    # é¡¯ç¤ºæ¨¡å‹æ’å
    print(f"\nğŸ“Š æ¨¡å‹æ’å:")
    model_rankings = comparison_analysis.model_rankings
    for model, rank in sorted(model_rankings.items(), key=lambda x: x[1]):
        print(f"   ç¬¬ {rank} å: {model}")
    
    # é¡¯ç¤ºæ€§èƒ½çŸ©é™£
    print(f"\nğŸ“‹ æ€§èƒ½çŸ©é™£:")
    performance_matrix = comparison_analysis.performance_matrix
    
    # å‰µå»ºè¡¨æ ¼æ¨™é¡Œ
    metrics = ['accuracy', 'f1_score', 'confidence_avg', 'stability_score', 'success_rate', 'overall_score']
    print(f"   {'æ¨¡å‹åç¨±':<20} " + " ".join([f"{m:<12}" for m in metrics]))
    print(f"   {'-'*20} " + " ".join([f"{'-'*12}" for _ in metrics]))
    
    # é¡¯ç¤ºæ¯å€‹æ¨¡å‹çš„æ€§èƒ½æ•¸æ“š
    for model_name, model_data in performance_matrix.items():
        row = f"   {model_name:<20}"
        for metric in metrics:
            value = model_data.get(metric, 0)
            if metric in ['accuracy', 'f1_score', 'confidence_avg', 'stability_score', 'success_rate', 'overall_score']:
                row += f" {value:<12.3f}"
            else:
                row += f" {value:<12.2f}"
        print(row)
    
    # é¡¯ç¤ºå»ºè­°
    print(f"\nğŸ’¡ æ¨¡å‹é¸æ“‡å»ºè­°:")
    recommendations = comparison_analysis.recommendations
    for i, rec in enumerate(recommendations, 1):
        print(f"   {i}. {rec}")
    
    # é¡¯ç¤ºæ¯”è¼ƒæ‘˜è¦
    print(f"\nğŸ“ æ¯”è¼ƒæ‘˜è¦:")
    print(f"   {comparison_analysis.comparison_summary}")
    
    # é¡¯ç¤ºçµ±ä¸€å ±å‘Šçš„é—œéµä¿¡æ¯
    unified_report = validation_result['unified_report']
    print(f"\nğŸ“Š çµ±ä¸€å ±å‘Šæ‘˜è¦:")
    print("-" * 40)
    
    exec_summary = unified_report['executive_summary']
    print(f"   å ±å‘Šç”Ÿæˆæ™‚é–“: {unified_report['report_metadata']['generated_at']}")
    print(f"   è©•ä¼°æ¨¡å‹æ•¸: {exec_summary['model_count']}")
    print(f"   å¹³å‡æº–ç¢ºç‡: {exec_summary['average_accuracy']:.2%}")
    print(f"   ä¸»è¦å»ºè­°: {exec_summary['top_recommendation']}")
    
    # é¡¯ç¤ºçµ±è¨ˆæ‘˜è¦
    if 'statistical_summary' in unified_report:
        stats = unified_report['statistical_summary']
        print(f"\nğŸ“ˆ çµ±è¨ˆæ‘˜è¦:")
        
        if 'accuracy_stats' in stats:
            acc_stats = stats['accuracy_stats']
            print(f"   æº–ç¢ºç‡çµ±è¨ˆ:")
            print(f"     å¹³å‡å€¼: {acc_stats['mean']:.3f}")
            print(f"     æ¨™æº–å·®: {acc_stats['std']:.3f}")
            print(f"     ç¯„åœ: {acc_stats['min']:.3f} - {acc_stats['max']:.3f}")
        
        if 'performance_stats' in stats:
            perf_stats = stats['performance_stats']
            print(f"   æ€§èƒ½çµ±è¨ˆ:")
            print(f"     å¹³å‡è™•ç†æ™‚é–“: {perf_stats['avg_processing_time']:.3f}ç§’")
            print(f"     æœ€å¿«æ¨¡å‹: {perf_stats['fastest_model']}")
            print(f"     æœ€æ…¢æ¨¡å‹: {perf_stats['slowest_model']}")
    
    # é¡¯ç¤ºè©³ç´°é©—è­‰çµæœ
    print(f"\nğŸ” è©³ç´°é©—è­‰çµæœ:")
    detailed_results = unified_report['detailed_results']
    
    for model_name, details in detailed_results.items():
        print(f"\n   ğŸ“Š {model_name}:")
        print(f"     æ¨¡å‹é¡å‹: {details['model_type']}")
        
        metrics = details['metrics']
        print(f"     æ€§èƒ½æŒ‡æ¨™:")
        print(f"       æº–ç¢ºç‡: {metrics.get('accuracy', 0):.3f}")
        print(f"       F1åˆ†æ•¸: {metrics.get('f1_score', 0):.3f}")
        print(f"       å¹³å‡ä¿¡å¿ƒåº¦: {metrics.get('confidence_avg', 0):.3f}")
        print(f"       ç©©å®šæ€§åˆ†æ•¸: {metrics.get('stability_score', 0):.3f}")
        print(f"       æˆåŠŸç‡: {metrics.get('success_rate', 0):.3f}")
        
        processing_info = details['processing_info']
        print(f"     è™•ç†ä¿¡æ¯:")
        print(f"       è™•ç†æ™‚é–“: {processing_info['processing_time']:.3f}ç§’")
        print(f"       é æ¸¬æ•¸é‡: {processing_info['predictions_count']}")
        print(f"       å¹³å‡ä¿¡å¿ƒåº¦: {processing_info['average_confidence']:.3f}")
        
        # é¡¯ç¤ºé æ¸¬åˆ†ä½ˆ
        if 'prediction_distribution' in metrics:
            pred_dist = metrics['prediction_distribution']
            print(f"     é æ¸¬åˆ†ä½ˆ: {pred_dist}")
    
    # é¡¯ç¤ºæ¨¡å‹é¸æ“‡æŒ‡å—
    if 'recommendations' in unified_report and 'model_selection_guide' in unified_report['recommendations']:
        guide = unified_report['recommendations']['model_selection_guide']
        print(f"\nğŸ¯ æ¨¡å‹é¸æ“‡æŒ‡å—:")
        for scenario, recommendation in guide.items():
            print(f"   {scenario}: {recommendation}")
    
    # é¡¯ç¤ºå„ªåŒ–å»ºè­°
    if 'recommendations' in unified_report and 'optimization_suggestions' in unified_report['recommendations']:
        opt_suggestions = unified_report['recommendations']['optimization_suggestions']
        if opt_suggestions:
            print(f"\nğŸ”§ å„ªåŒ–å»ºè­°:")
            for i, suggestion in enumerate(opt_suggestions, 1):
                print(f"   {i}. {suggestion}")
    
    # ä¿å­˜å ±å‘Š
    print(f"\nğŸ’¾ ä¿å­˜å ±å‘Š...")
    saved_files = report_generator.save_report(unified_report)
    
    if 'error' not in saved_files:
        print(f"âœ… å ±å‘Šä¿å­˜æˆåŠŸ:")
        for format_type, file_path in saved_files.items():
            print(f"   {format_type.upper()}: {file_path}")
    else:
        print(f"âŒ å ±å‘Šä¿å­˜å¤±æ•—: {saved_files['error']}")
    
    # æ¸¬è©¦æ¨¡å‹æ€§èƒ½æ­·å²åŠŸèƒ½
    print(f"\nğŸ“š æ¸¬è©¦æ¨¡å‹æ€§èƒ½æ­·å²...")
    history = report_generator.get_model_performance_history()
    
    if 'error' not in history:
        print(f"âœ… æˆåŠŸç²å– {len(history)} å€‹æ¨¡å‹çš„æ€§èƒ½æ­·å²")
        for model_name in history.keys():
            print(f"   - {model_name}")
    else:
        print(f"âŒ ç²å–æ­·å²å¤±æ•—: {history['error']}")
    
    # æ¸¬è©¦åœ–è¡¨æ•¸æ“šç”Ÿæˆ
    if 'charts' in unified_report and unified_report['charts']:
        charts = unified_report['charts']
        print(f"\nğŸ“Š åœ–è¡¨æ•¸æ“šç”Ÿæˆ:")
        print(f"   å¯ç”¨åœ–è¡¨é¡å‹: {list(charts.keys())}")
        
        if 'radar_chart' in charts:
            print(f"   é›·é”åœ–æ•¸æ“š: {len(charts['radar_chart'])} å€‹æ¨¡å‹")
        
        if 'bar_chart' in charts:
            print(f"   æŸ±ç‹€åœ–æ•¸æ“š: {len(charts['bar_chart'])} å€‹æŒ‡æ¨™")
    
    # è¨ˆç®—æ¸¬è©¦æˆåŠŸç‡
    print(f"\nğŸ¯ æ¸¬è©¦çµæœçµ±è¨ˆ:")
    print("-" * 40)
    
    total_tests = 8  # ç¸½æ¸¬è©¦é …ç›®æ•¸
    passed_tests = 0
    
    # æª¢æŸ¥å„é …æ¸¬è©¦çµæœ
    test_results = [
        ("æ¨¡å‹é©—è­‰", validation_result['success']),
        ("æ¯”è¼ƒåˆ†æ", len(comparison_analysis.model_rankings) > 0),
        ("çµ±ä¸€å ±å‘Šç”Ÿæˆ", 'executive_summary' in unified_report),
        ("æ€§èƒ½çŸ©é™£", len(performance_matrix) > 0),
        ("å»ºè­°ç”Ÿæˆ", len(recommendations) > 0),
        ("çµ±è¨ˆæ‘˜è¦", 'statistical_summary' in unified_report),
        ("å ±å‘Šä¿å­˜", 'error' not in saved_files),
        ("æ­·å²è¨˜éŒ„", 'error' not in history)
    ]
    
    for test_name, result in test_results:
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        print(f"   {test_name}: {status}")
        if result:
            passed_tests += 1
    
    success_rate = passed_tests / total_tests
    print(f"\nğŸ“Š æ¸¬è©¦æˆåŠŸç‡: {success_rate:.1%} ({passed_tests}/{total_tests})")
    
    # ç³»çµ±å¥åº·åº¦è©•ä¼°
    if success_rate >= 0.9:
        health_status = "å„ªç§€"
    elif success_rate >= 0.7:
        health_status = "è‰¯å¥½"
    elif success_rate >= 0.5:
        health_status = "ä¸€èˆ¬"
    else:
        health_status = "éœ€æ”¹é€²"
    
    print(f"   ç³»çµ±å¥åº·åº¦: {health_status}")
    
    print(f"\nğŸ¯ æ¨¡å‹æ¯”è¼ƒå ±å‘Šç³»çµ±æ¸¬è©¦å®Œæˆï¼")
    print("=" * 60)
    
    return {
        'test_successful': validation_result['success'],
        'models_validated': len(models_validated),
        'best_model': comparison_analysis.best_overall_model,
        'average_accuracy': exec_summary['average_accuracy'],
        'reports_saved': len(saved_files),
        'test_success_rate': success_rate,
        'system_health': health_status
    }

if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    result = asyncio.run(test_model_comparison_report())
    
    # è¼¸å‡ºæœ€çµ‚æ¸¬è©¦çµæœ
    print(f"\nğŸ† æœ€çµ‚æ¸¬è©¦çµæœ:")
    print(f"   æ¸¬è©¦æˆåŠŸ: {'âœ…' if result['test_successful'] else 'âŒ'}")
    print(f"   é©—è­‰æ¨¡å‹æ•¸: {result['models_validated']}")
    print(f"   æœ€ä½³æ¨¡å‹: {result['best_model']}")
    print(f"   å¹³å‡æº–ç¢ºç‡: {result['average_accuracy']:.2%}")
    print(f"   å ±å‘Šä¿å­˜æ•¸: {result['reports_saved']}")
    print(f"   æ¸¬è©¦æˆåŠŸç‡: {result['test_success_rate']:.1%}")
    print(f"   ç³»çµ±å¥åº·åº¦: {result['system_health']}")