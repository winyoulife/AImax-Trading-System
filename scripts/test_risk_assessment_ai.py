#!/usr/bin/env python3
"""
é¢¨éšªè©•ä¼°AIæ¸¬è©¦è…³æœ¬
æ¸¬è©¦ä»»å‹™2.1çš„å¯¦ç¾ï¼šéƒ¨ç½²é¢¨éšªè©•ä¼°AIæ¨¡å‹
"""

import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.ai.risk_assessment_ai import (
        RiskAssessmentAI, RiskLevel, RiskType, 
        get_risk_assessment_ai
    )
except ImportError as e:
    print(f"âŒ å°å…¥éŒ¯èª¤: {e}")
    print("è«‹ç¢ºä¿åœ¨AImaxé …ç›®æ ¹ç›®éŒ„ä¸‹é‹è¡Œæ­¤è…³æœ¬")
    sys.exit(1)

import logging
import json
from datetime import datetime

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RiskAssessmentAITester:
    """é¢¨éšªè©•ä¼°AIæ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.ai = None
        self.test_results = {}
    
    async def run_comprehensive_tests(self):
        """é‹è¡Œå…¨é¢æ¸¬è©¦"""
        print("ğŸ§ª é–‹å§‹é¢¨éšªè©•ä¼°AIç³»çµ±æ¸¬è©¦...")
        print("=" * 70)
        
        try:
            # æ¸¬è©¦1: AIæ¨¡å‹åˆå§‹åŒ–
            await self.test_ai_initialization()
            
            # æ¸¬è©¦2: å–®å€‹äº¤æ˜“å°é¢¨éšªè©•ä¼°
            await self.test_single_pair_risk_assessment()
            
            # æ¸¬è©¦3: é¢¨éšªæŒ‡æ¨™è¨ˆç®—
            await self.test_risk_metrics_calculation()
            
            # æ¸¬è©¦4: é¢¨éšªå› å­åˆ†æ
            await self.test_risk_factor_analysis()
            
            # æ¸¬è©¦5: å¤šäº¤æ˜“å°é¢¨éšªè©•ä¼°
            await self.test_multi_pair_risk_assessment()
            
            # æ¸¬è©¦6: çµ„åˆé¢¨éšªåˆ†æ
            await self.test_portfolio_risk_analysis()
            
            # æ¸¬è©¦7: AIæ¨ç†å’Œæ±ºç­–é‚è¼¯
            await self.test_ai_reasoning_logic()
            
            # ç”Ÿæˆæ¸¬è©¦å ±å‘Š
            self.generate_test_report()
            
        except Exception as e:
            logger.error(f"âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    async def test_ai_initialization(self):
        """æ¸¬è©¦1: AIæ¨¡å‹åˆå§‹åŒ–"""
        print("\nğŸ¤– æ¸¬è©¦1: AIæ¨¡å‹åˆå§‹åŒ–")
        print("-" * 50)
        
        try:
            # å‰µå»ºé¢¨éšªè©•ä¼°AIå¯¦ä¾‹
            self.ai = RiskAssessmentAI(model_name="qwen2.5:7b")
            
            # æª¢æŸ¥åˆå§‹åŒ–ç‹€æ…‹
            initialization_checks = {
                'ai_instance_created': self.ai is not None,
                'model_name_set': self.ai.model_name == "qwen2.5:7b",
                'risk_thresholds_configured': len(self.ai.risk_thresholds) == 5,
                'risk_weights_configured': len(self.ai.risk_weights) == 6,
                'config_manager_connected': self.ai.config_manager is not None
            }
            
            all_checks_passed = all(initialization_checks.values())
            
            print(f"âœ… AIåˆå§‹åŒ–: {'æˆåŠŸ' if all_checks_passed else 'éƒ¨åˆ†å¤±æ•—'}")
            for check, status in initialization_checks.items():
                status_icon = "âœ…" if status else "âŒ"
                print(f"   {status_icon} {check}: {status}")
            
            # æª¢æŸ¥é¢¨éšªç­‰ç´šé…ç½®
            print(f"ğŸ“Š é¢¨éšªç­‰ç´šé…ç½®:")
            for level, (min_val, max_val) in self.ai.risk_thresholds.items():
                print(f"   - {level.value}: {min_val}-{max_val}")
            
            # æª¢æŸ¥é¢¨éšªæ¬Šé‡é…ç½®
            print(f"âš–ï¸ é¢¨éšªæ¬Šé‡é…ç½®:")
            for risk_type, weight in self.ai.risk_weights.items():
                print(f"   - {risk_type.value}: {weight:.2f}")
            
            self.test_results['ai_initialization'] = {
                'status': 'success' if all_checks_passed else 'partial',
                'initialization_checks': initialization_checks,
                'risk_levels_count': len(self.ai.risk_thresholds),
                'risk_weights_count': len(self.ai.risk_weights)
            }
            
        except Exception as e:
            print(f"âŒ AIåˆå§‹åŒ–æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results['ai_initialization'] = {
                'status': 'failed',
                'error': str(e)
            }
    
    async def test_single_pair_risk_assessment(self):
        """æ¸¬è©¦2: å–®å€‹äº¤æ˜“å°é¢¨éšªè©•ä¼°"""
        print("\nğŸ“Š æ¸¬è©¦2: å–®å€‹äº¤æ˜“å°é¢¨éšªè©•ä¼°")
        print("-" * 50)
        
        try:
            test_pairs = ["BTCTWD", "ETHTWD", "LTCTWD"]
            assessment_results = {}
            
            for pair in test_pairs:
                print(f"   ğŸ” è©•ä¼° {pair} é¢¨éšª...")
                
                start_time = datetime.now()
                result = await self.ai.assess_risk(pair, timeframe='5m', lookback_periods=100)
                end_time = datetime.now()
                
                assessment_time = (end_time - start_time).total_seconds()
                
                assessment_results[pair] = {
                    'result': result,
                    'assessment_time': assessment_time,
                    'success': True
                }
                
                print(f"      âœ… é¢¨éšªç­‰ç´š: {result.risk_metrics.overall_risk_level.value}")
                print(f"      ğŸ“Š é¢¨éšªè©•åˆ†: {result.risk_metrics.risk_score:.1f}/100")
                print(f"      ğŸ’° å»ºè­°å€‰ä½: {result.recommended_position_size:.4f}")
                print(f"      ğŸ›¡ï¸ å»ºè­°æ­¢æ: {result.stop_loss_suggestion:.4f}")
                print(f"      â±ï¸ è©•ä¼°æ™‚é–“: {assessment_time:.3f}ç§’")
                print(f"      ğŸ¤– AIä¿¡å¿ƒåº¦: {result.ai_confidence:.2f}")
            
            # æª¢æŸ¥è©•ä¼°çµæœçš„å®Œæ•´æ€§
            completeness_checks = {}
            for pair, assessment in assessment_results.items():
                result = assessment['result']
                completeness_checks[pair] = {
                    'has_risk_metrics': result.risk_metrics is not None,
                    'has_risk_factors': len(result.risk_factors) > 0,
                    'has_warnings': isinstance(result.risk_warnings, list),
                    'has_recommendations': isinstance(result.risk_recommendations, list),
                    'has_position_suggestion': result.recommended_position_size > 0,
                    'has_ai_reasoning': len(result.assessment_reasoning) > 0
                }
            
            success_count = len([r for r in assessment_results.values() if r['success']])
            avg_assessment_time = sum(r['assessment_time'] for r in assessment_results.values()) / len(assessment_results)
            
            print(f"ğŸ“ˆ å–®å€‹äº¤æ˜“å°è©•ä¼°çµ±è¨ˆ:")
            print(f"   - æˆåŠŸè©•ä¼°: {success_count}/{len(test_pairs)}")
            print(f"   - å¹³å‡è©•ä¼°æ™‚é–“: {avg_assessment_time:.3f}ç§’")
            print(f"   - è©•ä¼°å®Œæ•´æ€§: æ‰€æœ‰å­—æ®µå®Œæ•´")
            
            self.test_results['single_pair_risk_assessment'] = {
                'status': 'success' if success_count == len(test_pairs) else 'partial',
                'assessment_results': {k: {'success': v['success'], 'time': v['assessment_time']} 
                                     for k, v in assessment_results.items()},
                'success_count': success_count,
                'total_pairs': len(test_pairs),
                'avg_assessment_time': avg_assessment_time,
                'completeness_checks': completeness_checks
            }
            
        except Exception as e:
            print(f"âŒ å–®å€‹äº¤æ˜“å°é¢¨éšªè©•ä¼°æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results['single_pair_risk_assessment'] = {
                'status': 'failed',
                'error': str(e)
            }    

    async def test_risk_metrics_calculation(self):
        """æ¸¬è©¦3: é¢¨éšªæŒ‡æ¨™è¨ˆç®—"""
        print("\nğŸ“ æ¸¬è©¦3: é¢¨éšªæŒ‡æ¨™è¨ˆç®—")
        print("-" * 50)
        
        try:
            # æ¸¬è©¦é¢¨éšªæŒ‡æ¨™è¨ˆç®—çš„æº–ç¢ºæ€§
            test_pair = "BTCTWD"
            
            # ç²å–å¸‚å ´æ•¸æ“š
            market_data = await self.ai._get_market_data(test_pair, '5m', 100)
            
            if market_data.empty:
                print(f"   âš ï¸ ç„¡æ³•ç²å– {test_pair} å¸‚å ´æ•¸æ“šï¼Œä½¿ç”¨æ¨¡æ“¬æ•¸æ“š")
            
            # è¨ˆç®—é¢¨éšªæŒ‡æ¨™
            risk_metrics = self.ai._calculate_risk_metrics(test_pair, market_data)
            
            # é©—è­‰é¢¨éšªæŒ‡æ¨™çš„åˆç†æ€§
            metrics_validation = {
                'volatility_reasonable': 0 <= risk_metrics.volatility <= 5,  # å¹´åŒ–æ³¢å‹•ç‡æ‡‰åœ¨åˆç†ç¯„åœ
                'var_negative': risk_metrics.var_95 <= 0,  # VaRæ‡‰ç‚ºè² å€¼
                'max_drawdown_negative': risk_metrics.max_drawdown <= 0,  # æœ€å¤§å›æ’¤æ‡‰ç‚ºè² å€¼
                'rsi_in_range': 0 <= risk_metrics.rsi <= 100,  # RSIæ‡‰åœ¨0-100ç¯„åœ
                'risk_score_in_range': 0 <= risk_metrics.risk_score <= 100,  # é¢¨éšªè©•åˆ†æ‡‰åœ¨0-100
                'confidence_in_range': 0 <= risk_metrics.confidence <= 1,  # ä¿¡å¿ƒåº¦æ‡‰åœ¨0-1
                'bollinger_position_in_range': 0 <= risk_metrics.bollinger_position <= 1  # å¸ƒæ—å¸¶ä½ç½®æ‡‰åœ¨0-1
            }
            
            all_metrics_valid = all(metrics_validation.values())
            
            print(f"âœ… é¢¨éšªæŒ‡æ¨™è¨ˆç®—: {'æˆåŠŸ' if all_metrics_valid else 'éƒ¨åˆ†ç•°å¸¸'}")
            print(f"   ğŸ“Š æ³¢å‹•ç‡: {risk_metrics.volatility:.4f}")
            print(f"   ğŸ“‰ VaR(95%): {risk_metrics.var_95:.4f}")
            print(f"   ğŸ“ˆ æœ€å¤§å›æ’¤: {risk_metrics.max_drawdown:.4f}")
            print(f"   ğŸ“Š å¤æ™®æ¯”ç‡: {risk_metrics.sharpe_ratio:.2f}")
            print(f"   ğŸ“Š RSI: {risk_metrics.rsi:.1f}")
            print(f"   ğŸ“Š é¢¨éšªè©•åˆ†: {risk_metrics.risk_score:.1f}")
            print(f"   ğŸ“Š é¢¨éšªç­‰ç´š: {risk_metrics.overall_risk_level.value}")
            
            # æª¢æŸ¥æŒ‡æ¨™é©—è­‰çµæœ
            for check, valid in metrics_validation.items():
                status_icon = "âœ…" if valid else "âŒ"
                print(f"   {status_icon} {check}: {valid}")
            
            self.test_results['risk_metrics_calculation'] = {
                'status': 'success' if all_metrics_valid else 'partial',
                'metrics_validation': metrics_validation,
                'calculated_metrics': {
                    'volatility': risk_metrics.volatility,
                    'var_95': risk_metrics.var_95,
                    'max_drawdown': risk_metrics.max_drawdown,
                    'rsi': risk_metrics.rsi,
                    'risk_score': risk_metrics.risk_score
                }
            }
            
        except Exception as e:
            print(f"âŒ é¢¨éšªæŒ‡æ¨™è¨ˆç®—æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results['risk_metrics_calculation'] = {
                'status': 'failed',
                'error': str(e)
            }
    
    async def test_risk_factor_analysis(self):
        """æ¸¬è©¦4: é¢¨éšªå› å­åˆ†æ"""
        print("\nğŸ” æ¸¬è©¦4: é¢¨éšªå› å­åˆ†æ")
        print("-" * 50)
        
        try:
            test_pair = "BTCTWD"
            
            # ç²å–é¢¨éšªè©•ä¼°çµæœ
            result = await self.ai.assess_risk(test_pair)
            risk_factors = result.risk_factors
            
            # æª¢æŸ¥é¢¨éšªå› å­å®Œæ•´æ€§
            expected_risk_types = set(RiskType)
            actual_risk_types = set(risk_factors.keys())
            
            factor_completeness = expected_risk_types == actual_risk_types
            
            print(f"âœ… é¢¨éšªå› å­åˆ†æ: {'å®Œæ•´' if factor_completeness else 'ä¸å®Œæ•´'}")
            print(f"   ğŸ“Š é¢¨éšªå› å­æ•¸é‡: {len(risk_factors)}/{len(RiskType)}")
            
            # é¡¯ç¤ºå„é¢¨éšªå› å­
            for risk_type, score in risk_factors.items():
                risk_level = "é«˜" if score > 70 else "ä¸­" if score > 40 else "ä½"
                print(f"   ğŸ“Š {risk_type.value}: {score:.1f} ({risk_level})")
            
            # æª¢æŸ¥é¢¨éšªå› å­æ•¸å€¼åˆç†æ€§
            factor_validation = {
                'all_factors_present': factor_completeness,
                'scores_in_range': all(0 <= score <= 100 for score in risk_factors.values()),
                'market_risk_calculated': RiskType.MARKET_RISK in risk_factors,
                'technical_risk_calculated': RiskType.TECHNICAL_RISK in risk_factors
            }
            
            all_factors_valid = all(factor_validation.values())
            
            for check, valid in factor_validation.items():
                status_icon = "âœ…" if valid else "âŒ"
                print(f"   {status_icon} {check}: {valid}")
            
            self.test_results['risk_factor_analysis'] = {
                'status': 'success' if all_factors_valid else 'partial',
                'factor_validation': factor_validation,
                'risk_factors': {k.value: v for k, v in risk_factors.items()},
                'factor_count': len(risk_factors)
            }
            
        except Exception as e:
            print(f"âŒ é¢¨éšªå› å­åˆ†ææ¸¬è©¦å¤±æ•—: {e}")
            self.test_results['risk_factor_analysis'] = {
                'status': 'failed',
                'error': str(e)
            }
    
    async def test_multi_pair_risk_assessment(self):
        """æ¸¬è©¦5: å¤šäº¤æ˜“å°é¢¨éšªè©•ä¼°"""
        print("\nğŸ”„ æ¸¬è©¦5: å¤šäº¤æ˜“å°é¢¨éšªè©•ä¼°")
        print("-" * 50)
        
        try:
            test_pairs = ["BTCTWD", "ETHTWD", "LTCTWD", "BCHTWD"]
            
            print(f"   ğŸ” è©•ä¼° {len(test_pairs)} å€‹äº¤æ˜“å°...")
            
            start_time = datetime.now()
            results = await self.ai.assess_multi_pair_risk(test_pairs)
            end_time = datetime.now()
            
            total_time = (end_time - start_time).total_seconds()
            
            success_count = len(results)
            success_rate = success_count / len(test_pairs)
            
            print(f"âœ… å¤šäº¤æ˜“å°è©•ä¼°: {success_count}/{len(test_pairs)} æˆåŠŸ")
            print(f"   â±ï¸ ç¸½è©•ä¼°æ™‚é–“: {total_time:.3f}ç§’")
            print(f"   ğŸ“Š æˆåŠŸç‡: {success_rate:.1%}")
            print(f"   âš¡ å¹³å‡æ¯å°æ™‚é–“: {total_time/len(test_pairs):.3f}ç§’")
            
            # æª¢æŸ¥çµæœå®Œæ•´æ€§
            for pair, assessment in results.items():
                risk_level = assessment.risk_metrics.overall_risk_level.value
                risk_score = assessment.risk_metrics.risk_score
                print(f"   ğŸ“Š {pair}: {risk_level} ({risk_score:.1f})")
            
            # æª¢æŸ¥ç›¸é—œæ€§åˆ†æ
            correlation_analysis = self.ai._analyze_cross_pair_correlation(results)
            correlation_complete = len(correlation_analysis) == len(results)
            
            print(f"   ğŸ”— ç›¸é—œæ€§åˆ†æ: {'å®Œæˆ' if correlation_complete else 'ä¸å®Œæ•´'}")
            
            self.test_results['multi_pair_risk_assessment'] = {
                'status': 'success' if success_rate > 0.8 else 'partial',
                'success_count': success_count,
                'total_pairs': len(test_pairs),
                'success_rate': success_rate,
                'total_time': total_time,
                'correlation_analysis_complete': correlation_complete
            }
            
        except Exception as e:
            print(f"âŒ å¤šäº¤æ˜“å°é¢¨éšªè©•ä¼°æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results['multi_pair_risk_assessment'] = {
                'status': 'failed',
                'error': str(e)
            }
    
    async def test_portfolio_risk_analysis(self):
        """æ¸¬è©¦6: çµ„åˆé¢¨éšªåˆ†æ"""
        print("\nğŸ“ˆ æ¸¬è©¦6: çµ„åˆé¢¨éšªåˆ†æ")
        print("-" * 50)
        
        try:
            # ç²å–å¤šäº¤æ˜“å°è©•ä¼°çµæœ
            test_pairs = ["BTCTWD", "ETHTWD", "LTCTWD"]
            results = await self.ai.assess_multi_pair_risk(test_pairs)
            
            if not results:
                print("   âš ï¸ ç„¡æ³•ç²å–äº¤æ˜“å°è©•ä¼°çµæœï¼Œè·³éçµ„åˆåˆ†æ")
                return
            
            # ç”Ÿæˆçµ„åˆé¢¨éšªæ‘˜è¦
            summary = self.ai.get_portfolio_risk_summary(results)
            
            if not summary:
                print("   âŒ çµ„åˆé¢¨éšªæ‘˜è¦ç”Ÿæˆå¤±æ•—")
                return
            
            print(f"âœ… çµ„åˆé¢¨éšªåˆ†æå®Œæˆ")
            print(f"   ğŸ“Š äº¤æ˜“å°æ•¸é‡: {summary.get('total_pairs')}")
            print(f"   ğŸ“Š çµ„åˆé¢¨éšªç­‰ç´š: {summary.get('portfolio_risk_level')}")
            print(f"   ğŸ“Š å¹³å‡é¢¨éšªè©•åˆ†: {summary.get('average_risk_score')}")
            print(f"   ğŸ“Š æœ€é«˜é¢¨éšªè©•åˆ†: {summary.get('max_risk_score')}")
            print(f"   ğŸ“Š æœ€ä½é¢¨éšªè©•åˆ†: {summary.get('min_risk_score')}")
            print(f"   ğŸ“Š åˆ†æ•£åŒ–è©•åˆ†: {summary.get('diversification_score')}")
            print(f"   ğŸ“Š å»ºè­°ç¸½å€‰ä½: {summary.get('total_recommended_position')}")
            
            # é¡¯ç¤ºé¢¨éšªåˆ†å¸ƒ
            risk_distribution = summary.get('risk_distribution', {})
            print(f"   ğŸ“Š é¢¨éšªåˆ†å¸ƒ:")
            for level, count in risk_distribution.items():
                if count > 0:
                    print(f"      - {level}: {count}å€‹")
            
            # é¡¯ç¤ºé«˜é¢¨éšªäº¤æ˜“å°
            high_risk_pairs = summary.get('high_risk_pairs', [])
            if high_risk_pairs:
                print(f"   âš ï¸ é«˜é¢¨éšªäº¤æ˜“å°: {', '.join(high_risk_pairs)}")
            
            # æª¢æŸ¥æ‘˜è¦å®Œæ•´æ€§
            summary_completeness = {
                'has_total_pairs': 'total_pairs' in summary,
                'has_risk_level': 'portfolio_risk_level' in summary,
                'has_avg_score': 'average_risk_score' in summary,
                'has_diversification': 'diversification_score' in summary,
                'has_risk_distribution': 'risk_distribution' in summary
            }
            
            all_complete = all(summary_completeness.values())
            
            for check, complete in summary_completeness.items():
                status_icon = "âœ…" if complete else "âŒ"
                print(f"   {status_icon} {check}: {complete}")
            
            self.test_results['portfolio_risk_analysis'] = {
                'status': 'success' if all_complete else 'partial',
                'summary_completeness': summary_completeness,
                'portfolio_summary': summary
            }
            
        except Exception as e:
            print(f"âŒ çµ„åˆé¢¨éšªåˆ†ææ¸¬è©¦å¤±æ•—: {e}")
            self.test_results['portfolio_risk_analysis'] = {
                'status': 'failed',
                'error': str(e)
            }
    
    async def test_ai_reasoning_logic(self):
        """æ¸¬è©¦7: AIæ¨ç†å’Œæ±ºç­–é‚è¼¯"""
        print("\nğŸ§  æ¸¬è©¦7: AIæ¨ç†å’Œæ±ºç­–é‚è¼¯")
        print("-" * 50)
        
        try:
            test_pair = "BTCTWD"
            
            # ç²å–å®Œæ•´çš„é¢¨éšªè©•ä¼°çµæœ
            result = await self.ai.assess_risk(test_pair)
            
            # æª¢æŸ¥AIæ¨ç†çµæœ
            reasoning_checks = {
                'has_reasoning': len(result.assessment_reasoning) > 0,
                'has_confidence': 0 <= result.ai_confidence <= 1,
                'has_warnings': isinstance(result.risk_warnings, list),
                'has_recommendations': isinstance(result.risk_recommendations, list),
                'position_reasonable': 0 < result.recommended_position_size <= 1,
                'stop_loss_reasonable': 0 < result.stop_loss_suggestion <= 1
            }
            
            all_reasoning_valid = all(reasoning_checks.values())
            
            print(f"âœ… AIæ¨ç†é‚è¼¯: {'å®Œæ•´' if all_reasoning_valid else 'éƒ¨åˆ†ç¼ºå¤±'}")
            print(f"   ğŸ¤– AIä¿¡å¿ƒåº¦: {result.ai_confidence:.2f}")
            print(f"   ğŸ’­ æ¨ç†å…§å®¹é•·åº¦: {len(result.assessment_reasoning)}å­—ç¬¦")
            print(f"   âš ï¸ é¢¨éšªè­¦å‘Šæ•¸é‡: {len(result.risk_warnings)}")
            print(f"   ğŸ’¡ å»ºè­°æ•¸é‡: {len(result.risk_recommendations)}")
            print(f"   ğŸ’° å»ºè­°å€‰ä½: {result.recommended_position_size:.4f}")
            print(f"   ğŸ›¡ï¸ å»ºè­°æ­¢æ: {result.stop_loss_suggestion:.4f}")
            
            # é¡¯ç¤ºæ¨ç†å…§å®¹æ‘˜è¦
            if result.assessment_reasoning:
                reasoning_preview = result.assessment_reasoning[:100] + "..." if len(result.assessment_reasoning) > 100 else result.assessment_reasoning
                print(f"   ğŸ’­ æ¨ç†æ‘˜è¦: {reasoning_preview}")
            
            # é¡¯ç¤ºé¢¨éšªè­¦å‘Š
            if result.risk_warnings:
                print(f"   âš ï¸ é¢¨éšªè­¦å‘Š:")
                for warning in result.risk_warnings[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                    print(f"      - {warning}")
            
            # é¡¯ç¤ºå»ºè­°
            if result.risk_recommendations:
                print(f"   ğŸ’¡ é¢¨éšªå»ºè­°:")
                for recommendation in result.risk_recommendations[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                    print(f"      - {recommendation}")
            
            # æª¢æŸ¥æ¨ç†é‚è¼¯
            for check, valid in reasoning_checks.items():
                status_icon = "âœ…" if valid else "âŒ"
                print(f"   {status_icon} {check}: {valid}")
            
            self.test_results['ai_reasoning_logic'] = {
                'status': 'success' if all_reasoning_valid else 'partial',
                'reasoning_checks': reasoning_checks,
                'ai_confidence': result.ai_confidence,
                'reasoning_length': len(result.assessment_reasoning),
                'warnings_count': len(result.risk_warnings),
                'recommendations_count': len(result.risk_recommendations)
            }
            
        except Exception as e:
            print(f"âŒ AIæ¨ç†é‚è¼¯æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results['ai_reasoning_logic'] = {
                'status': 'failed',
                'error': str(e)
            }
    
    def generate_test_report(self):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        print("\nğŸ“‹ æ¸¬è©¦å ±å‘Šç”Ÿæˆ")
        print("=" * 70)
        
        try:
            # è¨ˆç®—ç¸½é«”æ¸¬è©¦çµæœ
            total_tests = len(self.test_results)
            successful_tests = len([r for r in self.test_results.values() if r.get('status') == 'success'])
            partial_tests = len([r for r in self.test_results.values() if r.get('status') == 'partial'])
            failed_tests = len([r for r in self.test_results.values() if r.get('status') == 'failed'])
            
            success_rate = successful_tests / total_tests if total_tests > 0 else 0
            
            print(f"ğŸ“Š æ¸¬è©¦çµ±è¨ˆ:")
            print(f"   - ç¸½æ¸¬è©¦æ•¸: {total_tests}")
            print(f"   - æˆåŠŸ: {successful_tests}")
            print(f"   - éƒ¨åˆ†æˆåŠŸ: {partial_tests}")
            print(f"   - å¤±æ•—: {failed_tests}")
            print(f"   - æˆåŠŸç‡: {success_rate:.1%}")
            
            # è©³ç´°æ¸¬è©¦çµæœ
            print(f"\nğŸ“‹ è©³ç´°çµæœ:")
            for test_name, result in self.test_results.items():
                status = result.get('status', 'unknown')
                status_icon = {"success": "âœ…", "partial": "âš ï¸", "failed": "âŒ"}.get(status, "â“")
                print(f"   {status_icon} {test_name}: {status}")
                
                if 'error' in result:
                    print(f"      éŒ¯èª¤: {result['error']}")
            
            # ä¿å­˜æ¸¬è©¦å ±å‘Šåˆ°æ–‡ä»¶
            report_data = {
                'timestamp': datetime.now().isoformat(),
                'test_summary': {
                    'total_tests': total_tests,
                    'successful_tests': successful_tests,
                    'partial_tests': partial_tests,
                    'failed_tests': failed_tests,
                    'success_rate': success_rate
                },
                'detailed_results': self.test_results
            }
            
            report_file = project_root / 'logs' / f'risk_assessment_ai_test_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
            report_file.parent.mkdir(exist_ok=True)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"\nğŸ’¾ æ¸¬è©¦å ±å‘Šå·²ä¿å­˜: {report_file}")
            
            # ç¸½çµ
            if success_rate >= 0.8:
                print(f"\nğŸ‰ é¢¨éšªè©•ä¼°AIæ¸¬è©¦æ•´é«”æˆåŠŸï¼")
                print(f"   ç³»çµ±å·²æº–å‚™å¥½é€²è¡Œä¸‹ä¸€éšæ®µé–‹ç™¼")
            elif success_rate >= 0.6:
                print(f"\nâš ï¸ é¢¨éšªè©•ä¼°AIæ¸¬è©¦éƒ¨åˆ†æˆåŠŸ")
                print(f"   å»ºè­°ä¿®å¾©å¤±æ•—çš„æ¸¬è©¦é …ç›®å¾Œç¹¼çºŒ")
            else:
                print(f"\nâŒ é¢¨éšªè©•ä¼°AIæ¸¬è©¦å­˜åœ¨è¼ƒå¤šå•é¡Œ")
                print(f"   å»ºè­°æª¢æŸ¥å¯¦ç¾ä¸¦ä¿®å¾©å•é¡Œ")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ¸¬è©¦å ±å‘Šå¤±æ•—: {e}")


async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ å•Ÿå‹•é¢¨éšªè©•ä¼°AIæ¸¬è©¦ç³»çµ±")
    print("=" * 70)
    
    tester = RiskAssessmentAITester()
    await tester.run_comprehensive_tests()
    
    print("\nğŸ æ¸¬è©¦å®Œæˆ")


if __name__ == "__main__":
    asyncio.run(main())