#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AImax æ€§èƒ½å„ªåŒ–æ¸¬è©¦è…³æœ¬
æ¸¬è©¦ä¸‰AIå”ä½œç³»çµ±çš„æ€§èƒ½å„ªåŒ–æ•ˆæœ
"""

import asyncio
import logging
import time
import sys
import os
from datetime import datetime
from typing import Dict, Any, List

# æ·»åŠ AImaxè·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.optimization.performance_optimizer import create_performance_optimizer
from src.ai.ai_manager import AICollaborationManager
from src.data.market_enhancer import MarketDataEnhancer

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PerformanceTestSuite:
    """æ€§èƒ½æ¸¬è©¦å¥—ä»¶"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¸¬è©¦å¥—ä»¶"""
        self.optimizer = create_performance_optimizer()
        self.test_results = []
        
        # æ¸¬è©¦æ•¸æ“š
        self.test_market_data = {
            'current_price': 1500000,
            'price_change_1m': 0.5,
            'volume_ratio': 1.1,
            'ai_formatted_data': 'å¸‚å ´å‘ˆç¾ä¸Šæ¼²è¶¨å‹¢ï¼Œæˆäº¤é‡æ”¾å¤§'
        }
        
        logger.info("ğŸ§ª æ€§èƒ½æ¸¬è©¦å¥—ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """é‹è¡Œæ‰€æœ‰æ€§èƒ½æ¸¬è©¦"""
        try:
            logger.info("ğŸš€ é–‹å§‹AImaxæ€§èƒ½å„ªåŒ–æ¸¬è©¦...")
            
            # å•Ÿå‹•æ€§èƒ½ç›£æ§
            self.optimizer.start_monitoring()
            
            # æ¸¬è©¦çµæœæ”¶é›†
            test_results = {}
            
            # 1. AIæ¨ç†æ€§èƒ½æ¸¬è©¦
            logger.info("ğŸ¤– æ¸¬è©¦AIæ¨ç†æ€§èƒ½å„ªåŒ–...")
            ai_result = await self.test_ai_inference_optimization()
            test_results['ai_inference'] = ai_result
            
            # 2. æ•¸æ“šè™•ç†æ€§èƒ½æ¸¬è©¦
            logger.info("ğŸ“Š æ¸¬è©¦æ•¸æ“šè™•ç†æ€§èƒ½å„ªåŒ–...")
            data_result = await self.test_data_processing_optimization()
            test_results['data_processing'] = data_result
            
            # 3. å…§å­˜ä½¿ç”¨å„ªåŒ–æ¸¬è©¦
            logger.info("ğŸ’¾ æ¸¬è©¦å…§å­˜ä½¿ç”¨å„ªåŒ–...")
            memory_result = await self.test_memory_optimization()
            test_results['memory_usage'] = memory_result
            
            # 4. ä¸¦è¡Œè™•ç†æ¸¬è©¦
            logger.info("âš¡ æ¸¬è©¦ä¸¦è¡Œè™•ç†æ€§èƒ½...")
            parallel_result = await self.test_parallel_processing()
            test_results['parallel_processing'] = parallel_result
            
            # 5. ç·©å­˜ç³»çµ±æ¸¬è©¦
            logger.info("ğŸ¯ æ¸¬è©¦ç·©å­˜ç³»çµ±æ€§èƒ½...")
            cache_result = await self.test_cache_performance()
            test_results['cache_performance'] = cache_result
            
            # 6. æ•´é«”ç³»çµ±æ€§èƒ½æ¸¬è©¦
            logger.info("ğŸ† æ¸¬è©¦æ•´é«”ç³»çµ±æ€§èƒ½...")
            system_result = await self.test_overall_system_performance()
            test_results['overall_system'] = system_result
            
            # åœæ­¢ç›£æ§
            self.optimizer.stop_monitoring()
            
            # ç”Ÿæˆæ¸¬è©¦å ±å‘Š
            report = self.generate_test_report(test_results)
            
            logger.info("âœ… æ‰€æœ‰æ€§èƒ½æ¸¬è©¦å®Œæˆ!")
            return report
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
            return {'error': str(e), 'timestamp': datetime.now()}
    
    async def test_ai_inference_optimization(self) -> Dict[str, Any]:
        """æ¸¬è©¦AIæ¨ç†æ€§èƒ½å„ªåŒ–"""
        try:
            # å‰µå»ºæ¨¡æ“¬AIç®¡ç†å™¨
            ai_manager = None  # åœ¨å¯¦éš›æ¸¬è©¦ä¸­æœƒä½¿ç”¨çœŸå¯¦çš„AIç®¡ç†å™¨
            
            # åŸ·è¡ŒAIæ¨ç†å„ªåŒ–æ¸¬è©¦
            result = await self.optimizer.optimize_ai_inference_parallel(
                ai_manager, self.test_market_data
            )
            
            return {
                'success': result.success,
                'original_time': result.original_time,
                'optimized_time': result.optimized_time,
                'improvement_ratio': result.improvement_ratio,
                'optimization_method': result.optimization_method,
                'meets_target': result.details.get('meets_target', False),
                'target_time': result.details.get('target_time', 30.0),
                'parallel_enabled': result.details.get('parallel_enabled', True)
            }
            
        except Exception as e:
            logger.error(f"âŒ AIæ¨ç†å„ªåŒ–æ¸¬è©¦å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}
    
    async def test_data_processing_optimization(self) -> Dict[str, Any]:
        """æ¸¬è©¦æ•¸æ“šè™•ç†æ€§èƒ½å„ªåŒ–"""
        try:
            # å‰µå»ºæ¨¡æ“¬æ•¸æ“šç®¡ç†å™¨
            data_manager = None  # åœ¨å¯¦éš›æ¸¬è©¦ä¸­æœƒä½¿ç”¨çœŸå¯¦çš„æ•¸æ“šç®¡ç†å™¨
            
            # åŸ·è¡Œæ•¸æ“šè™•ç†å„ªåŒ–æ¸¬è©¦
            result = await self.optimizer.optimize_data_processing(data_manager)
            
            return {
                'success': result.success,
                'original_time': result.original_time,
                'optimized_time': result.optimized_time,
                'improvement_ratio': result.improvement_ratio,
                'optimization_method': result.optimization_method,
                'meets_target': result.details.get('meets_target', False),
                'target_time': result.details.get('target_time', 5.0)
            }
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šè™•ç†å„ªåŒ–æ¸¬è©¦å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}
    
    async def test_memory_optimization(self) -> Dict[str, Any]:
        """æ¸¬è©¦å…§å­˜ä½¿ç”¨å„ªåŒ–"""
        try:
            # å…ˆå‰µå»ºä¸€äº›å…§å­˜ä½¿ç”¨
            await self._create_memory_load()
            
            # åŸ·è¡Œå…§å­˜å„ªåŒ–
            result = self.optimizer.optimize_memory_usage()
            
            return {
                'success': result.success,
                'original_memory_mb': result.original_time,  # ç”¨æ™‚é–“å­—æ®µå­˜å„²å…§å­˜
                'optimized_memory_mb': result.optimized_time,
                'memory_saved_mb': result.details.get('memory_saved_mb', 0),
                'improvement_ratio': result.improvement_ratio,
                'meets_target': result.details.get('meets_target', False),
                'optimizations_applied': result.details.get('optimizations_applied', [])
            }
            
        except Exception as e:
            logger.error(f"âŒ å…§å­˜å„ªåŒ–æ¸¬è©¦å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}
    
    async def _create_memory_load(self):
        """å‰µå»ºå…§å­˜è² è¼‰ç”¨æ–¼æ¸¬è©¦"""
        try:
            # å¡«å……AIç·©å­˜
            for i in range(50):
                cache_key = f"test_cache_{i}"
                self.optimizer.ai_cache[cache_key] = {
                    'test_data': [j for j in range(100)],
                    'timestamp': datetime.now()
                }
            
            # å¡«å……æ€§èƒ½æŒ‡æ¨™æ­·å²
            for i in range(600):
                from src.optimization.performance_optimizer import PerformanceMetrics
                metrics = PerformanceMetrics(
                    component_name=f"test_component_{i}",
                    execution_time=i * 0.1,
                    memory_usage=100 + i,
                    cpu_usage=50 + (i % 50),
                    throughput=0.8,
                    success_rate=0.9,
                    timestamp=datetime.now()
                )
                self.optimizer.metrics_history.append(metrics)
            
            logger.info("ğŸ“ˆ å‰µå»ºå…§å­˜è² è¼‰å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å‰µå»ºå…§å­˜è² è¼‰å¤±æ•—: {e}")
    
    async def test_parallel_processing(self) -> Dict[str, Any]:
        """æ¸¬è©¦ä¸¦è¡Œè™•ç†æ€§èƒ½"""
        try:
            # æ¸¬è©¦é †åºåŸ·è¡Œ
            start_time = time.time()
            sequential_results = []
            for i in range(3):
                result = await self._simulate_task(f"task_{i}", delay=0.5)
                sequential_results.append(result)
            sequential_time = time.time() - start_time
            
            # æ¸¬è©¦ä¸¦è¡ŒåŸ·è¡Œ
            start_time = time.time()
            tasks = [self._simulate_task(f"parallel_task_{i}", delay=0.5) for i in range(3)]
            parallel_results = await asyncio.gather(*tasks)
            parallel_time = time.time() - start_time
            
            # è¨ˆç®—æ”¹é€²
            improvement_ratio = (sequential_time - parallel_time) / sequential_time if sequential_time > 0 else 0
            
            return {
                'success': improvement_ratio > 0.5,  # ä¸¦è¡Œæ‡‰è©²æœ‰é¡¯è‘—æ”¹é€²
                'sequential_time': sequential_time,
                'parallel_time': parallel_time,
                'improvement_ratio': improvement_ratio,
                'tasks_completed': len(parallel_results),
                'parallel_efficiency': improvement_ratio
            }
            
        except Exception as e:
            logger.error(f"âŒ ä¸¦è¡Œè™•ç†æ¸¬è©¦å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}
    
    async def _simulate_task(self, task_name: str, delay: float = 1.0) -> Dict[str, Any]:
        """æ¨¡æ“¬ä»»å‹™åŸ·è¡Œ"""
        await asyncio.sleep(delay)
        return {
            'task_name': task_name,
            'execution_time': delay,
            'success': True
        }
    
    async def test_cache_performance(self) -> Dict[str, Any]:
        """æ¸¬è©¦ç·©å­˜ç³»çµ±æ€§èƒ½"""
        try:
            # æ¸…ç©ºç·©å­˜çµ±è¨ˆ
            original_hits = self.optimizer.cache_hits
            original_misses = self.optimizer.cache_misses
            
            # æ¸¬è©¦ç·©å­˜æœªå‘½ä¸­ï¼ˆç¬¬ä¸€æ¬¡è¨ªå•ï¼‰
            start_time = time.time()
            result1 = await self.optimizer._execute_parallel_ai_analysis(None, self.test_market_data)
            first_access_time = time.time() - start_time
            
            # æ¸¬è©¦ç·©å­˜å‘½ä¸­ï¼ˆç¬¬äºŒæ¬¡è¨ªå•ç›¸åŒæ•¸æ“šï¼‰
            start_time = time.time()
            result2 = await self.optimizer._execute_parallel_ai_analysis(None, self.test_market_data)
            second_access_time = time.time() - start_time
            
            # è¨ˆç®—ç·©å­˜æ•ˆæœ
            cache_hits = self.optimizer.cache_hits - original_hits
            cache_misses = self.optimizer.cache_misses - original_misses
            total_requests = cache_hits + cache_misses
            hit_rate = cache_hits / max(1, total_requests)
            
            # è¨ˆç®—æ€§èƒ½æ”¹é€²
            cache_improvement = (first_access_time - second_access_time) / first_access_time if first_access_time > 0 else 0
            
            return {
                'success': hit_rate > 0,
                'cache_hits': cache_hits,
                'cache_misses': cache_misses,
                'hit_rate': hit_rate,
                'first_access_time': first_access_time,
                'second_access_time': second_access_time,
                'cache_improvement': cache_improvement,
                'cache_size': len(self.optimizer.ai_cache)
            }
            
        except Exception as e:
            logger.error(f"âŒ ç·©å­˜æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}
    
    async def test_overall_system_performance(self) -> Dict[str, Any]:
        """æ¸¬è©¦æ•´é«”ç³»çµ±æ€§èƒ½"""
        try:
            # æ¨¡æ“¬å®Œæ•´çš„äº¤æ˜“é€±æœŸ
            start_time = time.time()
            
            # 1. æ•¸æ“šç²å–å’Œè™•ç†
            data_start = time.time()
            await self.optimizer._simulate_data_processing(optimized=True)
            data_time = time.time() - data_start
            
            # 2. AIåˆ†æ
            ai_start = time.time()
            ai_results = await self.optimizer._execute_parallel_ai_analysis(None, self.test_market_data)
            ai_time = time.time() - ai_start
            
            # 3. æ±ºç­–æ•´åˆ
            decision_start = time.time()
            await asyncio.sleep(0.2)  # æ¨¡æ“¬æ±ºç­–æ•´åˆæ™‚é–“
            decision_time = time.time() - decision_start
            
            total_time = time.time() - start_time
            
            # æª¢æŸ¥æ˜¯å¦é”åˆ°æ€§èƒ½ç›®æ¨™
            target_total_time = self.optimizer.config['optimization_targets']['total_cycle_time']
            meets_target = total_time <= target_total_time
            
            return {
                'success': meets_target,
                'total_cycle_time': total_time,
                'data_processing_time': data_time,
                'ai_analysis_time': ai_time,
                'decision_integration_time': decision_time,
                'target_time': target_total_time,
                'meets_target': meets_target,
                'performance_breakdown': {
                    'data_processing_ratio': data_time / total_time,
                    'ai_analysis_ratio': ai_time / total_time,
                    'decision_integration_ratio': decision_time / total_time
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ•´é«”ç³»çµ±æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}
    
    def generate_test_report(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        try:
            # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
            total_tests = len(test_results)
            successful_tests = sum(1 for result in test_results.values() 
                                 if isinstance(result, dict) and result.get('success', False))
            
            # æ”¶é›†æ€§èƒ½æ”¹é€²æ•¸æ“š
            improvements = []
            for test_name, result in test_results.items():
                if isinstance(result, dict) and 'improvement_ratio' in result:
                    improvements.append({
                        'test': test_name,
                        'improvement': result['improvement_ratio']
                    })
            
            # è¨ˆç®—å¹³å‡æ”¹é€²
            avg_improvement = np.mean([imp['improvement'] for imp in improvements]) if improvements else 0
            
            # ç²å–æ€§èƒ½å„ªåŒ–å™¨å ±å‘Š
            optimizer_report = self.optimizer.get_performance_report()
            
            # ç”Ÿæˆæœ€çµ‚å ±å‘Š
            report = {
                'test_summary': {
                    'total_tests': total_tests,
                    'successful_tests': successful_tests,
                    'success_rate': successful_tests / max(1, total_tests),
                    'average_improvement': avg_improvement,
                    'test_timestamp': datetime.now()
                },
                'performance_improvements': improvements,
                'detailed_results': test_results,
                'optimizer_report': optimizer_report,
                'recommendations': self._generate_recommendations(test_results),
                'aimax_performance_status': self._assess_aimax_performance(test_results)
            }
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ¸¬è©¦å ±å‘Šå¤±æ•—: {e}")
            return {'error': str(e), 'timestamp': datetime.now()}
    
    def _generate_recommendations(self, test_results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        recommendations = []
        
        try:
            # AIæ¨ç†å„ªåŒ–å»ºè­°
            ai_result = test_results.get('ai_inference', {})
            if not ai_result.get('meets_target', False):
                recommendations.append("å»ºè­°å•Ÿç”¨AIä¸¦è¡Œè™•ç†ä»¥æå‡æ¨ç†é€Ÿåº¦")
            
            # æ•¸æ“šè™•ç†å„ªåŒ–å»ºè­°
            data_result = test_results.get('data_processing', {})
            if not data_result.get('meets_target', False):
                recommendations.append("å»ºè­°å¯¦æ–½æ•¸æ“šä¸¦è¡Œç²å–å’Œç·©å­˜ç­–ç•¥")
            
            # å…§å­˜ä½¿ç”¨å»ºè­°
            memory_result = test_results.get('memory_usage', {})
            if not memory_result.get('meets_target', False):
                recommendations.append("å»ºè­°å®šæœŸæ¸…ç†ç·©å­˜å’Œå„ªåŒ–å…§å­˜ä½¿ç”¨")
            
            # ç·©å­˜æ€§èƒ½å»ºè­°
            cache_result = test_results.get('cache_performance', {})
            if cache_result.get('hit_rate', 0) < 0.7:
                recommendations.append("å»ºè­°èª¿æ•´ç·©å­˜ç­–ç•¥ä»¥æé«˜å‘½ä¸­ç‡")
            
            # æ•´é«”æ€§èƒ½å»ºè­°
            system_result = test_results.get('overall_system', {})
            if not system_result.get('meets_target', False):
                recommendations.append("å»ºè­°é€²ä¸€æ­¥å„ªåŒ–ç³»çµ±æ•´é«”æ€§èƒ½")
            
            if not recommendations:
                recommendations.append("ç³»çµ±æ€§èƒ½è¡¨ç¾è‰¯å¥½ï¼Œå»ºè­°ä¿æŒç•¶å‰å„ªåŒ–ç­–ç•¥")
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå»ºè­°å¤±æ•—: {e}")
            recommendations.append("ç„¡æ³•ç”Ÿæˆå…·é«”å»ºè­°ï¼Œè«‹æª¢æŸ¥æ¸¬è©¦çµæœ")
        
        return recommendations
    
    def _assess_aimax_performance(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """è©•ä¼°AImaxç³»çµ±æ€§èƒ½ç‹€æ…‹"""
        try:
            # æª¢æŸ¥é—œéµæ€§èƒ½æŒ‡æ¨™
            ai_meets_target = test_results.get('ai_inference', {}).get('meets_target', False)
            data_meets_target = test_results.get('data_processing', {}).get('meets_target', False)
            system_meets_target = test_results.get('overall_system', {}).get('meets_target', False)
            
            # è¨ˆç®—æ•´é«”æ€§èƒ½ç­‰ç´š
            targets_met = sum([ai_meets_target, data_meets_target, system_meets_target])
            
            if targets_met == 3:
                performance_level = "å„ªç§€"
                status = "æ‰€æœ‰æ€§èƒ½ç›®æ¨™å‡å·²é”æˆ"
            elif targets_met == 2:
                performance_level = "è‰¯å¥½"
                status = "å¤§éƒ¨åˆ†æ€§èƒ½ç›®æ¨™å·²é”æˆ"
            elif targets_met == 1:
                performance_level = "ä¸€èˆ¬"
                status = "éƒ¨åˆ†æ€§èƒ½ç›®æ¨™éœ€è¦æ”¹é€²"
            else:
                performance_level = "éœ€è¦æ”¹é€²"
                status = "å¤šé …æ€§èƒ½æŒ‡æ¨™æœªé”æ¨™"
            
            return {
                'performance_level': performance_level,
                'status': status,
                'targets_met': targets_met,
                'total_targets': 3,
                'ai_inference_status': "é”æ¨™" if ai_meets_target else "æœªé”æ¨™",
                'data_processing_status': "é”æ¨™" if data_meets_target else "æœªé”æ¨™",
                'overall_system_status': "é”æ¨™" if system_meets_target else "æœªé”æ¨™"
            }
            
        except Exception as e:
            logger.error(f"âŒ è©•ä¼°æ€§èƒ½ç‹€æ…‹å¤±æ•—: {e}")
            return {
                'performance_level': "æœªçŸ¥",
                'status': "è©•ä¼°å¤±æ•—",
                'error': str(e)
            }


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    try:
        print("ğŸš€ é–‹å§‹AImaxæ€§èƒ½å„ªåŒ–æ¸¬è©¦...")
        print("=" * 60)
        
        # å‰µå»ºæ¸¬è©¦å¥—ä»¶
        test_suite = PerformanceTestSuite()
        
        # é‹è¡Œæ‰€æœ‰æ¸¬è©¦
        report = await test_suite.run_all_tests()
        
        # é¡¯ç¤ºæ¸¬è©¦çµæœ
        print("\n" + "=" * 60)
        print("ğŸ“Š AImaxæ€§èƒ½å„ªåŒ–æ¸¬è©¦å ±å‘Š")
        print("=" * 60)
        
        if 'error' in report:
            print(f"âŒ æ¸¬è©¦å¤±æ•—: {report['error']}")
            return
        
        # æ¸¬è©¦æ‘˜è¦
        summary = report['test_summary']
        print(f"ğŸ“‹ æ¸¬è©¦æ‘˜è¦:")
        print(f"   ç¸½æ¸¬è©¦æ•¸: {summary['total_tests']}")
        print(f"   æˆåŠŸæ¸¬è©¦: {summary['successful_tests']}")
        print(f"   æˆåŠŸç‡: {summary['success_rate']:.1%}")
        print(f"   å¹³å‡æ”¹é€²: {summary['average_improvement']:.1%}")
        
        # æ€§èƒ½ç‹€æ…‹
        performance_status = report['aimax_performance_status']
        print(f"\nğŸ† AImaxæ€§èƒ½ç‹€æ…‹:")
        print(f"   æ€§èƒ½ç­‰ç´š: {performance_status['performance_level']}")
        print(f"   ç‹€æ…‹: {performance_status['status']}")
        print(f"   é”æ¨™é …ç›®: {performance_status['targets_met']}/{performance_status['total_targets']}")
        
        # é—œéµæ€§èƒ½æŒ‡æ¨™
        print(f"\nâš¡ é—œéµæ€§èƒ½æŒ‡æ¨™:")
        detailed = report['detailed_results']
        
        if 'ai_inference' in detailed:
            ai_result = detailed['ai_inference']
            print(f"   AIæ¨ç†æ™‚é–“: {ai_result.get('optimized_time', 0):.1f}s (ç›®æ¨™: {ai_result.get('target_time', 30)}s)")
        
        if 'data_processing' in detailed:
            data_result = detailed['data_processing']
            print(f"   æ•¸æ“šè™•ç†æ™‚é–“: {data_result.get('optimized_time', 0):.1f}s (ç›®æ¨™: {data_result.get('target_time', 5)}s)")
        
        if 'overall_system' in detailed:
            system_result = detailed['overall_system']
            print(f"   ç¸½é€±æœŸæ™‚é–“: {system_result.get('total_cycle_time', 0):.1f}s (ç›®æ¨™: {system_result.get('target_time', 35)}s)")
        
        # å„ªåŒ–å»ºè­°
        recommendations = report['recommendations']
        print(f"\nğŸ’¡ å„ªåŒ–å»ºè­°:")
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
        
        print("\n" + "=" * 60)
        print("âœ… AImaxæ€§èƒ½å„ªåŒ–æ¸¬è©¦å®Œæˆ!")
        
        # ä¿å­˜å ±å‘Š
        import json
        with open('AImax/performance_test_results.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        print("ğŸ“„ æ¸¬è©¦å ±å‘Šå·²ä¿å­˜åˆ°: AImax/performance_test_results.json")
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # å°å…¥numpyç”¨æ–¼è¨ˆç®—
    import numpy as np
    
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(main())