#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šäº¤æ˜“å°ç³»çµ±è‡ªå‹•åŒ–æ¸¬è©¦å¥—ä»¶ - ä»»å‹™8.3å¯¦ç¾
å‰µå»ºå®Œæ•´çš„è‡ªå‹•åŒ–æ¸¬è©¦æµç¨‹
"""

import sys
import os
import json
import time
import subprocess
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
import logging
from pathlib import Path

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

logger = logging.getLogger(__name__)

@dataclass
class TestResult:
    """æ¸¬è©¦çµæœæ•¸æ“šçµæ§‹"""
    test_name: str
    status: str  # PASSED, FAILED, SKIPPED
    duration: float
    details: Dict[str, Any] = field(default_factory=dict)
    error_message: str = ""
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class TestSuite:
    """æ¸¬è©¦å¥—ä»¶æ•¸æ“šçµæ§‹"""
    name: str
    description: str
    test_files: List[str]
    dependencies: List[str] = field(default_factory=list)
    timeout: int = 300  # 5åˆ†é˜è¶…æ™‚
    critical: bool = False  # æ˜¯å¦ç‚ºé—œéµæ¸¬è©¦

class AutomatedTestRunner:
    """è‡ªå‹•åŒ–æ¸¬è©¦é‹è¡Œå™¨"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root or os.getcwd())
        self.test_results: List[TestResult] = []
        self.test_suites: Dict[str, TestSuite] = {}
        self.setup_test_suites()
        
    def setup_test_suites(self):
        """è¨­ç½®æ¸¬è©¦å¥—ä»¶"""
        try:
            # 1. æ ¸å¿ƒåŠŸèƒ½æ¸¬è©¦å¥—ä»¶
            self.test_suites["core"] = TestSuite(
                name="æ ¸å¿ƒåŠŸèƒ½æ¸¬è©¦",
                description="æ¸¬è©¦ç³»çµ±æ ¸å¿ƒåŠŸèƒ½å’Œçµ„ä»¶",
                test_files=[
                    "AImax/scripts/test_strategy_control.py",
                    "AImax/scripts/test_multi_pair_monitor.py",
                    "AImax/scripts/test_realtime_monitoring.py"
                ],
                critical=True,
                timeout=180
            )
            
            # 2. ç³»çµ±æ•´åˆæ¸¬è©¦å¥—ä»¶
            self.test_suites["integration"] = TestSuite(
                name="ç³»çµ±æ•´åˆæ¸¬è©¦",
                description="æ¸¬è©¦ç³»çµ±çµ„ä»¶é–“çš„æ•´åˆ",
                test_files=[
                    "AImax/scripts/run_multi_pair_integration_test.py"
                ],
                dependencies=["core"],
                critical=True,
                timeout=300
            )
            
            # 3. æ€§èƒ½æ¸¬è©¦å¥—ä»¶
            self.test_suites["performance"] = TestSuite(
                name="æ€§èƒ½æ¸¬è©¦",
                description="æ¸¬è©¦ç³»çµ±æ€§èƒ½å’Œè³‡æºä½¿ç”¨",
                test_files=[
                    "AImax/scripts/test_system_performance_optimization.py"
                ],
                dependencies=["core"],
                timeout=240
            )
            
            # 4. AIç³»çµ±æ¸¬è©¦å¥—ä»¶
            self.test_suites["ai"] = TestSuite(
                name="AIç³»çµ±æ¸¬è©¦",
                description="æ¸¬è©¦AIå”ä½œå’Œæ±ºç­–ç³»çµ±",
                test_files=[
                    "AImax/scripts/test_enhanced_ai_system.py",
                    "AImax/scripts/test_five_ai_simple.py"
                ],
                timeout=300
            )
            
            # 5. æ•¸æ“šç³»çµ±æ¸¬è©¦å¥—ä»¶
            self.test_suites["data"] = TestSuite(
                name="æ•¸æ“šç³»çµ±æ¸¬è©¦",
                description="æ¸¬è©¦æ•¸æ“šç®¡ç†å’Œè™•ç†ç³»çµ±",
                test_files=[
                    "AImax/scripts/test_multi_pair_data_system.py",
                    "AImax/scripts/validate_historical_data.py"
                ],
                timeout=180
            )
            
            logger.info(f"âœ… è¨­ç½®äº† {len(self.test_suites)} å€‹æ¸¬è©¦å¥—ä»¶")
            
        except Exception as e:
            logger.error(f"âŒ æ¸¬è©¦å¥—ä»¶è¨­ç½®å¤±æ•—: {e}")
    
    def run_test_file(self, test_file: str, timeout: int = 300) -> TestResult:
        """é‹è¡Œå–®å€‹æ¸¬è©¦æ–‡ä»¶"""
        test_name = Path(test_file).stem
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ§ª é‹è¡Œæ¸¬è©¦: {test_name}")
            
            # æª¢æŸ¥æ¸¬è©¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            test_path = self.project_root / test_file
            if not test_path.exists():
                return TestResult(
                    test_name=test_name,
                    status="SKIPPED",
                    duration=0,
                    error_message=f"æ¸¬è©¦æ–‡ä»¶ä¸å­˜åœ¨: {test_file}"
                )
            
            # é‹è¡Œæ¸¬è©¦
            result = subprocess.run(
                [sys.executable, str(test_path)],
                cwd=str(self.project_root),
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            duration = time.time() - start_time
            
            if result.returncode == 0:
                return TestResult(
                    test_name=test_name,
                    status="PASSED",
                    duration=duration,
                    details={
                        "stdout": result.stdout[-1000:] if result.stdout else "",  # æœ€å¾Œ1000å­—ç¬¦
                        "stderr": result.stderr[-500:] if result.stderr else ""
                    }
                )
            else:
                return TestResult(
                    test_name=test_name,
                    status="FAILED",
                    duration=duration,
                    error_message=f"é€€å‡ºç¢¼: {result.returncode}",
                    details={
                        "stdout": result.stdout[-1000:] if result.stdout else "",
                        "stderr": result.stderr[-500:] if result.stderr else ""
                    }
                )
                
        except subprocess.TimeoutExpired:
            duration = time.time() - start_time
            return TestResult(
                test_name=test_name,
                status="FAILED",
                duration=duration,
                error_message=f"æ¸¬è©¦è¶…æ™‚ ({timeout}ç§’)"
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return TestResult(
                test_name=test_name,
                status="FAILED",
                duration=duration,
                error_message=f"æ¸¬è©¦åŸ·è¡ŒéŒ¯èª¤: {e}"
            )
    
    def run_test_suite(self, suite_name: str) -> List[TestResult]:
        """é‹è¡Œæ¸¬è©¦å¥—ä»¶"""
        if suite_name not in self.test_suites:
            logger.error(f"âŒ æ¸¬è©¦å¥—ä»¶ä¸å­˜åœ¨: {suite_name}")
            return []
        
        suite = self.test_suites[suite_name]
        logger.info(f"ğŸš€ é–‹å§‹é‹è¡Œæ¸¬è©¦å¥—ä»¶: {suite.name}")
        
        suite_results = []
        
        for test_file in suite.test_files:
            result = self.run_test_file(test_file, suite.timeout)
            suite_results.append(result)
            self.test_results.append(result)
            
            # å¦‚æœæ˜¯é—œéµæ¸¬è©¦ä¸”å¤±æ•—ï¼Œè¨˜éŒ„è­¦å‘Š
            if suite.critical and result.status == "FAILED":
                logger.warning(f"âš ï¸ é—œéµæ¸¬è©¦å¤±æ•—: {result.test_name}")
        
        # çµ±è¨ˆå¥—ä»¶çµæœ
        passed = sum(1 for r in suite_results if r.status == "PASSED")
        failed = sum(1 for r in suite_results if r.status == "FAILED")
        skipped = sum(1 for r in suite_results if r.status == "SKIPPED")
        
        logger.info(f"ğŸ“Š å¥—ä»¶ {suite.name} å®Œæˆ: {passed} é€šé, {failed} å¤±æ•—, {skipped} è·³é")
        
        return suite_results
    
    def run_all_tests(self, parallel: bool = False) -> Dict[str, Any]:
        """é‹è¡Œæ‰€æœ‰æ¸¬è©¦"""
        logger.info("ğŸš€ é–‹å§‹é‹è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶")
        start_time = time.time()
        
        # æŒ‰ä¾è³´é †åºé‹è¡Œæ¸¬è©¦å¥—ä»¶
        execution_order = self._get_execution_order()
        
        all_results = {}
        
        for suite_name in execution_order:
            suite_results = self.run_test_suite(suite_name)
            all_results[suite_name] = suite_results
            
            # æª¢æŸ¥é—œéµæ¸¬è©¦æ˜¯å¦å¤±æ•—
            suite = self.test_suites[suite_name]
            if suite.critical:
                failed_critical = [r for r in suite_results if r.status == "FAILED"]
                if failed_critical:
                    logger.error(f"âŒ é—œéµæ¸¬è©¦å¥—ä»¶ {suite.name} æœ‰å¤±æ•—é …ç›®ï¼Œè€ƒæ…®åœæ­¢å¾ŒçºŒæ¸¬è©¦")
        
        total_duration = time.time() - start_time
        
        # ç”Ÿæˆæ¸¬è©¦å ±å‘Š
        report = self._generate_test_report(all_results, total_duration)
        
        logger.info(f"âœ… æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼Œç¸½è€—æ™‚: {total_duration:.2f}ç§’")
        
        return report
    
    def _get_execution_order(self) -> List[str]:
        """ç²å–æ¸¬è©¦å¥—ä»¶åŸ·è¡Œé †åºï¼ˆè€ƒæ…®ä¾è³´é—œä¿‚ï¼‰"""
        order = []
        visited = set()
        
        def visit(suite_name: str):
            if suite_name in visited:
                return
            
            visited.add(suite_name)
            suite = self.test_suites[suite_name]
            
            # å…ˆè™•ç†ä¾è³´
            for dep in suite.dependencies:
                if dep in self.test_suites:
                    visit(dep)
            
            order.append(suite_name)
        
        # è¨ªå•æ‰€æœ‰å¥—ä»¶
        for suite_name in self.test_suites:
            visit(suite_name)
        
        return order
    
    def _generate_test_report(self, all_results: Dict[str, List[TestResult]], total_duration: float) -> Dict[str, Any]:
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        try:
            # çµ±è¨ˆç¸½é«”çµæœ
            total_tests = len(self.test_results)
            passed_tests = sum(1 for r in self.test_results if r.status == "PASSED")
            failed_tests = sum(1 for r in self.test_results if r.status == "FAILED")
            skipped_tests = sum(1 for r in self.test_results if r.status == "SKIPPED")
            
            success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
            
            # æŒ‰å¥—ä»¶çµ±è¨ˆ
            suite_stats = {}
            for suite_name, results in all_results.items():
                suite_stats[suite_name] = {
                    "total": len(results),
                    "passed": sum(1 for r in results if r.status == "PASSED"),
                    "failed": sum(1 for r in results if r.status == "FAILED"),
                    "skipped": sum(1 for r in results if r.status == "SKIPPED"),
                    "duration": sum(r.duration for r in results),
                    "critical": self.test_suites[suite_name].critical
                }
            
            # å¤±æ•—æ¸¬è©¦è©³æƒ…
            failed_tests_details = [
                {
                    "test_name": r.test_name,
                    "error_message": r.error_message,
                    "duration": r.duration
                }
                for r in self.test_results if r.status == "FAILED"
            ]
            
            report = {
                "test_summary": {
                    "timestamp": datetime.now().isoformat(),
                    "total_duration": total_duration,
                    "total_tests": total_tests,
                    "passed_tests": passed_tests,
                    "failed_tests": failed_tests,
                    "skipped_tests": skipped_tests,
                    "success_rate": success_rate
                },
                "suite_statistics": suite_stats,
                "failed_tests": failed_tests_details,
                "test_details": [
                    {
                        "test_name": r.test_name,
                        "status": r.status,
                        "duration": r.duration,
                        "timestamp": r.timestamp.isoformat()
                    }
                    for r in self.test_results
                ],
                "recommendations": self._generate_recommendations(suite_stats, failed_tests_details)
            }
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ æ¸¬è©¦å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
            return {"error": f"å ±å‘Šç”Ÿæˆå¤±æ•—: {e}"}
    
    def _generate_recommendations(self, suite_stats: Dict, failed_tests: List) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        # æª¢æŸ¥é—œéµæ¸¬è©¦å¤±æ•—
        critical_failures = []
        for suite_name, stats in suite_stats.items():
            if stats["critical"] and stats["failed"] > 0:
                critical_failures.append(suite_name)
        
        if critical_failures:
            recommendations.append(f"é—œéµæ¸¬è©¦å¥—ä»¶å¤±æ•—: {', '.join(critical_failures)}ï¼Œéœ€è¦å„ªå…ˆä¿®å¾©")
        
        # æª¢æŸ¥æˆåŠŸç‡
        total_tests = sum(stats["total"] for stats in suite_stats.values())
        total_passed = sum(stats["passed"] for stats in suite_stats.values())
        success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        if success_rate < 80:
            recommendations.append(f"æ•´é«”æˆåŠŸç‡ {success_rate:.1f}% åä½ï¼Œå»ºè­°æª¢æŸ¥å¤±æ•—æ¸¬è©¦")
        elif success_rate < 95:
            recommendations.append(f"æˆåŠŸç‡ {success_rate:.1f}% è‰¯å¥½ï¼Œä½†ä»æœ‰æ”¹é€²ç©ºé–“")
        else:
            recommendations.append("æ¸¬è©¦æˆåŠŸç‡å„ªç§€ï¼Œç³»çµ±ç‹€æ…‹è‰¯å¥½")
        
        # æª¢æŸ¥æ€§èƒ½å•é¡Œ
        slow_tests = [r for r in self.test_results if r.duration > 60]  # è¶…é1åˆ†é˜
        if slow_tests:
            recommendations.append(f"ç™¼ç¾ {len(slow_tests)} å€‹æ…¢é€Ÿæ¸¬è©¦ï¼Œå»ºè­°å„ªåŒ–æ€§èƒ½")
        
        return recommendations

class ContinuousIntegration:
    """æŒçºŒé›†æˆç³»çµ±"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root or os.getcwd())
        self.test_runner = AutomatedTestRunner(project_root)
        
    def create_ci_config(self):
        """å‰µå»ºCIé…ç½®æ–‡ä»¶"""
        try:
            # å‰µå»ºGitHub Actionsé…ç½®
            github_actions_config = """name: AImax Multi-Pair Trading System CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run automated tests
      run: |
        python AImax/scripts/automated_test_suite.py
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results
        path: AImax/logs/test_reports/
"""
            
            # å‰µå»º.github/workflowsç›®éŒ„
            workflows_dir = self.project_root / ".github" / "workflows"
            workflows_dir.mkdir(parents=True, exist_ok=True)
            
            # å¯«å…¥é…ç½®æ–‡ä»¶
            with open(workflows_dir / "ci.yml", "w", encoding="utf-8") as f:
                f.write(github_actions_config)
            
            logger.info("âœ… GitHub Actions CIé…ç½®å·²å‰µå»º")
            
            # å‰µå»ºæœ¬åœ°CIè…³æœ¬
            local_ci_script = """#!/bin/bash
# æœ¬åœ°æŒçºŒé›†æˆè…³æœ¬

echo "ğŸš€ é–‹å§‹æœ¬åœ°CIæµç¨‹"

# æª¢æŸ¥Pythonç’°å¢ƒ
python --version

# å®‰è£ä¾è³´
echo "ğŸ“¦ å®‰è£ä¾è³´..."
pip install -r requirements.txt

# é‹è¡Œæ¸¬è©¦
echo "ğŸ§ª é‹è¡Œè‡ªå‹•åŒ–æ¸¬è©¦..."
python AImax/scripts/automated_test_suite.py

# æª¢æŸ¥æ¸¬è©¦çµæœ
if [ $? -eq 0 ]; then
    echo "âœ… æ‰€æœ‰æ¸¬è©¦é€šé"
else
    echo "âŒ æ¸¬è©¦å¤±æ•—"
    exit 1
fi

echo "ğŸ‰ æœ¬åœ°CIæµç¨‹å®Œæˆ"
"""
            
            with open(self.project_root / "run_ci.sh", "w", encoding="utf-8") as f:
                f.write(local_ci_script)
            
            # è¨­ç½®åŸ·è¡Œæ¬Šé™ï¼ˆåœ¨Unixç³»çµ±ä¸Šï¼‰
            try:
                os.chmod(self.project_root / "run_ci.sh", 0o755)
            except:
                pass
            
            logger.info("âœ… æœ¬åœ°CIè…³æœ¬å·²å‰µå»º")
            
        except Exception as e:
            logger.error(f"âŒ CIé…ç½®å‰µå»ºå¤±æ•—: {e}")
    
    def run_ci_pipeline(self) -> Dict[str, Any]:
        """é‹è¡ŒCIæµç¨‹"""
        logger.info("ğŸš€ é–‹å§‹CIæµç¨‹")
        
        try:
            # 1. ç’°å¢ƒæª¢æŸ¥
            env_check = self._check_environment()
            if not env_check["success"]:
                return {"success": False, "error": "ç’°å¢ƒæª¢æŸ¥å¤±æ•—", "details": env_check}
            
            # 2. é‹è¡Œæ¸¬è©¦
            test_results = self.test_runner.run_all_tests()
            
            # 3. åˆ†æçµæœ
            success_rate = test_results.get("test_summary", {}).get("success_rate", 0)
            ci_success = success_rate >= 80  # 80%ä»¥ä¸ŠæˆåŠŸç‡è¦–ç‚ºCIé€šé
            
            # 4. ç”ŸæˆCIå ±å‘Š
            ci_report = {
                "success": ci_success,
                "timestamp": datetime.now().isoformat(),
                "environment": env_check,
                "test_results": test_results,
                "ci_status": "PASSED" if ci_success else "FAILED",
                "recommendations": test_results.get("recommendations", [])
            }
            
            # 5. ä¿å­˜CIå ±å‘Š
            self._save_ci_report(ci_report)
            
            logger.info(f"{'âœ…' if ci_success else 'âŒ'} CIæµç¨‹å®Œæˆ: {ci_report['ci_status']}")
            
            return ci_report
            
        except Exception as e:
            logger.error(f"âŒ CIæµç¨‹åŸ·è¡Œå¤±æ•—: {e}")
            return {"success": False, "error": str(e)}
    
    def _check_environment(self) -> Dict[str, Any]:
        """æª¢æŸ¥ç’°å¢ƒ"""
        try:
            checks = {}
            
            # Pythonç‰ˆæœ¬æª¢æŸ¥
            python_version = sys.version_info
            checks["python_version"] = {
                "version": f"{python_version.major}.{python_version.minor}.{python_version.micro}",
                "valid": python_version >= (3, 8)
            }
            
            # ä¾è³´æª¢æŸ¥
            required_modules = ["pandas", "numpy", "psutil", "aiohttp"]
            missing_modules = []
            
            for module in required_modules:
                try:
                    __import__(module)
                except ImportError:
                    missing_modules.append(module)
            
            checks["dependencies"] = {
                "missing": missing_modules,
                "valid": len(missing_modules) == 0
            }
            
            # é …ç›®çµæ§‹æª¢æŸ¥
            required_dirs = ["AImax/src", "AImax/scripts", "AImax/logs"]
            missing_dirs = []
            
            for dir_path in required_dirs:
                if not (self.project_root / dir_path).exists():
                    missing_dirs.append(dir_path)
            
            checks["project_structure"] = {
                "missing_dirs": missing_dirs,
                "valid": len(missing_dirs) == 0
            }
            
            # æ•´é«”æª¢æŸ¥çµæœ
            all_valid = all(check["valid"] for check in checks.values())
            
            return {
                "success": all_valid,
                "checks": checks,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def _save_ci_report(self, report: Dict[str, Any]):
        """ä¿å­˜CIå ±å‘Š"""
        try:
            reports_dir = self.project_root / "AImax" / "logs" / "ci_reports"
            reports_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = reports_dir / f"ci_report_{timestamp}.json"
            
            with open(report_file, "w", encoding="utf-8") as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"ğŸ“„ CIå ±å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            logger.error(f"âŒ CIå ±å‘Šä¿å­˜å¤±æ•—: {e}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ AImax è‡ªå‹•åŒ–æ¸¬è©¦å¥—ä»¶")
    print("=" * 50)
    
    try:
        # å‰µå»ºæ¸¬è©¦é‹è¡Œå™¨
        test_runner = AutomatedTestRunner()
        
        # é‹è¡Œæ‰€æœ‰æ¸¬è©¦
        results = test_runner.run_all_tests()
        
        # é¡¯ç¤ºçµæœ
        summary = results.get("test_summary", {})
        print(f"\nğŸ“Š æ¸¬è©¦çµæœç¸½çµ:")
        print(f"ç¸½æ¸¬è©¦æ•¸: {summary.get('total_tests', 0)}")
        print(f"é€šé: {summary.get('passed_tests', 0)}")
        print(f"å¤±æ•—: {summary.get('failed_tests', 0)}")
        print(f"è·³é: {summary.get('skipped_tests', 0)}")
        print(f"æˆåŠŸç‡: {summary.get('success_rate', 0):.1f}%")
        print(f"ç¸½è€—æ™‚: {summary.get('total_duration', 0):.2f}ç§’")
        
        # ä¿å­˜æ¸¬è©¦å ±å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_dir = Path("AImax/logs/test_reports")
        report_dir.mkdir(parents=True, exist_ok=True)
        
        report_file = report_dir / f"automated_test_report_{timestamp}.json"
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜: {report_file}")
        
        # è¿”å›æˆåŠŸç‹€æ…‹
        success_rate = summary.get('success_rate', 0)
        return 0 if success_rate >= 80 else 1
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¥—ä»¶åŸ·è¡Œå¤±æ•—: {e}")
        return 1

if __name__ == "__main__":
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    exit_code = main()
    sys.exit(exit_code)