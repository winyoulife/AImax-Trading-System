#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦é›†æˆè©•åˆ†å™¨é©—è­‰åŠŸèƒ½
"""

import sys
import os
import asyncio
import logging
from datetime import datetime
import numpy as np

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.ml.ensemble_scorer import create_ensemble_scorer

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def generate_test_data(num_samples: int = 50) -> tuple:
    """ç”Ÿæˆæ¸¬è©¦æ•¸æ“š"""
    test_data = []
    ground_truth = []
    
    np.random.seed(42)  # ç¢ºä¿çµæœå¯é‡ç¾
    
    for i in range(num_samples):
        # ç”Ÿæˆæ¨¡æ“¬å¸‚å ´æ•¸æ“š
        base_price = 1500000 + np.random.normal(0, 100000)
        price_change = np.random.normal(0, 2)  # -2% åˆ° +2% çš„åƒ¹æ ¼è®ŠåŒ–
        volume_ratio = max(0.5, np.random.lognormal(0, 0.3))  # æˆäº¤é‡æ¯”ç‡
        
        # ç”ŸæˆAIæ ¼å¼åŒ–æ•¸æ“š
        if price_change > 1:
            ai_data = "å¸‚å ´é¡¯ç¤ºä¸Šæ¼²è¶¨å‹¢ï¼ŒæŠ€è¡“æŒ‡æ¨™çœ‹æ¼²"
            true_signal = "BUY"
        elif price_change < -1:
            ai_data = "å¸‚å ´é¡¯ç¤ºä¸‹è·Œè¶¨å‹¢ï¼ŒæŠ€è¡“æŒ‡æ¨™çœ‹è·Œ"
            true_signal = "SELL"
        else:
            ai_data = "å¸‚å ´æ©«ç›¤æ•´ç†ï¼ŒæŠ€è¡“æŒ‡æ¨™ä¸­æ€§"
            true_signal = "HOLD"
        
        market_data = {
            'current_price': base_price,
            'price_change_1m': price_change,
            'volume_ratio': volume_ratio,
            'ai_formatted_data': ai_data
        }
        
        test_data.append(market_data)
        ground_truth.append(true_signal)
    
    return test_data, ground_truth

async def test_ensemble_validation():
    """æ¸¬è©¦é›†æˆè©•åˆ†å™¨é©—è­‰åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦é›†æˆè©•åˆ†å™¨é©—è­‰åŠŸèƒ½...")
    print("=" * 60)
    
    # å‰µå»ºé›†æˆè©•åˆ†å™¨
    scorer = create_ensemble_scorer()
    
    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
    print("\nğŸ“Š ç”Ÿæˆæ¸¬è©¦æ•¸æ“š...")
    test_data, ground_truth = generate_test_data(50)
    print(f"   ç”Ÿæˆäº† {len(test_data)} å€‹æ¸¬è©¦æ¨£æœ¬")
    print(f"   çœŸå¯¦æ¨™ç±¤åˆ†ä½ˆ: {dict(zip(*np.unique(ground_truth, return_counts=True)))}")
    
    # åŸ·è¡Œé›†æˆè©•åˆ†å™¨é©—è­‰
    print("\nğŸ” åŸ·è¡Œé›†æˆè©•åˆ†å™¨é©—è­‰...")
    validation_result = scorer.validate_ensemble(test_data, ground_truth)
    
    if not validation_result['success']:
        print(f"âŒ é©—è­‰å¤±æ•—: {validation_result.get('error', 'Unknown error')}")
        return
    
    # é¡¯ç¤ºé©—è­‰çµæœ
    metrics = validation_result['validation_metrics']
    summary = validation_result['summary']
    
    print("\nğŸ“ˆ é©—è­‰çµæœæ‘˜è¦:")
    print("-" * 40)
    print(f"âœ… ç¸½é«”è©•åˆ†: {summary['overall_score']:.3f}")
    print(f"   é æ¸¬æˆåŠŸç‡: {metrics['success_rate']:.2%}")
    print(f"   é æ¸¬æ•¸é‡: {metrics['prediction_count']}")
    print(f"   æ¨¡å‹ä¸€è‡´æ€§: {metrics['model_agreement']:.3f}")
    
    # åˆ†æ•¸çµ±è¨ˆ
    score_stats = metrics['score_statistics']
    print(f"\nğŸ“Š åˆ†æ•¸çµ±è¨ˆ:")
    print(f"   å¹³å‡åˆ†æ•¸: {score_stats['mean']:.1f}")
    print(f"   æ¨™æº–å·®: {score_stats['std']:.1f}")
    print(f"   åˆ†æ•¸ç¯„åœ: {score_stats['min']:.1f} - {score_stats['max']:.1f}")
    print(f"   ä¸­ä½æ•¸: {score_stats['median']:.1f}")
    
    # ä¿¡è™Ÿåˆ†ä½ˆ
    signal_dist = metrics['signal_distribution']
    print(f"\nğŸ“¡ ä¿¡è™Ÿåˆ†ä½ˆ:")
    for signal, count in signal_dist['counts'].items():
        percentage = signal_dist['percentages'][signal]
        print(f"   {signal}: {count} ({percentage:.1%})")
    print(f"   ä¿¡è™Ÿå¤šæ¨£æ€§: {signal_dist['diversity']} ç¨®")
    
    # ä¿¡å¿ƒåº¦åˆ†æ
    confidence_analysis = metrics['confidence_analysis']
    print(f"\nğŸ¯ ä¿¡å¿ƒåº¦åˆ†æ:")
    conf_stats = confidence_analysis['statistics']
    print(f"   å¹³å‡ä¿¡å¿ƒåº¦: {conf_stats['mean']:.1f}%")
    print(f"   ä¿¡å¿ƒåº¦æ¨™æº–å·®: {conf_stats['std']:.1f}%")
    
    conf_dist = confidence_analysis['percentages']
    print(f"   é«˜ä¿¡å¿ƒåº¦ (â‰¥80%): {conf_dist['high_confidence']:.1%}")
    print(f"   ä¸­ä¿¡å¿ƒåº¦ (60-80%): {conf_dist['medium_confidence']:.1%}")
    print(f"   ä½ä¿¡å¿ƒåº¦ (<60%): {conf_dist['low_confidence']:.1%}")
    
    # çµ„ä»¶ä¸€è‡´æ€§
    component_consistency = metrics['component_consistency']
    print(f"\nğŸ”§ çµ„ä»¶ä¸€è‡´æ€§åˆ†æ:")
    print(f"   æ•´é«”ä¸€è‡´æ€§: {component_consistency['overall_consistency']:.3f}")
    
    for component, comp_metrics in component_consistency['component_metrics'].items():
        print(f"   {component}:")
        print(f"     å¹³å‡åˆ†æ•¸: {comp_metrics['mean']:.1f}")
        print(f"     æ¨™æº–å·®: {comp_metrics['std']:.1f}")
        print(f"     ä¸€è‡´æ€§åˆ†æ•¸: {comp_metrics['consistency_score']:.3f}")
    
    # çµ„ä»¶é–“ç›¸é—œæ€§
    correlations = component_consistency['correlations']
    if correlations:
        print(f"   çµ„ä»¶é–“ç›¸é—œæ€§:")
        for pair, corr in correlations.items():
            print(f"     {pair}: {corr:.3f}")
    
    # æ¬Šé‡æœ‰æ•ˆæ€§
    weight_effectiveness = metrics['weight_effectiveness']
    print(f"\nâš–ï¸ æ¬Šé‡æœ‰æ•ˆæ€§åˆ†æ:")
    print(f"   æœ‰æ•ˆæ€§åˆ†æ•¸: {weight_effectiveness['effectiveness_score']:.3f}")
    
    contributions = weight_effectiveness['component_contributions']
    for component, contrib in contributions.items():
        print(f"   {component}:")
        print(f"     ç•¶å‰æ¬Šé‡: {contrib['weight']:.3f}")
        print(f"     èˆ‡æœ€çµ‚åˆ†æ•¸ç›¸é—œæ€§: {contrib['correlation_with_final']:.3f}")
        print(f"     å¹³å‡è²¢ç»: {contrib['average_contribution']:.1f}")
    
    weight_balance = weight_effectiveness['weight_balance']
    print(f"   æ¬Šé‡å¹³è¡¡æ€§:")
    print(f"     æ¨™æº–åŒ–ç†µ: {weight_balance['normalized_entropy']:.3f}")
    print(f"     å¹³è¡¡åˆ†æ•¸: {weight_balance['balance_score']:.3f}")
    
    # é æ¸¬ç©©å®šæ€§
    stability = metrics['prediction_stability']
    if stability:
        print(f"\nğŸ“ˆ é æ¸¬ç©©å®šæ€§åˆ†æ:")
        
        score_stability = stability['score_stability']
        print(f"   åˆ†æ•¸ç©©å®šæ€§:")
        print(f"     å¹³å‡è®ŠåŒ–: {score_stability['mean_change']:.1f}")
        print(f"     æœ€å¤§è®ŠåŒ–: {score_stability['max_change']:.1f}")
        print(f"     ç©©å®šæ€§åˆ†æ•¸: {score_stability['stability_score']:.3f}")
        
        signal_stability = stability['signal_stability']
        print(f"   ä¿¡è™Ÿç©©å®šæ€§:")
        print(f"     ä¿¡è™Ÿè®ŠåŒ–æ¬¡æ•¸: {signal_stability['change_count']}")
        print(f"     è®ŠåŒ–ç‡: {signal_stability['change_rate']:.1%}")
        print(f"     ç©©å®šæ€§åˆ†æ•¸: {signal_stability['stability_score']:.3f}")
        
        confidence_stability = stability['confidence_stability']
        print(f"   ä¿¡å¿ƒåº¦ç©©å®šæ€§:")
        print(f"     å¹³å‡è®ŠåŒ–: {confidence_stability['mean_change']:.1f}%")
        print(f"     ç©©å®šæ€§åˆ†æ•¸: {confidence_stability['stability_score']:.3f}")
    
    # æº–ç¢ºæ€§æŒ‡æ¨™ (å¦‚æœæœ‰çœŸå¯¦æ¨™ç±¤)
    if 'accuracy_metrics' in metrics:
        accuracy = metrics['accuracy_metrics']
        print(f"\nğŸ¯ æº–ç¢ºæ€§æŒ‡æ¨™:")
        print(f"   ç¸½é«”æº–ç¢ºç‡: {accuracy['accuracy']:.2%}")
        print(f"   æ­£ç¢ºé æ¸¬æ•¸: {accuracy['correct_predictions']}")
        print(f"   ç¸½é æ¸¬æ•¸: {accuracy['total_predictions']}")
        print(f"   å®å¹³å‡F1åˆ†æ•¸: {accuracy['macro_f1']:.3f}")
        
        print(f"   å„é¡åˆ¥æ€§èƒ½:")
        for label, metrics_data in accuracy['precision_recall'].items():
            print(f"     {label}:")
            print(f"       ç²¾ç¢ºç‡: {metrics_data['precision']:.3f}")
            print(f"       å¬å›ç‡: {metrics_data['recall']:.3f}")
            print(f"       F1åˆ†æ•¸: {metrics_data['f1_score']:.3f}")
            print(f"       æ”¯æŒæ•¸: {metrics_data['support']}")
    
    # æ¬Šé‡å„ªåŒ–å»ºè­°
    weight_optimization = metrics['weight_optimization']
    if 'final_recommendation' in weight_optimization:
        final_rec = weight_optimization['final_recommendation']
        print(f"\nğŸ”§ æ¬Šé‡å„ªåŒ–å»ºè­°:")
        print(f"   å»ºè­°ä¿¡å¿ƒåº¦: {final_rec['confidence']:.3f}")
        print(f"   ä½¿ç”¨çš„å„ªåŒ–æ–¹æ³•æ•¸: {final_rec['optimization_methods_used']}")
        
        print(f"   ç•¶å‰æ¬Šé‡:")
        for component, weight in final_rec['current_weights'].items():
            print(f"     {component}: {weight:.3f}")
        
        print(f"   å»ºè­°æ¬Šé‡:")
        for component, weight in final_rec['recommended_weights'].items():
            print(f"     {component}: {weight:.3f}")
        
        print(f"   æ¬Šé‡è®ŠåŒ–:")
        for component, change in final_rec['weight_changes'].items():
            if abs(change) > 0.01:  # åªé¡¯ç¤ºé¡¯è‘—è®ŠåŒ–
                print(f"     {component}: {change:+.3f}")
    
    # è¡çªè§£æ±ºæ©Ÿåˆ¶è©•ä¼°
    conflict_resolution = metrics['conflict_resolution']
    if conflict_resolution:
        print(f"\nâš”ï¸ è¡çªè§£æ±ºæ©Ÿåˆ¶è©•ä¼°:")
        print(f"   ç¸½é æ¸¬æ•¸: {conflict_resolution['total_predictions']}")
        print(f"   è¡çªæ¡ˆä¾‹æ•¸: {conflict_resolution['conflict_cases']}")
        print(f"   è§£æ±ºæœ‰æ•ˆæ€§: {conflict_resolution['resolution_effectiveness']:.3f}")
        
        conflict_types = conflict_resolution['conflict_types']
        print(f"   è¡çªé¡å‹:")
        print(f"     é«˜åˆ†æ­§: {conflict_types['high_disagreement']}")
        print(f"     æ··åˆä¿¡è™Ÿ: {conflict_types['mixed_signals']}")
        print(f"     ä½ä¿¡å¿ƒåº¦: {conflict_types['low_confidence']}")
        
        if 'resolution_strategies' in conflict_resolution:
            strategies = conflict_resolution['resolution_strategies']
            print(f"   è§£æ±ºç­–ç•¥æ•ˆæœ:")
            for strategy, data in strategies.items():
                if data['used'] > 0:
                    print(f"     {strategy}: ä½¿ç”¨{data['used']}æ¬¡, æœ‰æ•ˆç‡{data['effectiveness_rate']:.2%}")
    
    # å¯é æ€§è©•ä¼°
    reliability = metrics['reliability_assessment']
    if reliability:
        print(f"\nğŸ›¡ï¸ å¯é æ€§è©•ä¼°:")
        print(f"   ç¸½é«”å¯é æ€§åˆ†æ•¸: {reliability['overall_score']:.3f}")
        print(f"   ç¸½é«”è©•ä¼°: {reliability['overall_assessment']}")
        print(f"   å¯é æ€§ç­‰ç´š: {reliability['reliability_level']}")
        
        print(f"   å„å› å­è©•åˆ†:")
        for factor, data in reliability['factor_scores'].items():
            print(f"     {factor}: {data['score']:.3f} ({data['assessment']})")
        
        print(f"   æ”¹é€²å»ºè­°:")
        for rec in reliability['recommendations']:
            print(f"     - {rec}")
    
    # ç³»çµ±å„ªå‹¢å’Œå¼±é»
    print(f"\nğŸ’ª ç³»çµ±å„ªå‹¢:")
    for strength in summary['strengths']:
        print(f"   âœ… {strength}")
    
    print(f"\nâš ï¸ ç³»çµ±å¼±é»:")
    for weakness in summary['weaknesses']:
        print(f"   âŒ {weakness}")
    
    print(f"\nğŸ”§ æ”¹é€²å»ºè­°:")
    for recommendation in summary['recommendations']:
        print(f"   ğŸ’¡ {recommendation}")
    
    # æ¸¬è©¦æ¬Šé‡å„ªåŒ–åŠŸèƒ½
    print(f"\nğŸ”§ æ¸¬è©¦æ¬Šé‡å„ªåŒ–åŠŸèƒ½...")
    if 'final_recommendation' in weight_optimization and weight_optimization['final_recommendation']['confidence'] > 0.5:
        new_weights = weight_optimization['final_recommendation']['recommended_weights']
        print(f"   å˜—è©¦æ‡‰ç”¨å„ªåŒ–æ¬Šé‡...")
        
        if scorer.update_weights(new_weights):
            print(f"   âœ… æ¬Šé‡æ›´æ–°æˆåŠŸ")
            
            # é‡æ–°é©—è­‰ä»¥æŸ¥çœ‹æ”¹é€²æ•ˆæœ
            print(f"   ğŸ”„ ä½¿ç”¨æ–°æ¬Šé‡é‡æ–°é©—è­‰...")
            new_validation = scorer.validate_ensemble(test_data[:20], ground_truth[:20])  # ä½¿ç”¨è¼ƒå°æ¨£æœ¬å¿«é€Ÿæ¸¬è©¦
            
            if new_validation['success']:
                new_score = new_validation['summary']['overall_score']
                old_score = summary['overall_score']
                improvement = new_score - old_score
                
                print(f"   ğŸ“Š æ¬Šé‡å„ªåŒ–æ•ˆæœ:")
                print(f"     åŸå§‹ç¸½é«”è©•åˆ†: {old_score:.3f}")
                print(f"     å„ªåŒ–å¾Œè©•åˆ†: {new_score:.3f}")
                print(f"     æ”¹é€²å¹…åº¦: {improvement:+.3f}")
                
                if improvement > 0:
                    print(f"   âœ… æ¬Šé‡å„ªåŒ–æœ‰æ•ˆï¼Œç³»çµ±æ€§èƒ½æå‡")
                else:
                    print(f"   âš ï¸ æ¬Šé‡å„ªåŒ–æ•ˆæœä¸æ˜é¡¯")
        else:
            print(f"   âŒ æ¬Šé‡æ›´æ–°å¤±æ•—")
    else:
        print(f"   âš ï¸ æ¬Šé‡å„ªåŒ–å»ºè­°ä¿¡å¿ƒåº¦ä¸è¶³ï¼Œè·³éæ¸¬è©¦")
    
    print(f"\nğŸ¯ é›†æˆè©•åˆ†å™¨é©—è­‰æ¸¬è©¦å®Œæˆï¼")
    print("=" * 60)
    
    return {
        'validation_successful': validation_result['success'],
        'overall_score': summary['overall_score'],
        'success_rate': metrics['success_rate'],
        'model_agreement': metrics['model_agreement'],
        'accuracy': metrics.get('accuracy_metrics', {}).get('accuracy', 0),
        'reliability_score': reliability.get('overall_score', 0) if reliability else 0
    }

if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    result = asyncio.run(test_ensemble_validation())
    
    # è¼¸å‡ºæœ€çµ‚æ¸¬è©¦çµæœ
    print(f"\nğŸ† æœ€çµ‚æ¸¬è©¦çµæœ:")
    print(f"   é©—è­‰æˆåŠŸ: {'âœ…' if result['validation_successful'] else 'âŒ'}")
    print(f"   ç¸½é«”è©•åˆ†: {result['overall_score']:.3f}")
    print(f"   é æ¸¬æˆåŠŸç‡: {result['success_rate']:.2%}")
    print(f"   æ¨¡å‹ä¸€è‡´æ€§: {result['model_agreement']:.3f}")
    if result['accuracy'] > 0:
        print(f"   é æ¸¬æº–ç¢ºç‡: {result['accuracy']:.2%}")
    print(f"   å¯é æ€§åˆ†æ•¸: {result['reliability_score']:.3f}")
    
    # è©•ä¼°ç³»çµ±å¥åº·åº¦
    if result['overall_score'] > 0.8:
        health_status = "å„ªç§€"
    elif result['overall_score'] > 0.6:
        health_status = "è‰¯å¥½"
    elif result['overall_score'] > 0.4:
        health_status = "ä¸€èˆ¬"
    else:
        health_status = "éœ€æ”¹é€²"
    
    print(f"   ç³»çµ±å¥åº·åº¦: {health_status}")