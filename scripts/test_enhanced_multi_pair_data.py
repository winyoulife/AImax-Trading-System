#!/usr/bin/env python3
"""
å¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ±æ¸¬è©¦è…³æœ¬
æ¸¬è©¦ä»»å‹™1.2çš„å®Œæ•´å¯¦ç¾
"""

import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from src.data.enhanced_multi_pair_data_manager import EnhancedMultiPairDataManager, create_enhanced_multi_pair_data_manager
except ImportError as e:
    print(f"âŒ å°å…¥éŒ¯èª¤: {e}")
    print("è«‹ç¢ºä¿åœ¨AImaxé …ç›®æ ¹ç›®éŒ„ä¸‹é‹è¡Œæ­¤è…³æœ¬")
    sys.exit(1)

import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class EnhancedMultiPairDataTester:
    """å¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç³»çµ±æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.manager = None
        self.test_results = {}
    
    async def run_comprehensive_tests(self):
        """é‹è¡Œå…¨é¢æ¸¬è©¦"""
        print("ğŸ§ª é–‹å§‹å¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ±æ¸¬è©¦...")
        print("=" * 70)
        
        try:
            # æ¸¬è©¦1: ç³»çµ±åˆå§‹åŒ–
            await self.test_system_initialization()
            
            # æ¸¬è©¦2: ç¨ç«‹æ­·å²æ•¸æ“šç®¡ç†å™¨
            await self.test_independent_data_managers()
            
            # æ¸¬è©¦3: ä¸¦è¡Œæ•¸æ“šæµè™•ç†
            await self.test_parallel_data_processing()
            
            # æ¸¬è©¦4: æ•¸æ“šåŒæ­¥å’Œä¸€è‡´æ€§æ©Ÿåˆ¶
            await self.test_data_sync_consistency()
            
            # æ¸¬è©¦5: å„ªåŒ–å­˜å„²çµæ§‹
            await self.test_optimized_storage_structure()
            
            # æ¸¬è©¦6: å¤šäº¤æ˜“å°æŸ¥è©¢æ€§èƒ½
            await self.test_multi_pair_query_performance()
            
            # ç”Ÿæˆæœ€çµ‚æ¸¬è©¦å ±å‘Š
            self.generate_comprehensive_report()
            
        except Exception as e:
            logger.error(f"âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            if self.manager:
                await self.manager.close()
    
    async def test_system_initialization(self):
        """æ¸¬è©¦1: ç³»çµ±åˆå§‹åŒ–"""
        print("\nğŸš€ æ¸¬è©¦1: ç³»çµ±åˆå§‹åŒ–")
        print("-" * 50)
        
        try:
            # å‰µå»ºå¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†å™¨
            self.manager = create_enhanced_multi_pair_data_manager()
            
            # é©—è­‰æ ¸å¿ƒçµ„ä»¶
            components_check = {
                'max_client': self.manager.max_client is not None,
                'pair_manager': self.manager.pair_manager is not None,
                'sync_coordinator': self.manager.sync_coordinator is not None,
                'pair_data_managers': len(self.manager.pair_data_managers) > 0,
                'stream_configs': len(self.manager.stream_configs) > 0,
                'database_initialized': self.manager.db_path.exists()
            }
            
            all_passed = all(components_check.values())
            
            print(f"âœ… ç³»çµ±åˆå§‹åŒ–: {'æˆåŠŸ' if all_passed else 'éƒ¨åˆ†å¤±æ•—'}")
            for component, status in components_check.items():
                status_icon = "âœ…" if status else "âŒ"
                print(f"   {status_icon} {component}: {status}")
            
            print(f"ğŸ“Š ç³»çµ±çµ±è¨ˆ:")
            print(f"   - æ”¯æŒäº¤æ˜“å°: {len(self.manager.stream_configs)}")
            print(f"   - ç¨ç«‹æ•¸æ“šç®¡ç†å™¨: {len(self.manager.pair_data_managers)}")
            print(f"   - ç·šç¨‹æ± å¤§å°: {self.manager.executor._max_workers}")
            print(f"   - æ•¸æ“šåº«è·¯å¾‘: {self.manager.db_path}")
            
            self.test_results['system_initialization'] = {
                'status': 'success' if all_passed else 'partial',
                'components_check': components_check,
                'pairs_count': len(self.manager.stream_configs),
                'managers_count': len(self.manager.pair_data_managers)
            }
            
        except Exception as e:
            print(f"âŒ ç³»çµ±åˆå§‹åŒ–æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results['system_initialization'] = {
                'status': 'failed',
                'error': str(e)
            }
            raise
    
    async def test_independent_data_managers(self):
        """æ¸¬è©¦2: ç¨ç«‹æ­·å²æ•¸æ“šç®¡ç†å™¨"""
        print("\nğŸ“Š æ¸¬è©¦2: ç¨ç«‹æ­·å²æ•¸æ“šç®¡ç†å™¨")
        print("-" * 50)
        
        try:
            managers_status = {}
            
            for pair, data_manager in self.manager.pair_data_managers.items():
                # æª¢æŸ¥æ¯å€‹äº¤æ˜“å°çš„ç¨ç«‹æ•¸æ“šç®¡ç†å™¨
                manager_check = {
                    'exists': data_manager is not None,
                    'type': type(data_manager).__name__,
                    'has_db_path': hasattr(data_manager, 'db_path')
                }
                
                managers_status[pair] = manager_check
                
                status_icon = "âœ…" if manager_check['exists'] else "âŒ"
                print(f"   {status_icon} {pair}: {manager_check['type']}")
            
            success_count = sum(1 for status in managers_status.values() 
                              if status['exists'])
            total_count = len(managers_status)
            
            print(f"ğŸ“ˆ ç¨ç«‹æ•¸æ“šç®¡ç†å™¨çµ±è¨ˆ:")
            print(f"   - æˆåŠŸå‰µå»º: {success_count}/{total_count}")
            print(f"   - æˆåŠŸç‡: {success_count/total_count*100:.1f}%")
            
            self.test_results['independent_data_managers'] = {
                'status': 'success' if success_count == total_count else 'partial',
                'managers_status': managers_status,
                'success_count': success_count,
                'total_count': total_count,
                'success_rate': success_count / total_count
            }
            
        except Exception as e:
            print(f"âŒ ç¨ç«‹æ•¸æ“šç®¡ç†å™¨æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results['independent_data_managers'] = {
                'status': 'failed',
                'error': str(e)
            }
    
    async def test_parallel_data_processing(self):
        """æ¸¬è©¦3: ä¸¦è¡Œæ•¸æ“šæµè™•ç†"""
        print("\nğŸ“¡ æ¸¬è©¦3: ä¸¦è¡Œæ•¸æ“šæµè™•ç†")
        print("-" * 50)
        
        try:
            # æª¢æŸ¥æ•¸æ“šæµé…ç½®
            stream_analysis = {
                'total_streams': len(self.manager.stream_configs),
                'enabled_streams': 0,
                'timeframes_configured': 0,
                'buffers_initialized': len(self.manager.data_buffers),
                'locks_created': len(self.manager.sync_locks)
            }
            
            for pair, config in self.manager.stream_configs.items():
                if config.enabled:
                    stream_analysis['enabled_streams'] += 1
                
                stream_analysis['timeframes_configured'] += len(config.timeframes)
                
                print(f"   ğŸ“Š {pair}:")
                print(f"      - ç‹€æ…‹: {'å•Ÿç”¨' if config.enabled else 'ç¦ç”¨'}")
                print(f"      - æ™‚é–“æ¡†æ¶: {config.timeframes}")
                print(f"      - ç·©è¡å€å¤§å°: {config.buffer_size}")
            
            # æ¸¬è©¦ä¸¦è¡Œæ•¸æ“šæµå•Ÿå‹•
            print(f"\nğŸš€ æ¸¬è©¦ä¸¦è¡Œæ•¸æ“šæµå•Ÿå‹•...")
            await self.manager.start_parallel_data_streams()
            
            print(f"ğŸ“ˆ ä¸¦è¡Œè™•ç†çµ±è¨ˆ:")
            print(f"   - ç¸½æ•¸æ“šæµ: {stream_analysis['total_streams']}")
            print(f"   - å•Ÿç”¨æ•¸æ“šæµ: {stream_analysis['enabled_streams']}")
            print(f"   - é…ç½®æ™‚é–“æ¡†æ¶: {stream_analysis['timeframes_configured']}")
            print(f"   - æ•¸æ“šç·©è¡å€: {stream_analysis['buffers_initialized']}")
            print(f"   - åŒæ­¥é–: {stream_analysis['locks_created']}")
            
            self.test_results['parallel_data_processing'] = {
                'status': 'success',
                'stream_analysis': stream_analysis,
                'parallel_start_completed': True
            }
            
        except Exception as e:
            print(f"âŒ ä¸¦è¡Œæ•¸æ“šæµè™•ç†æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results['parallel_data_processing'] = {
                'status': 'failed',
                'error': str(e)
            }
    
    async def test_data_sync_consistency(self):
        """æ¸¬è©¦4: æ•¸æ“šåŒæ­¥å’Œä¸€è‡´æ€§æ©Ÿåˆ¶"""
        print("\nğŸ”„ æ¸¬è©¦4: æ•¸æ“šåŒæ­¥å’Œä¸€è‡´æ€§æ©Ÿåˆ¶")
        print("-" * 50)
        
        try:
            # æ¸¬è©¦åŒæ­¥å”èª¿å™¨
            coordinator = self.manager.sync_coordinator
            test_pairs = list(self.manager.stream_configs.keys())[:3]
            
            print(f"ğŸ”„ æ¸¬è©¦å”èª¿åŒæ­¥ ({len(test_pairs)} å€‹äº¤æ˜“å°)...")
            sync_results = await coordinator.coordinate_sync(test_pairs, 'consistency_test')
            
            # åˆ†æåŒæ­¥çµæœ
            sync_analysis = {
                'total_pairs': len(test_pairs),
                'successful_syncs': 0,
                'failed_syncs': 0,
                'sync_details': []
            }
            
            for result in sync_results:
                if isinstance(result, dict):
                    if result.get('status') == 'success':
                        sync_analysis['successful_syncs'] += 1
                        print(f"   âœ… {result.get('pair')}: åŒæ­¥æˆåŠŸ")
                    else:
                        sync_analysis['failed_syncs'] += 1
                        print(f"   âŒ {result.get('pair')}: åŒæ­¥å¤±æ•—")
                    
                    sync_analysis['sync_details'].append(result)
                else:
                    sync_analysis['failed_syncs'] += 1
                    print(f"   âš ï¸ åŒæ­¥ç•°å¸¸: {result}")
            
            # æ¸¬è©¦æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥
            print(f"\nğŸ” æ¸¬è©¦æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥...")
            consistency_report = self.manager.check_data_consistency(test_pairs)
            
            consistency_issues = len(consistency_report.get('consistency_issues', []))
            print(f"   ğŸ“Š ä¸€è‡´æ€§å•é¡Œ: {consistency_issues} å€‹")
            
            for issue in consistency_report.get('consistency_issues', [])[:3]:  # é¡¯ç¤ºå‰3å€‹
                print(f"      - {issue.get('pair')}: {issue.get('issue')}")
            
            # æ¸¬è©¦åŒæ­¥å»ºè­°
            print(f"\nğŸ’¡ æ¸¬è©¦åŒæ­¥å»ºè­°...")
            for pair in test_pairs[:2]:
                recommendations = coordinator.get_sync_recommendations(pair)
                print(f"   ğŸ“‹ {pair}: å„ªå…ˆç´š {recommendations.get('priority', 'normal')}")
            
            sync_success_rate = sync_analysis['successful_syncs'] / sync_analysis['total_pairs']
            
            print(f"ğŸ“ˆ åŒæ­¥ä¸€è‡´æ€§çµ±è¨ˆ:")
            print(f"   - åŒæ­¥æˆåŠŸç‡: {sync_success_rate*100:.1f}%")
            print(f"   - ä¸€è‡´æ€§å•é¡Œ: {consistency_issues} å€‹")
            print(f"   - å»ºè­°æ•¸é‡: {len(consistency_report.get('recommendations', []))}")
            
            self.test_results['data_sync_consistency'] = {
                'status': 'success',
                'sync_analysis': sync_analysis,
                'consistency_report': consistency_report,
                'sync_success_rate': sync_success_rate
            }
            
        except Exception as e:
            print(f"âŒ æ•¸æ“šåŒæ­¥å’Œä¸€è‡´æ€§æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results['data_sync_consistency'] = {
                'status': 'failed',
                'error': str(e)
            }
    
    async def test_optimized_storage_structure(self):
        """æ¸¬è©¦5: å„ªåŒ–å­˜å„²çµæ§‹"""
        print("\nğŸ—„ï¸ æ¸¬è©¦5: å„ªåŒ–å­˜å„²çµæ§‹")
        print("-" * 50)
        
        try:
            # åŸ·è¡Œå­˜å„²å„ªåŒ–
            print(f"ğŸ”§ åŸ·è¡Œå­˜å„²çµæ§‹å„ªåŒ–...")
            optimization_report = self.manager.optimize_storage_structure()
            
            optimizations_applied = len(optimization_report.get('optimizations_applied', []))
            performance_improvements = len(optimization_report.get('performance_improvements', []))
            
            print(f"ğŸ“Š å„ªåŒ–çµæœ:")
            print(f"   - æ‡‰ç”¨å„ªåŒ–: {optimizations_applied} é …")
            print(f"   - æ€§èƒ½æ”¹é€²: {performance_improvements} é …")
            
            for optimization in optimization_report.get('optimizations_applied', []):
                print(f"      âœ… {optimization}")
            
            # æª¢æŸ¥å­˜å„²çµ±è¨ˆ
            storage_stats = optimization_report.get('storage_stats', {})
            if storage_stats:
                print(f"ğŸ“ˆ å­˜å„²çµ±è¨ˆ:")
                print(f"   - ç¸½è¨˜éŒ„æ•¸: {storage_stats.get('total_records', 0)}")
                print(f"   - å”¯ä¸€äº¤æ˜“å°: {storage_stats.get('unique_pairs', 0)}")
                print(f"   - æœ€æ—©æ•¸æ“š: {storage_stats.get('earliest_data', 'N/A')}")
                print(f"   - æœ€æ–°æ•¸æ“š: {storage_stats.get('latest_data', 'N/A')}")
            
            self.test_results['optimized_storage_structure'] = {
                'status': 'success',
                'optimization_report': optimization_report,
                'optimizations_count': optimizations_applied,
                'improvements_count': performance_improvements
            }
            
        except Exception as e:
            print(f"âŒ å„ªåŒ–å­˜å„²çµæ§‹æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results['optimized_storage_structure'] = {
                'status': 'failed',
                'error': str(e)
            }
    
    async def test_multi_pair_query_performance(self):
        """æ¸¬è©¦6: å¤šäº¤æ˜“å°æŸ¥è©¢æ€§èƒ½"""
        print("\nâš¡ æ¸¬è©¦6: å¤šäº¤æ˜“å°æŸ¥è©¢æ€§èƒ½")
        print("-" * 50)
        
        try:
            import time
            
            test_pairs = list(self.manager.stream_configs.keys())[:4]  # æ¸¬è©¦å‰4å€‹äº¤æ˜“å°
            
            # æ¸¬è©¦æ­·å²æ•¸æ“šæŸ¥è©¢
            print(f"ğŸ“š æ¸¬è©¦å¤šäº¤æ˜“å°æ­·å²æ•¸æ“šæŸ¥è©¢...")
            start_time = time.time()
            
            historical_data = self.manager.get_multi_pair_historical_data(
                pairs=test_pairs,
                timeframe='5m',
                limit=50
            )
            
            historical_query_time = time.time() - start_time
            
            print(f"   â±ï¸ æŸ¥è©¢æ™‚é–“: {historical_query_time:.3f} ç§’")
            print(f"   ğŸ“Š è¿”å›æ•¸æ“š: {len(historical_data)} å€‹äº¤æ˜“å°")
            
            for pair, df in historical_data.items():
                record_count = len(df) if not df.empty else 0
                print(f"      - {pair}: {record_count} æ¢è¨˜éŒ„")
            
            # æ¸¬è©¦å¯¦æ™‚æ•¸æ“šæ‘˜è¦æŸ¥è©¢
            print(f"\nğŸ“¡ æ¸¬è©¦å¯¦æ™‚æ•¸æ“šæ‘˜è¦æŸ¥è©¢...")
            start_time = time.time()
            
            real_time_summary = self.manager.get_real_time_data_summary(pairs=test_pairs)
            
            realtime_query_time = time.time() - start_time
            
            print(f"   â±ï¸ æŸ¥è©¢æ™‚é–“: {realtime_query_time:.3f} ç§’")
            print(f"   ğŸ“Š è¿”å›æ•¸æ“š: {len(real_time_summary)} å€‹äº¤æ˜“å°")
            
            for pair, data in real_time_summary.items():
                price = data.get('price', 'N/A')
                print(f"      - {pair}: åƒ¹æ ¼ {price}")
            
            # æ¸¬è©¦åŒæ­¥ç‹€æ…‹æŸ¥è©¢
            print(f"\nğŸ“Š æ¸¬è©¦åŒæ­¥ç‹€æ…‹æŸ¥è©¢...")
            start_time = time.time()
            
            sync_summary = self.manager.get_sync_status_summary()
            
            sync_query_time = time.time() - start_time
            
            print(f"   â±ï¸ æŸ¥è©¢æ™‚é–“: {sync_query_time:.3f} ç§’")
            print(f"   ğŸ“Š ç‹€æ…‹çµ±è¨ˆ:")
            print(f"      - ç¸½äº¤æ˜“å°: {sync_summary.get('total_pairs', 0)}")
            print(f"      - æ´»èºäº¤æ˜“å°: {sync_summary.get('active_count', 0)}")
            print(f"      - éŒ¯èª¤äº¤æ˜“å°: {sync_summary.get('error_count', 0)}")
            
            # æ€§èƒ½è©•ä¼°
            total_query_time = historical_query_time + realtime_query_time + sync_query_time
            avg_query_time = total_query_time / 3
            
            performance_rating = "å„ªç§€" if avg_query_time < 0.1 else "è‰¯å¥½" if avg_query_time < 0.5 else "éœ€è¦å„ªåŒ–"
            
            print(f"ğŸ“ˆ æŸ¥è©¢æ€§èƒ½çµ±è¨ˆ:")
            print(f"   - å¹³å‡æŸ¥è©¢æ™‚é–“: {avg_query_time:.3f} ç§’")
            print(f"   - æ€§èƒ½è©•ç´š: {performance_rating}")
            print(f"   - æ­·å²æ•¸æ“šæŸ¥è©¢: {historical_query_time:.3f} ç§’")
            print(f"   - å¯¦æ™‚æ•¸æ“šæŸ¥è©¢: {realtime_query_time:.3f} ç§’")
            print(f"   - ç‹€æ…‹æŸ¥è©¢: {sync_query_time:.3f} ç§’")
            
            self.test_results['multi_pair_query_performance'] = {
                'status': 'success',
                'query_times': {
                    'historical': historical_query_time,
                    'realtime': realtime_query_time,
                    'sync_status': sync_query_time,
                    'average': avg_query_time
                },
                'data_counts': {
                    'historical_pairs': len(historical_data),
                    'realtime_pairs': len(real_time_summary),
                    'total_pairs': sync_summary.get('total_pairs', 0)
                },
                'performance_rating': performance_rating
            }
            
        except Exception as e:
            print(f"âŒ å¤šäº¤æ˜“å°æŸ¥è©¢æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results['multi_pair_query_performance'] = {
                'status': 'failed',
                'error': str(e)
            }
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆå…¨é¢æ¸¬è©¦å ±å‘Š"""
        print("\n" + "=" * 70)
        print("ğŸ“‹ å¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ± - å…¨é¢æ¸¬è©¦å ±å‘Š")
        print("=" * 70)
        
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results.values() 
                          if result.get('status') == 'success')
        partial_tests = sum(1 for result in self.test_results.values() 
                           if result.get('status') == 'partial')
        failed_tests = total_tests - passed_tests - partial_tests
        
        print(f"ğŸ“Š æ¸¬è©¦çµ±è¨ˆç¸½è¦½:")
        print(f"   - ç¸½æ¸¬è©¦æ•¸: {total_tests}")
        print(f"   - å®Œå…¨é€šé: {passed_tests}")
        print(f"   - éƒ¨åˆ†é€šé: {partial_tests}")
        print(f"   - æ¸¬è©¦å¤±æ•—: {failed_tests}")
        print(f"   - ç¸½é«”æˆåŠŸç‡: {(passed_tests + partial_tests*0.5)/total_tests*100:.1f}%")
        
        print(f"\nğŸ“‹ è©³ç´°æ¸¬è©¦çµæœ:")
        for test_name, result in self.test_results.items():
            status = result.get('status', 'unknown')
            if status == 'success':
                status_icon = "âœ…"
            elif status == 'partial':
                status_icon = "ğŸŸ¡"
            else:
                status_icon = "âŒ"
            
            print(f"   {status_icon} {test_name}: {status}")
            
            if status == 'failed':
                print(f"      éŒ¯èª¤: {result.get('error', 'Unknown error')}")
        
        # ä»»å‹™1.2å¯¦ç¾ç¢ºèª
        print(f"\nğŸ¯ ä»»å‹™1.2å¯¦ç¾ç¢ºèª:")
        implementation_checks = [
            ("ç‚ºæ¯å€‹äº¤æ˜“å°å‰µå»ºç¨ç«‹çš„æ­·å²æ•¸æ“šç®¡ç†å™¨", 
             self.test_results.get('independent_data_managers', {}).get('status') == 'success'),
            ("å¯¦ç¾å¤šäº¤æ˜“å°å¯¦æ™‚æ•¸æ“šæµä¸¦è¡Œè™•ç†", 
             self.test_results.get('parallel_data_processing', {}).get('status') == 'success'),
            ("å»ºç«‹äº¤æ˜“å°é–“æ•¸æ“šåŒæ­¥å’Œä¸€è‡´æ€§æ©Ÿåˆ¶", 
             self.test_results.get('data_sync_consistency', {}).get('status') == 'success'),
            ("å„ªåŒ–æ•¸æ“šå­˜å„²çµæ§‹ä»¥æ”¯æŒå¤šäº¤æ˜“å°æŸ¥è©¢", 
             self.test_results.get('optimized_storage_structure', {}).get('status') == 'success')
        ]
        
        for requirement, implemented in implementation_checks:
            status_icon = "âœ…" if implemented else "âŒ"
            print(f"   {status_icon} {requirement}")
        
        # æ€§èƒ½æŒ‡æ¨™
        if 'multi_pair_query_performance' in self.test_results:
            perf_data = self.test_results['multi_pair_query_performance']
            if perf_data.get('status') == 'success':
                print(f"\nâš¡ æ€§èƒ½æŒ‡æ¨™:")
                query_times = perf_data.get('query_times', {})
                print(f"   - å¹³å‡æŸ¥è©¢æ™‚é–“: {query_times.get('average', 0):.3f} ç§’")
                print(f"   - æ€§èƒ½è©•ç´š: {perf_data.get('performance_rating', 'N/A')}")
        
        # ç³»çµ±çµ±è¨ˆ
        if 'system_initialization' in self.test_results:
            init_data = self.test_results['system_initialization']
            if init_data.get('status') in ['success', 'partial']:
                print(f"\nğŸ“Š ç³»çµ±çµ±è¨ˆ:")
                print(f"   - æ”¯æŒäº¤æ˜“å°: {init_data.get('pairs_count', 0)}")
                print(f"   - ç¨ç«‹æ•¸æ“šç®¡ç†å™¨: {init_data.get('managers_count', 0)}")
        
        # æœ€çµ‚è©•ä¼°
        all_requirements_met = all(implemented for _, implemented in implementation_checks)
        
        if all_requirements_met and passed_tests >= 4:
            print(f"\nğŸ‰ ä»»å‹™1.2å¯¦ç¾æˆåŠŸï¼")
            print(f"   å¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ±å·²å®Œå…¨å¯¦ç¾æ‰€æœ‰è¦æ±‚ã€‚")
        elif passed_tests + partial_tests >= 4:
            print(f"\nâœ… ä»»å‹™1.2åŸºæœ¬å¯¦ç¾ï¼")
            print(f"   å¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ±æ ¸å¿ƒåŠŸèƒ½å·²å¯¦ç¾ï¼Œéƒ¨åˆ†åŠŸèƒ½éœ€è¦é€²ä¸€æ­¥å®Œå–„ã€‚")
        else:
            print(f"\nâš ï¸ ä»»å‹™1.2éœ€è¦æ”¹é€²ï¼")
            print(f"   éƒ¨åˆ†æ ¸å¿ƒåŠŸèƒ½æœªèƒ½æ­£ç¢ºå¯¦ç¾ï¼Œéœ€è¦é€²ä¸€æ­¥é–‹ç™¼ã€‚")
        
        # ä¿å­˜æ¸¬è©¦å ±å‘Š
        self.save_comprehensive_report()
    
    def save_comprehensive_report(self):
        """ä¿å­˜å…¨é¢æ¸¬è©¦å ±å‘Š"""
        try:
            import json
            from datetime import datetime
            
            report = {
                'test_time': datetime.now().isoformat(),
                'test_type': 'enhanced_multi_pair_data_system',
                'task': '1.2 å»ºç«‹å¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ±',
                'test_results': self.test_results,
                'summary': {
                    'total_tests': len(self.test_results),
                    'passed_tests': sum(1 for r in self.test_results.values() 
                                      if r.get('status') == 'success'),
                    'partial_tests': sum(1 for r in self.test_results.values() 
                                       if r.get('status') == 'partial'),
                    'failed_tests': sum(1 for r in self.test_results.values() 
                                      if r.get('status') == 'failed'),
                    'overall_success_rate': (
                        sum(1 for r in self.test_results.values() if r.get('status') == 'success') +
                        sum(0.5 for r in self.test_results.values() if r.get('status') == 'partial')
                    ) / len(self.test_results)
                }
            }
            
            report_path = Path("AImax/logs/enhanced_multi_pair_data_test_report.json")
            report_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ å…¨é¢æ¸¬è©¦å ±å‘Šå·²ä¿å­˜: {report_path}")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ¸¬è©¦å ±å‘Šå¤±æ•—: {e}")


async def main():
    """ä¸»å‡½æ•¸"""
    tester = EnhancedMultiPairDataTester()
    await tester.run_comprehensive_tests()


if __name__ == "__main__":
    asyncio.run(main())