#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªé©æ‡‰å­¸ç¿’æ©Ÿåˆ¶å‰µå»ºå·¥å…·
å¯¦ç¾AIæ±ºç­–çµæœçš„è‡ªå‹•åé¥‹å­¸ç¿’å’Œç­–ç•¥åƒæ•¸å‹•æ…‹èª¿æ•´
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

import json
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass, asdict

@dataclass
class LearningRecord:
    """å­¸ç¿’è¨˜éŒ„"""
    timestamp: datetime
    decision_type: str
    confidence: float
    market_conditions: Dict[str, Any]
    outcome: str  # success, failure, neutral
    pnl: float
    lesson_learned: str
    adjustment_made: Dict[str, Any]

class AdaptiveLearningEngine:
    """è‡ªé©æ‡‰å­¸ç¿’å¼•æ“"""
    
    def __init__(self):
        self.learning_records: List[LearningRecord] = []
        self.performance_metrics = {
            'total_decisions': 0,
            'successful_decisions': 0,
            'failed_decisions': 0,
            'learning_rate': 0.1,
            'adaptation_threshold': 0.6
        }
        self.current_strategy_params = self._load_default_params()
        self.learned_patterns = {}
        
    def _load_default_params(self) -> Dict[str, Any]:
        """è¼‰å…¥é»˜èªç­–ç•¥åƒæ•¸"""
        return {
            'confidence_threshold': 0.65,
            'rsi_oversold': 35,
            'rsi_overbought': 65,
            'risk_reward_ratio': 1.5,
            'ai_weights': {
                'market_scanner': 0.35,
                'deep_analyst': 0.35,
                'decision_maker': 0.30
            },
            'volatility_sensitivity': 1.0,
            'trend_following_strength': 0.7
        }
    
    def create_adaptive_framework(self) -> Dict[str, Any]:
        """å‰µå»ºè‡ªé©æ‡‰å­¸ç¿’æ¡†æ¶"""
        print("ğŸ§  å‰µå»ºè‡ªé©æ‡‰å­¸ç¿’æ¡†æ¶...")
        
        framework = {
            'framework_info': {
                'name': 'AImaxè‡ªé©æ‡‰å­¸ç¿’å¼•æ“',
                'version': '1.0',
                'created': datetime.now().isoformat(),
                'description': 'åŸºæ–¼äº¤æ˜“çµæœè‡ªå‹•èª¿æ•´AIç­–ç•¥åƒæ•¸çš„å­¸ç¿’ç³»çµ±'
            },
            'learning_mechanisms': {
                'outcome_feedback': 'è¨˜éŒ„æ¯æ¬¡æ±ºç­–çš„çµæœä¸¦åˆ†ææˆåŠŸ/å¤±æ•—æ¨¡å¼',
                'parameter_adjustment': 'åŸºæ–¼å­¸ç¿’çµæœå‹•æ…‹èª¿æ•´ç­–ç•¥åƒæ•¸',
                'pattern_recognition': 'è­˜åˆ¥æˆåŠŸäº¤æ˜“çš„å¸‚å ´æ¢ä»¶å’Œæ±ºç­–ç‰¹å¾µ',
                'continuous_optimization': 'æŒçºŒå„ªåŒ–AIæ¬Šé‡å’ŒæŠ€è¡“æŒ‡æ¨™åƒæ•¸'
            },
            'adaptation_rules': {
                'confidence_adjustment': {
                    'success_case': 'æˆåŠŸæ™‚é™ä½ä¿¡å¿ƒåº¦è¦æ±‚2%',
                    'failure_case': 'å¤±æ•—æ™‚æé«˜ä¿¡å¿ƒåº¦è¦æ±‚3%',
                    'range_limit': '0.5-0.9'
                },
                'weight_adjustment': {
                    'success_case': 'å¢åŠ æˆåŠŸæ±ºç­–é¡å‹çš„AIæ¬Šé‡',
                    'failure_case': 'å¢åŠ æ·±åº¦åˆ†æå¸«æ¬Šé‡ä»¥æé«˜æº–ç¢ºæ€§',
                    'learning_rate': '0.1'
                },
                'indicator_adjustment': {
                    'volatility_based': 'æ ¹æ“šå¸‚å ´æ³¢å‹•èª¿æ•´æŠ€è¡“æŒ‡æ¨™æ•æ„Ÿåº¦',
                    'time_based': 'æ ¹æ“šäº¤æ˜“æ™‚é–“å„ªåŒ–åƒæ•¸',
                    'trend_based': 'æ ¹æ“šè¶¨å‹¢å¼·åº¦èª¿æ•´è·Ÿéš¨ç­–ç•¥'
                }
            },
            'implementation_guide': {
                'integration_points': [
                    'åœ¨äº¤æ˜“åŸ·è¡Œå¾Œèª¿ç”¨record_decision_outcome()',
                    'å®šæœŸèª¿ç”¨analyze_learning_patterns()åˆ†æå­¸ç¿’æ•ˆæœ',
                    'ä½¿ç”¨get_current_strategy()ç²å–æœ€æ–°ç­–ç•¥åƒæ•¸',
                    'å®šæœŸèª¿ç”¨save_learning_state()ä¿å­˜å­¸ç¿’é€²åº¦'
                ],
                'monitoring_metrics': [
                    'success_rate: æ±ºç­–æˆåŠŸç‡',
                    'learning_rate: å­¸ç¿’é€Ÿåº¦',
                    'adaptation_frequency: åƒæ•¸èª¿æ•´é »ç‡',
                    'parameter_stability: åƒæ•¸èª¿æ•´ç©©å®šæ€§'
                ]
            }
        }
        
        print(f"   âœ… è‡ªé©æ‡‰å­¸ç¿’æ¡†æ¶å‰µå»ºå®Œæˆ")
        print(f"   ğŸ§  å­¸ç¿’æ©Ÿåˆ¶: {len(framework['learning_mechanisms'])} é …")
        print(f"   ğŸ“‹ é©æ‡‰è¦å‰‡: {len(framework['adaptation_rules'])} é¡")
        
        return framework
    
    def get_current_strategy(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰ç­–ç•¥åƒæ•¸"""
        return {
            'strategy_params': self.current_strategy_params.copy(),
            'performance_metrics': self.performance_metrics.copy(),
            'last_updated': datetime.now().isoformat(),
            'total_learning_records': len(self.learning_records)
        }
    
    def save_learning_state(self) -> str:
        """ä¿å­˜å­¸ç¿’ç‹€æ…‹"""
        try:
            learning_dir = Path("AImax/logs/adaptive_learning")
            learning_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # ä¿å­˜ç•¶å‰ç­–ç•¥
            strategy_file = learning_dir / f"adaptive_strategy_{timestamp}.json"
            strategy_data = {
                'strategy_params': self.current_strategy_params,
                'performance_metrics': self.performance_metrics,
                'timestamp': datetime.now().isoformat()
            }
            
            with open(strategy_file, 'w', encoding='utf-8') as f:
                json.dump(strategy_data, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"ğŸ“„ å­¸ç¿’ç‹€æ…‹å·²ä¿å­˜: {learning_dir}")
            return str(learning_dir)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å­¸ç¿’ç‹€æ…‹å¤±æ•—: {e}")
            return ""

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ§  AImax è‡ªé©æ‡‰å­¸ç¿’æ©Ÿåˆ¶å‰µå»º")
    print("=" * 40)
    
    # å‰µå»ºå­¸ç¿’å¼•æ“
    learning_engine = AdaptiveLearningEngine()
    
    # å‰µå»ºè‡ªé©æ‡‰æ¡†æ¶
    framework = learning_engine.create_adaptive_framework()
    
    # ä¿å­˜å­¸ç¿’ç‹€æ…‹
    save_path = learning_engine.save_learning_state()
    
    # é¡¯ç¤ºç•¶å‰ç­–ç•¥
    current_strategy = learning_engine.get_current_strategy()
    
    print(f"\nğŸ¯ ç•¶å‰ç­–ç•¥åƒæ•¸:")
    print(f"   ä¿¡å¿ƒåº¦é–¾å€¼: {current_strategy['strategy_params']['confidence_threshold']:.1%}")
    print(f"   RSIè¶…è³£/è¶…è²·: {current_strategy['strategy_params']['rsi_oversold']}/{current_strategy['strategy_params']['rsi_overbought']}")
    print(f"   é¢¨éšªæ”¶ç›Šæ¯”: {current_strategy['strategy_params']['risk_reward_ratio']:.1f}:1")
    
    print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ¨™:")
    print(f"   ç¸½æ±ºç­–æ•¸: {current_strategy['performance_metrics']['total_decisions']}")
    print(f"   å­¸ç¿’ç‡: {current_strategy['performance_metrics']['learning_rate']:.1%}")
    print(f"   é©æ‡‰é–¾å€¼: {current_strategy['performance_metrics']['adaptation_threshold']:.1%}")
    
    print(f"\nğŸ”§ å¯¦æ–½æŒ‡å—:")
    print("   1. åœ¨äº¤æ˜“åŸ·è¡Œå¾Œèª¿ç”¨ record_decision_outcome()")
    print("   2. å®šæœŸåˆ†æå­¸ç¿’æ¨¡å¼ analyze_learning_patterns()")
    print("   3. ç²å–æœ€æ–°ç­–ç•¥åƒæ•¸ get_current_strategy()")
    print("   4. ä¿å­˜å­¸ç¿’é€²åº¦ save_learning_state()")
    
    print(f"\nâœ… è‡ªé©æ‡‰å­¸ç¿’æ©Ÿåˆ¶å‰µå»ºå®Œæˆï¼")
    print(f"ğŸ“„ å­¸ç¿’ç‹€æ…‹ä¿å­˜è‡³: {save_path}")

if __name__ == "__main__":
    main()