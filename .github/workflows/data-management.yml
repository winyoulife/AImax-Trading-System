name: AImax æ•¸æ“šå­˜å„²å’Œç‹€æ…‹ç®¡ç†ç³»çµ±

on:
  schedule:
    # æ¯å°æ™‚åŸ·è¡Œæ•¸æ“šæ•´ç†å’Œå‚™ä»½
    - cron: '0 * * * *'
  workflow_dispatch:
    inputs:
      operation_type:
        description: 'æ“ä½œé¡å‹'
        required: true
        default: 'full_backup'
        type: choice
        options:
        - full_backup      # å®Œæ•´å‚™ä»½
        - data_cleanup     # æ•¸æ“šæ¸…ç†
        - state_sync       # ç‹€æ…‹åŒæ­¥
        - recovery         # æ•¸æ“šæ¢å¾©
      retention_days:
        description: 'æ•¸æ“šä¿ç•™å¤©æ•¸'
        required: false
        default: '30'
        type: string

env:
  DATA_MANAGEMENT_MODE: active
  PYTHON_VERSION: '3.10'
  MAX_DATA_SIZE_MB: 100  # æœ€å¤§æ•¸æ“šå¤§å°é™åˆ¶
  
jobs:
  data-storage-management:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: ğŸ”„ æª¢å‡ºä»£ç¢¼
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # ç²å–å®Œæ•´æ­·å²ï¼Œç”¨æ–¼æ•¸æ“šæ¢å¾©
        
    - name: ğŸ è¨­ç½®Pythonç’°å¢ƒ
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: ğŸ“¦ å®‰è£æ•¸æ“šè™•ç†ä¾è³´
      run: |
        python -m pip install --upgrade pip
        pip install pandas numpy pytz requests
        
    - name: ğŸ“Š å‰µå»ºäº¤æ˜“ç‹€æ…‹æ•¸æ“šçµæ§‹
      id: create_data_structure
      run: |
        echo "ğŸ“Š å‰µå»ºAImaxäº¤æ˜“ç‹€æ…‹æ•¸æ“šçµæ§‹..."
        python -c "
        import json
        import os
        from datetime import datetime, timedelta
        import pytz
        
        def create_data_structure():
            print('ğŸ“Š åˆå§‹åŒ–æ•¸æ“šå­˜å„²çµæ§‹...')
            
            # å‰µå»ºæ•¸æ“šç›®éŒ„çµæ§‹
            directories = [
                'data/trading',           # äº¤æ˜“æ•¸æ“š
                'data/monitoring',        # ç›£æ§æ•¸æ“š
                'data/keep_alive',        # ä¿æ´»æ•¸æ“š
                'data/backups',           # å‚™ä»½æ•¸æ“š
                'data/analytics',         # åˆ†ææ•¸æ“š
                'data/states',            # ç‹€æ…‹æ•¸æ“š
                'logs/trading',           # äº¤æ˜“æ—¥èªŒ
                'logs/system',            # ç³»çµ±æ—¥èªŒ
                'logs/errors'             # éŒ¯èª¤æ—¥èªŒ
            ]
            
            for directory in directories:
                os.makedirs(directory, exist_ok=True)
                print(f'ğŸ“ å‰µå»ºç›®éŒ„: {directory}')
            
            # å®šç¾©äº¤æ˜“ç‹€æ…‹æ•¸æ“šçµæ§‹
            trading_state_schema = {
                'schema_version': '1.0',
                'last_updated': datetime.now().isoformat(),
                'trading_status': {
                    'is_active': True,
                    'current_strategy': 'ultimate_optimized_85_percent',
                    'execution_mode': 'high_frequency',
                    'last_execution': None,
                    'next_scheduled': None,
                    'total_executions_today': 0,
                    'consecutive_failures': 0
                },
                'market_data': {
                    'current_btc_price': 0.0,
                    'last_price_update': None,
                    'price_change_24h': 0.0,
                    'volatility_level': 'unknown',
                    'market_trend': 'neutral'
                },
                'performance_metrics': {
                    'daily_win_rate': 0.0,
                    'total_trades': 0,
                    'successful_trades': 0,
                    'failed_trades': 0,
                    'average_confidence': 0.85,
                    'total_profit_loss': 0.0
                },
                'system_health': {
                    'github_actions_status': 'active',
                    'api_connectivity': 'unknown',
                    'data_integrity': 'good',
                    'last_health_check': None,
                    'resource_usage': {
                        'actions_minutes_used': 0,
                        'storage_mb_used': 0,
                        'api_calls_today': 0
                    }
                },
                'configuration': {
                    'max_daily_executions': 1440,
                    'high_volatility_threshold': 2.0,
                    'medium_volatility_threshold': 0.5,
                    'trading_hours': {
                        'start': '09:00',
                        'end': '13:00',
                        'timezone': 'Asia/Taipei'
                    }
                }
            }
            
            # ä¿å­˜æ•¸æ“šçµæ§‹æ¨¡æ¿
            with open('data/states/trading_state_schema.json', 'w') as f:
                json.dump(trading_state_schema, f, indent=2)
            
            # å‰µå»ºç•¶å‰ç‹€æ…‹æ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            current_state_file = 'data/states/current_trading_state.json'
            if not os.path.exists(current_state_file):
                with open(current_state_file, 'w') as f:
                    json.dump(trading_state_schema, f, indent=2)
                print('âœ… å‰µå»ºåˆå§‹äº¤æ˜“ç‹€æ…‹æ–‡ä»¶')
            
            # å‰µå»ºæ•¸æ“šç´¢å¼•æ–‡ä»¶
            data_index = {
                'created': datetime.now().isoformat(),
                'version': '1.0',
                'data_files': {
                    'trading_state': 'data/states/current_trading_state.json',
                    'daily_stats': 'data/analytics/daily_stats_{date}.json',
                    'execution_log': 'data/trading/execution_log_{date}.jsonl',
                    'price_history': 'data/trading/price_history_{date}.json',
                    'system_metrics': 'data/monitoring/system_metrics_{date}.json'
                },
                'backup_schedule': {
                    'hourly': True,
                    'daily': True,
                    'weekly': True,
                    'retention_days': 30
                }
            }
            
            with open('data/data_index.json', 'w') as f:
                json.dump(data_index, f, indent=2)
            
            print('âœ… æ•¸æ“šçµæ§‹å‰µå»ºå®Œæˆ')
            return True
        
        # åŸ·è¡Œæ•¸æ“šçµæ§‹å‰µå»º
        success = create_data_structure()
        
        # è¨­ç½®è¼¸å‡º
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f'data_structure_created={str(success).lower()}\\n')
        "
        
    - name: ğŸ”„ å¯¦ç¾ç‹€æ…‹åŒæ­¥å’Œä¸€è‡´æ€§æª¢æŸ¥
      run: |
        echo "ğŸ”„ åŸ·è¡Œç‹€æ…‹åŒæ­¥å’Œä¸€è‡´æ€§æª¢æŸ¥..."
        python -c "
        import json
        import os
        from datetime import datetime, timedelta
        import pytz
        
        def sync_and_check_consistency():
            print('ğŸ”„ é–‹å§‹ç‹€æ…‹åŒæ­¥å’Œä¸€è‡´æ€§æª¢æŸ¥...')
            
            taipei_tz = pytz.timezone('Asia/Taipei')
            now = datetime.now()
            now_taipei = now.astimezone(taipei_tz)
            today = now_taipei.strftime('%Y-%m-%d')
            
            sync_result = {
                'timestamp': now.isoformat(),
                'sync_status': 'success',
                'issues_found': [],
                'actions_taken': [],
                'data_integrity': 'good'
            }
            
            # æª¢æŸ¥ä¸¦åŒæ­¥äº¤æ˜“ç‹€æ…‹
            try:
                current_state_file = 'data/states/current_trading_state.json'
                if os.path.exists(current_state_file):
                    with open(current_state_file, 'r') as f:
                        current_state = json.load(f)
                    
                    # æ›´æ–°æ™‚é–“æˆ³
                    current_state['last_updated'] = now.isoformat()
                    
                    # æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§
                    required_keys = ['trading_status', 'market_data', 'performance_metrics', 'system_health']
                    for key in required_keys:
                        if key not in current_state:
                            sync_result['issues_found'].append(f'ç¼ºå°‘å¿…è¦å­—æ®µ: {key}')
                            current_state[key] = {}
                    
                    # åŒæ­¥é«˜é »äº¤æ˜“ç³»çµ±æ•¸æ“š
                    if os.path.exists('data/monitoring/frequency_control.json'):
                        with open('data/monitoring/frequency_control.json', 'r') as f:
                            freq_data = json.load(f)
                        
                        current_state['market_data']['current_btc_price'] = freq_data.get('btc_price', 0)
                        current_state['market_data']['volatility_level'] = freq_data.get('volatility_level', 'unknown')
                        current_state['market_data']['last_price_update'] = freq_data.get('timestamp')
                        
                        sync_result['actions_taken'].append('åŒæ­¥é«˜é »äº¤æ˜“æ•¸æ“š')
                    
                    # åŒæ­¥æ¯æ—¥åŸ·è¡Œçµ±è¨ˆ
                    daily_exec_file = f'data/monitoring/daily_executions_{today}.json'
                    if os.path.exists(daily_exec_file):
                        with open(daily_exec_file, 'r') as f:
                            daily_data = json.load(f)
                        
                        current_state['trading_status']['total_executions_today'] = daily_data.get('count', 0)
                        current_state['trading_status']['last_execution'] = daily_data.get('last_execution')
                        
                        sync_result['actions_taken'].append('åŒæ­¥æ¯æ—¥åŸ·è¡Œçµ±è¨ˆ')
                    
                    # è¨ˆç®—ç³»çµ±è³‡æºä½¿ç”¨
                    try:
                        # è¨ˆç®—å­˜å„²ä½¿ç”¨é‡
                        total_size = 0
                        for root, dirs, files in os.walk('data'):
                            for file in files:
                                file_path = os.path.join(root, file)
                                if os.path.exists(file_path):
                                    total_size += os.path.getsize(file_path)
                        
                        storage_mb = total_size / (1024 * 1024)
                        current_state['system_health']['resource_usage']['storage_mb_used'] = round(storage_mb, 2)
                        
                        if storage_mb > float(os.environ.get('MAX_DATA_SIZE_MB', 100)):
                            sync_result['issues_found'].append(f'å­˜å„²ä½¿ç”¨é‡éé«˜: {storage_mb:.1f}MB')
                        
                    except Exception as e:
                        sync_result['issues_found'].append(f'è³‡æºä½¿ç”¨è¨ˆç®—å¤±æ•—: {str(e)}')
                    
                    # ä¿å­˜æ›´æ–°å¾Œçš„ç‹€æ…‹
                    with open(current_state_file, 'w') as f:
                        json.dump(current_state, f, indent=2)
                    
                    print('âœ… äº¤æ˜“ç‹€æ…‹åŒæ­¥å®Œæˆ')
                    
                else:
                    sync_result['issues_found'].append('äº¤æ˜“ç‹€æ…‹æ–‡ä»¶ä¸å­˜åœ¨')
                    sync_result['sync_status'] = 'warning'
            
            except Exception as e:
                sync_result['issues_found'].append(f'ç‹€æ…‹åŒæ­¥å¤±æ•—: {str(e)}')
                sync_result['sync_status'] = 'error'
            
            # æª¢æŸ¥æ•¸æ“šæ–‡ä»¶å®Œæ•´æ€§
            critical_files = [
                'data/data_index.json',
                'data/states/trading_state_schema.json'
            ]
            
            for file_path in critical_files:
                if not os.path.exists(file_path):
                    sync_result['issues_found'].append(f'é—œéµæ–‡ä»¶ç¼ºå¤±: {file_path}')
                    sync_result['data_integrity'] = 'compromised'
            
            # ä¿å­˜åŒæ­¥çµæœ
            with open('data/states/last_sync_result.json', 'w') as f:
                json.dump(sync_result, f, indent=2)
            
            print(f'ğŸ”„ ç‹€æ…‹åŒæ­¥å®Œæˆï¼Œç‹€æ…‹: {sync_result[\"sync_status\"]}')
            if sync_result['issues_found']:
                print('âš ï¸ ç™¼ç¾å•é¡Œ:')
                for issue in sync_result['issues_found']:
                    print(f'  - {issue}')
            
            if sync_result['actions_taken']:
                print('âœ… åŸ·è¡Œçš„æ“ä½œ:')
                for action in sync_result['actions_taken']:
                    print(f'  - {action}')
            
            return sync_result
        
        # åŸ·è¡Œç‹€æ…‹åŒæ­¥
        result = sync_and_check_consistency()
        "
        
    - name: ğŸ’¾ å¯¦ç¾æ•¸æ“šå‚™ä»½å’Œæ¢å¾©åŠŸèƒ½
      run: |
        echo "ğŸ’¾ åŸ·è¡Œæ•¸æ“šå‚™ä»½å’Œæ¢å¾©åŠŸèƒ½..."
        python -c "
        import json
        import os
        import shutil
        from datetime import datetime, timedelta
        import pytz
        
        def backup_and_recovery():
            print('ğŸ’¾ é–‹å§‹æ•¸æ“šå‚™ä»½æµç¨‹...')
            
            now = datetime.now()
            taipei_tz = pytz.timezone('Asia/Taipei')
            now_taipei = now.astimezone(taipei_tz)
            
            backup_result = {
                'timestamp': now.isoformat(),
                'backup_type': os.environ.get('INPUT_OPERATION_TYPE', 'full_backup'),
                'status': 'success',
                'files_backed_up': [],
                'backup_size_mb': 0,
                'retention_applied': False
            }
            
            # å‰µå»ºå‚™ä»½ç›®éŒ„
            backup_dir = f'data/backups/{now_taipei.strftime(\"%Y-%m-%d_%H-%M-%S\")}'
            os.makedirs(backup_dir, exist_ok=True)
            
            # è¦å‚™ä»½çš„é—œéµæ•¸æ“š
            backup_sources = [
                ('data/states/', 'states/'),
                ('data/trading/', 'trading/'),
                ('data/monitoring/', 'monitoring/'),
                ('data/analytics/', 'analytics/'),
                ('data/data_index.json', 'data_index.json')
            ]
            
            total_size = 0
            
            for source, dest in backup_sources:
                source_path = source
                dest_path = os.path.join(backup_dir, dest)
                
                try:
                    if os.path.isfile(source_path):
                        # å‚™ä»½å–®å€‹æ–‡ä»¶
                        os.makedirs(os.path.dirname(dest_path), exist_ok=True)
                        shutil.copy2(source_path, dest_path)
                        file_size = os.path.getsize(source_path)
                        total_size += file_size
                        backup_result['files_backed_up'].append(source_path)
                        
                    elif os.path.isdir(source_path):
                        # å‚™ä»½ç›®éŒ„
                        if os.path.exists(source_path):
                            shutil.copytree(source_path, dest_path, dirs_exist_ok=True)
                            
                            # è¨ˆç®—ç›®éŒ„å¤§å°
                            for root, dirs, files in os.walk(dest_path):
                                for file in files:
                                    file_path = os.path.join(root, file)
                                    if os.path.exists(file_path):
                                        total_size += os.path.getsize(file_path)
                            
                            backup_result['files_backed_up'].append(source_path)
                
                except Exception as e:
                    print(f'âš ï¸ å‚™ä»½å¤±æ•— {source_path}: {str(e)}')
                    backup_result['status'] = 'partial'
            
            backup_result['backup_size_mb'] = round(total_size / (1024 * 1024), 2)
            
            # å‰µå»ºå‚™ä»½å…ƒæ•¸æ“š
            backup_metadata = {
                'created': now.isoformat(),
                'taipei_time': now_taipei.isoformat(),
                'backup_type': backup_result['backup_type'],
                'total_files': len(backup_result['files_backed_up']),
                'total_size_mb': backup_result['backup_size_mb'],
                'retention_days': int(os.environ.get('INPUT_RETENTION_DAYS', 30)),
                'system_info': {
                    'github_run_id': os.environ.get('GITHUB_RUN_ID', 'unknown'),
                    'github_run_number': os.environ.get('GITHUB_RUN_NUMBER', 'unknown')
                }
            }
            
            with open(os.path.join(backup_dir, 'backup_metadata.json'), 'w') as f:
                json.dump(backup_metadata, f, indent=2)
            
            # æ‡‰ç”¨ä¿ç•™æ”¿ç­–
            retention_days = int(os.environ.get('INPUT_RETENTION_DAYS', 30))
            cutoff_date = now - timedelta(days=retention_days)
            
            if os.path.exists('data/backups'):
                for backup_folder in os.listdir('data/backups'):
                    backup_path = os.path.join('data/backups', backup_folder)
                    if os.path.isdir(backup_path):
                        try:
                            # å¾æ–‡ä»¶å¤¾åç¨±è§£ææ—¥æœŸ
                            folder_date = datetime.strptime(backup_folder[:10], '%Y-%m-%d')
                            if folder_date < cutoff_date:
                                shutil.rmtree(backup_path)
                                print(f'ğŸ—‘ï¸ åˆªé™¤éæœŸå‚™ä»½: {backup_folder}')
                                backup_result['retention_applied'] = True
                        except ValueError:
                            # ç„¡æ³•è§£ææ—¥æœŸçš„æ–‡ä»¶å¤¾è·³é
                            continue
            
            # ä¿å­˜å‚™ä»½çµæœ
            with open('data/states/last_backup_result.json', 'w') as f:
                json.dump(backup_result, f, indent=2)
            
            print(f'ğŸ’¾ å‚™ä»½å®Œæˆ: {len(backup_result[\"files_backed_up\"])}å€‹é …ç›®')
            print(f'ğŸ“Š å‚™ä»½å¤§å°: {backup_result[\"backup_size_mb\"]}MB')
            print(f'ğŸ“ å‚™ä»½ä½ç½®: {backup_dir}')
            
            return backup_result
        
        # åŸ·è¡Œå‚™ä»½
        result = backup_and_recovery()
        "
        
    - name: ğŸ“ˆ ç”Ÿæˆæ•¸æ“šåˆ†æå ±å‘Š
      run: |
        echo "ğŸ“ˆ ç”Ÿæˆæ•¸æ“šåˆ†æå’Œçµ±è¨ˆå ±å‘Š..."
        python -c "
        import json
        import os
        from datetime import datetime, timedelta
        import pytz
        
        def generate_analytics_report():
            print('ğŸ“ˆ ç”ŸæˆAImaxæ•¸æ“šåˆ†æå ±å‘Š...')
            
            now = datetime.now()
            taipei_tz = pytz.timezone('Asia/Taipei')
            now_taipei = now.astimezone(taipei_tz)
            today = now_taipei.strftime('%Y-%m-%d')
            
            analytics_report = {
                'report_date': today,
                'generated_at': now.isoformat(),
                'taipei_time': now_taipei.isoformat(),
                'system_overview': {},
                'trading_performance': {},
                'data_health': {},
                'recommendations': []
            }
            
            # ç³»çµ±æ¦‚è¦½
            try:
                if os.path.exists('data/states/current_trading_state.json'):
                    with open('data/states/current_trading_state.json', 'r') as f:
                        current_state = json.load(f)
                    
                    analytics_report['system_overview'] = {
                        'trading_active': current_state.get('trading_status', {}).get('is_active', False),
                        'current_strategy': current_state.get('trading_status', {}).get('current_strategy', 'unknown'),
                        'execution_mode': current_state.get('trading_status', {}).get('execution_mode', 'unknown'),
                        'executions_today': current_state.get('trading_status', {}).get('total_executions_today', 0),
                        'current_btc_price': current_state.get('market_data', {}).get('current_btc_price', 0),
                        'volatility_level': current_state.get('market_data', {}).get('volatility_level', 'unknown')
                    }
            except Exception as e:
                analytics_report['system_overview']['error'] = str(e)
            
            # äº¤æ˜“æ€§èƒ½åˆ†æ
            try:
                # åˆ†æä»Šæ—¥äº¤æ˜“çµ±è¨ˆ
                daily_stats_file = f'data/trading/daily_stats_{today}.json'
                if os.path.exists(daily_stats_file):
                    with open(daily_stats_file, 'r') as f:
                        daily_stats = json.load(f)
                    
                    analytics_report['trading_performance'] = {
                        'total_executions': daily_stats.get('total_executions', 0),
                        'high_volatility_executions': daily_stats.get('high_volatility_executions', 0),
                        'medium_volatility_executions': daily_stats.get('medium_volatility_executions', 0),
                        'low_volatility_executions': daily_stats.get('low_volatility_executions', 0),
                        'avg_confidence': daily_stats.get('avg_confidence', 0.85)
                    }
                    
                    # è¨ˆç®—åŸ·è¡Œæ•ˆç‡
                    total_exec = daily_stats.get('total_executions', 0)
                    if total_exec > 0:
                        high_vol_ratio = daily_stats.get('high_volatility_executions', 0) / total_exec
                        analytics_report['trading_performance']['high_volatility_ratio'] = round(high_vol_ratio, 3)
                        
                        if high_vol_ratio > 0.3:
                            analytics_report['recommendations'].append('é«˜æ³¢å‹•æœŸåŸ·è¡Œæ¯”ä¾‹è¼ƒé«˜ï¼Œç³»çµ±éŸ¿æ‡‰è‰¯å¥½')
                        elif high_vol_ratio < 0.1:
                            analytics_report['recommendations'].append('å»ºè­°æª¢æŸ¥æ³¢å‹•æ€§æª¢æ¸¬é‚è¼¯')
                
            except Exception as e:
                analytics_report['trading_performance']['error'] = str(e)
            
            # æ•¸æ“šå¥åº·æª¢æŸ¥
            try:
                data_health = {
                    'total_data_files': 0,
                    'total_size_mb': 0,
                    'backup_status': 'unknown',
                    'last_backup': None,
                    'data_integrity': 'good'
                }
                
                # çµ±è¨ˆæ•¸æ“šæ–‡ä»¶
                for root, dirs, files in os.walk('data'):
                    data_health['total_data_files'] += len(files)
                    for file in files:
                        file_path = os.path.join(root, file)
                        if os.path.exists(file_path):
                            data_health['total_size_mb'] += os.path.getsize(file_path)
                
                data_health['total_size_mb'] = round(data_health['total_size_mb'] / (1024 * 1024), 2)
                
                # æª¢æŸ¥å‚™ä»½ç‹€æ…‹
                if os.path.exists('data/states/last_backup_result.json'):
                    with open('data/states/last_backup_result.json', 'r') as f:
                        backup_result = json.load(f)
                    
                    data_health['backup_status'] = backup_result.get('status', 'unknown')
                    data_health['last_backup'] = backup_result.get('timestamp')
                
                analytics_report['data_health'] = data_health
                
                # æ•¸æ“šå¥åº·å»ºè­°
                if data_health['total_size_mb'] > 80:
                    analytics_report['recommendations'].append('æ•¸æ“šä½¿ç”¨é‡æ¥è¿‘é™åˆ¶ï¼Œå»ºè­°åŸ·è¡Œæ¸…ç†')
                
                if data_health['backup_status'] != 'success':
                    analytics_report['recommendations'].append('æœ€è¿‘å‚™ä»½æœªæˆåŠŸï¼Œå»ºè­°æª¢æŸ¥å‚™ä»½ç³»çµ±')
                
            except Exception as e:
                analytics_report['data_health']['error'] = str(e)
            
            # ä¿å­˜åˆ†æå ±å‘Š
            os.makedirs('data/analytics', exist_ok=True)
            report_file = f'data/analytics/daily_analytics_{today}.json'
            with open(report_file, 'w') as f:
                json.dump(analytics_report, f, indent=2)
            
            # ä¿å­˜æœ€æ–°å ±å‘Šå‰¯æœ¬
            with open('data/analytics/latest_analytics.json', 'w') as f:
                json.dump(analytics_report, f, indent=2)
            
            print('ğŸ“ˆ === AImax æ•¸æ“šåˆ†æå ±å‘Š ===')
            print(f'ğŸ“… å ±å‘Šæ—¥æœŸ: {today}')
            print(f'ğŸ¤– äº¤æ˜“ç­–ç•¥: {analytics_report[\"system_overview\"].get(\"current_strategy\", \"unknown\")}')
            print(f'ğŸ“Š ä»Šæ—¥åŸ·è¡Œ: {analytics_report[\"system_overview\"].get(\"executions_today\", 0)}æ¬¡')
            print(f'ğŸ’° ç•¶å‰BTC: NT\${analytics_report[\"system_overview\"].get(\"current_btc_price\", 0):,.0f}')
            print(f'ğŸ“ˆ æ³¢å‹•æ€§: {analytics_report[\"system_overview\"].get(\"volatility_level\", \"unknown\")}')
            print(f'ğŸ’¾ æ•¸æ“šå¤§å°: {analytics_report[\"data_health\"].get(\"total_size_mb\", 0)}MB')
            
            if analytics_report['recommendations']:
                print('ğŸ’¡ å»ºè­°:')
                for rec in analytics_report['recommendations']:
                    print(f'  - {rec}')
            
            return analytics_report
        
        # ç”Ÿæˆåˆ†æå ±å‘Š
        report = generate_analytics_report()
        "
        
    - name: ğŸ“¤ ä¿å­˜æ•¸æ“šç®¡ç†çµæœ
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: data-management-${{ github.run_number }}
        path: |
          data/
          logs/
        retention-days: 7
        
    - name: ğŸ”§ æ•¸æ“šæ¸…ç†å’Œå„ªåŒ–
      if: github.event.inputs.operation_type == 'data_cleanup'
      run: |
        echo "ğŸ”§ åŸ·è¡Œæ•¸æ“šæ¸…ç†å’Œå„ªåŒ–..."
        python -c "
        import json
        import os
        import shutil
        from datetime import datetime, timedelta
        
        def cleanup_and_optimize():
            print('ğŸ”§ é–‹å§‹æ•¸æ“šæ¸…ç†å’Œå„ªåŒ–...')
            
            cleanup_result = {
                'timestamp': datetime.now().isoformat(),
                'files_cleaned': [],
                'space_freed_mb': 0,
                'optimization_applied': []
            }
            
            # æ¸…ç†éæœŸçš„è‡¨æ™‚æ–‡ä»¶
            temp_patterns = [
                'data/keep_alive/keep_alive_log.json',
                'data/monitoring/frequency_control.json'
            ]
            
            for pattern in temp_patterns:
                if os.path.exists(pattern):
                    try:
                        # æª¢æŸ¥æ–‡ä»¶å¹´é½¡
                        file_age = datetime.now() - datetime.fromtimestamp(os.path.getmtime(pattern))
                        if file_age.total_seconds() > 86400:  # è¶…é24å°æ™‚
                            file_size = os.path.getsize(pattern)
                            os.remove(pattern)
                            cleanup_result['files_cleaned'].append(pattern)
                            cleanup_result['space_freed_mb'] += file_size / (1024 * 1024)
                    except Exception as e:
                        print(f'âš ï¸ æ¸…ç†æ–‡ä»¶å¤±æ•— {pattern}: {str(e)}')
            
            # å£“ç¸®èˆŠçš„æ—¥èªŒæ–‡ä»¶
            log_dirs = ['logs/trading', 'logs/system', 'logs/errors']
            for log_dir in log_dirs:
                if os.path.exists(log_dir):
                    for file in os.listdir(log_dir):
                        file_path = os.path.join(log_dir, file)
                        if os.path.isfile(file_path) and file.endswith('.log'):
                            try:
                                file_age = datetime.now() - datetime.fromtimestamp(os.path.getmtime(file_path))
                                if file_age.days > 7:  # è¶…é7å¤©çš„æ—¥èªŒ
                                    # é€™è£¡å¯ä»¥æ·»åŠ å£“ç¸®é‚è¼¯
                                    cleanup_result['optimization_applied'].append(f'æ¨™è¨˜å£“ç¸®: {file_path}')
                            except Exception as e:
                                print(f'âš ï¸ è™•ç†æ—¥èªŒæ–‡ä»¶å¤±æ•— {file_path}: {str(e)}')
            
            cleanup_result['space_freed_mb'] = round(cleanup_result['space_freed_mb'], 2)
            
            # ä¿å­˜æ¸…ç†çµæœ
            with open('data/states/last_cleanup_result.json', 'w') as f:
                json.dump(cleanup_result, f, indent=2)
            
            print(f'ğŸ”§ æ¸…ç†å®Œæˆ: é‡‹æ”¾ {cleanup_result[\"space_freed_mb\"]}MB ç©ºé–“')
            print(f'ğŸ“ æ¸…ç†æ–‡ä»¶: {len(cleanup_result[\"files_cleaned\"])}å€‹')
            
            return cleanup_result
        
        # åŸ·è¡Œæ¸…ç†
        result = cleanup_and_optimize()
        "