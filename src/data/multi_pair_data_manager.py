#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ± - çµ±ä¸€ç®¡ç†å¤šå€‹äº¤æ˜“å°çš„æ­·å²å’Œå¯¦æ™‚æ•¸æ“š
åŸºæ–¼æœ¬åœ°AI + MAXæ­·å²æ•¸æ“š + è¶…é€²åŒ–Pythonçš„æ ¸å¿ƒç†å¿µ
"""

import asyncio
import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import logging
import json
from dataclasses import dataclass, asdict
import threading
from concurrent.futures import ThreadPoolExecutor

try:
    from .multi_pair_max_client import MultiPairMAXClient, create_multi_pair_client
    from .trading_pair_manager import TradingPairManager, create_trading_pair_manager
    from .historical_data_manager import HistoricalDataManager
except ImportError:
    # ç”¨æ–¼ç›´æ¥é‹è¡Œæ¸¬è©¦
    from multi_pair_max_client import MultiPairMAXClient, create_multi_pair_client
    from trading_pair_manager import TradingPairManager, create_trading_pair_manager
    from historical_data_manager import HistoricalDataManager

logger = logging.getLogger(__name__)

@dataclass
class DataStreamConfig:
    """æ•¸æ“šæµé…ç½®"""
    pair: str
    timeframes: List[str]
    update_intervals: Dict[str, int]  # ç§’
    buffer_size: int = 1000
    enabled: bool = True

@dataclass
class DataSyncStatus:
    """æ•¸æ“šåŒæ­¥ç‹€æ…‹"""
    pair: str
    last_sync: datetime
    sync_status: str  # 'synced', 'syncing', 'error'
    records_synced: int
    error_message: Optional[str] = None

class MultiPairDataManager:
    """å¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ± - å¢å¼·ç‰ˆ"""
    
    def __init__(self, db_path: str = "data/multi_pair_market.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ ¸å¿ƒçµ„ä»¶
        self.max_client = create_multi_pair_client()
        self.pair_manager = create_trading_pair_manager()
        
        # ç‚ºæ¯å€‹äº¤æ˜“å°å‰µå»ºç¨ç«‹çš„æ­·å²æ•¸æ“šç®¡ç†å™¨
        self.pair_data_managers: Dict[str, HistoricalDataManager] = {}
        
        # æ•¸æ“šæµé…ç½®
        self.stream_configs: Dict[str, DataStreamConfig] = {}
        self.sync_status: Dict[str, DataSyncStatus] = {}
        
        # æ•¸æ“šç·©å­˜å’ŒåŒæ­¥ - å¢å¼·ç‰ˆ
        self.data_buffers: Dict[str, Dict[str, List]] = {}
        self.sync_locks: Dict[str, threading.Lock] = {}
        self.data_consistency_cache: Dict[str, Dict[str, Any]] = {}
        
        # ä¸¦ç™¼æ§åˆ¶ - å„ªåŒ–
        self.executor = ThreadPoolExecutor(max_workers=8)  # å¢åŠ ç·šç¨‹æ•¸
        self.sync_interval = 300  # 5åˆ†é˜åŒæ­¥ä¸€æ¬¡
        self.is_running = False
        
        # æ•¸æ“šåŒæ­¥å”èª¿å™¨
        self.sync_coordinator = DataSyncCoordinator()
        
        # åˆå§‹åŒ–
        self._init_database()
        self._initialize_stream_configs()
        self._initialize_pair_data_managers()
        
        logger.info("ğŸš€ å¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ± (å¢å¼·ç‰ˆ) åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_pair_data_managers(self):
        """ç‚ºæ¯å€‹äº¤æ˜“å°å‰µå»ºç¨ç«‹çš„æ­·å²æ•¸æ“šç®¡ç†å™¨"""
        try:
            for pair in self.stream_configs.keys():
                # ç‚ºæ¯å€‹äº¤æ˜“å°å‰µå»ºå°ˆç”¨çš„æ•¸æ“šåº«è·¯å¾‘
                pair_db_path = f"data/{pair.lower()}_history.db"
                
                # å‰µå»ºç¨ç«‹çš„æ­·å²æ•¸æ“šç®¡ç†å™¨
                self.pair_data_managers[pair] = HistoricalDataManager(
                    db_path=pair_db_path,
                    pair=pair
                )
                
                logger.debug(f"âœ… ç‚º {pair} å‰µå»ºç¨ç«‹æ•¸æ“šç®¡ç†å™¨")
            
            logger.info(f"ğŸ“Š ç‚º {len(self.pair_data_managers)} å€‹äº¤æ˜“å°å‰µå»ºç¨ç«‹æ•¸æ“šç®¡ç†å™¨")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–äº¤æ˜“å°æ•¸æ“šç®¡ç†å™¨å¤±æ•—: {e}")


class DataSyncCoordinator:
    """æ•¸æ“šåŒæ­¥å”èª¿å™¨ - ç®¡ç†å¤šäº¤æ˜“å°é–“çš„æ•¸æ“šåŒæ­¥å’Œä¸€è‡´æ€§"""
    
    def __init__(self):
        self.sync_queue = asyncio.Queue()
        self.sync_locks = {}
        self.cross_pair_correlations = {}
        self.sync_priorities = {}
        
        logger.info("ğŸ”„ æ•¸æ“šåŒæ­¥å”èª¿å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def coordinate_sync(self, pairs: List[str], sync_type: str = 'incremental'):
        """å”èª¿å¤šäº¤æ˜“å°åŒæ­¥"""
        try:
            # æ ¹æ“šå„ªå…ˆç´šæ’åº
            sorted_pairs = self._sort_pairs_by_priority(pairs)
            
            # ä¸¦è¡ŒåŒæ­¥ï¼Œä½†æ§åˆ¶ä¸¦ç™¼æ•¸
            semaphore = asyncio.Semaphore(3)  # æœ€å¤š3å€‹ä¸¦ç™¼åŒæ­¥
            
            tasks = []
            for pair in sorted_pairs:
                task = self._sync_with_semaphore(semaphore, pair, sync_type)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # æª¢æŸ¥åŒæ­¥çµæœ
            success_count = sum(1 for r in results if not isinstance(r, Exception))
            logger.info(f"âœ… å”èª¿åŒæ­¥å®Œæˆ: {success_count}/{len(pairs)} æˆåŠŸ")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ å”èª¿åŒæ­¥å¤±æ•—: {e}")
            return []
    
    async def _sync_with_semaphore(self, semaphore: asyncio.Semaphore, 
                                 pair: str, sync_type: str):
        """ä½¿ç”¨ä¿¡è™Ÿé‡æ§åˆ¶çš„åŒæ­¥"""
        async with semaphore:
            return await self._sync_single_pair(pair, sync_type)
    
    async def _sync_single_pair(self, pair: str, sync_type: str):
        """åŒæ­¥å–®å€‹äº¤æ˜“å°"""
        try:
            # ç²å–åŒæ­¥é–
            if pair not in self.sync_locks:
                self.sync_locks[pair] = asyncio.Lock()
            
            async with self.sync_locks[pair]:
                logger.debug(f"ğŸ”„ é–‹å§‹åŒæ­¥ {pair} ({sync_type})")
                
                # æ¨¡æ“¬åŒæ­¥éç¨‹
                await asyncio.sleep(1)
                
                logger.debug(f"âœ… å®ŒæˆåŒæ­¥ {pair}")
                return {'pair': pair, 'status': 'success', 'type': sync_type}
                
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥ {pair} å¤±æ•—: {e}")
            return {'pair': pair, 'status': 'error', 'error': str(e)}
    
    def _sort_pairs_by_priority(self, pairs: List[str]) -> List[str]:
        """æ ¹æ“šå„ªå…ˆç´šæ’åºäº¤æ˜“å°"""
        # é»˜èªå„ªå…ˆç´šï¼šä¸»æµå¹£ç¨®å„ªå…ˆ
        priority_map = {
            'BTCTWD': 1,
            'ETHTWD': 2,
            'LTCTWD': 3,
            'BCHTWD': 4,
            'ADATWD': 5,
            'DOTTWD': 6
        }
        
        return sorted(pairs, key=lambda p: priority_map.get(p, 999))
    
    def update_cross_pair_correlations(self, correlations: Dict[str, Dict[str, float]]):
        """æ›´æ–°äº¤æ˜“å°é–“ç›¸é—œæ€§"""
        self.cross_pair_correlations = correlations
        logger.debug(f"ğŸ“Š æ›´æ–°äº¤æ˜“å°ç›¸é—œæ€§: {len(correlations)} å°")
    
    def get_sync_recommendations(self, pair: str) -> Dict[str, Any]:
        """ç²å–åŒæ­¥å»ºè­°"""
        try:
            recommendations = {
                'priority': self.sync_priorities.get(pair, 'normal'),
                'correlated_pairs': [],
                'sync_frequency': 'normal'
            }
            
            # æŸ¥æ‰¾é«˜ç›¸é—œæ€§çš„äº¤æ˜“å°
            if pair in self.cross_pair_correlations:
                for other_pair, correlation in self.cross_pair_correlations[pair].items():
                    if abs(correlation) > 0.7:  # é«˜ç›¸é—œæ€§é–¾å€¼
                        recommendations['correlated_pairs'].append({
                            'pair': other_pair,
                            'correlation': correlation
                        })
            
            return recommendations
            
        except Exception as e:
            logger.error(f"âŒ ç²å– {pair} åŒæ­¥å»ºè­°å¤±æ•—: {e}")
            return {}


# å°‡_init_databaseæ–¹æ³•ç§»å›MultiPairDataManageré¡ä¸­
# é€™å€‹æ–¹æ³•æ‡‰è©²åœ¨MultiPairDataManageré¡å…§éƒ¨ï¼Œè€Œä¸æ˜¯åœ¨DataSyncCoordinatoré¡ä¸­

# è®“æˆ‘å€‘åœ¨MultiPairDataManageré¡ä¸­æ·»åŠ ç¼ºå¤±çš„æ–¹æ³•
class MultiPairDataManagerMethods:
    """MultiPairDataManagerçš„é¡å¤–æ–¹æ³• - è‡¨æ™‚ä¿®å¾©"""
    
    def _init_database(self):
        """åˆå§‹åŒ–å¤šäº¤æ˜“å°æ•¸æ“šåº«"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # å‰µå»ºå¤šäº¤æ˜“å°Kç·šæ•¸æ“šè¡¨
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS multi_pair_klines (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        pair TEXT NOT NULL,
                        timeframe TEXT NOT NULL,
                        timestamp INTEGER NOT NULL,
                        open REAL NOT NULL,
                        high REAL NOT NULL,
                        low REAL NOT NULL,
                        close REAL NOT NULL,
                        volume REAL NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(pair, timeframe, timestamp)
                    )
                ''')
                
                # å‰µå»ºå¯¦æ™‚æ•¸æ“šè¡¨
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS real_time_data (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        pair TEXT NOT NULL,
                        timestamp INTEGER NOT NULL,
                        price REAL NOT NULL,
                        volume REAL NOT NULL,
                        bid REAL,
                        ask REAL,
                        technical_data TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(pair, timestamp)
                    )
                ''')
                
                # å‰µå»ºæ•¸æ“šåŒæ­¥æ—¥èªŒè¡¨
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS sync_log (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        pair TEXT NOT NULL,
                        sync_type TEXT NOT NULL,
                        start_time TIMESTAMP NOT NULL,
                        end_time TIMESTAMP,
                        records_processed INTEGER DEFAULT 0,
                        status TEXT NOT NULL,
                        error_message TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
                
                # å‰µå»ºç´¢å¼•
                cursor.execute('''
                    CREATE INDEX IF NOT EXISTS idx_multi_klines_pair_timeframe_timestamp 
                    ON multi_pair_klines(pair, timeframe, timestamp)
                ''')
                
                cursor.execute('''
                    CREATE INDEX IF NOT EXISTS idx_realtime_pair_timestamp 
                    ON real_time_data(pair, timestamp)
                ''')
                
                conn.commit()
                logger.info("âœ… å¤šäº¤æ˜“å°æ•¸æ“šåº«åˆå§‹åŒ–å®Œæˆ")
                
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šåº«åˆå§‹åŒ–å¤±æ•—: {e}")
            raise    

    def _initialize_stream_configs(self):
        """åˆå§‹åŒ–æ•¸æ“šæµé…ç½®"""
        try:
            # ç²å–æ‰€æœ‰æ´»èºäº¤æ˜“å°
            active_pairs = self.max_client.get_active_pairs()
            
            for pair in active_pairs:
                # ç‚ºæ¯å€‹äº¤æ˜“å°å‰µå»ºæ•¸æ“šæµé…ç½®
                config = DataStreamConfig(
                    pair=pair,
                    timeframes=['1m', '5m', '1h'],
                    update_intervals={'1m': 60, '5m': 300, '1h': 3600},
                    buffer_size=1000,
                    enabled=True
                )
                
                self.stream_configs[pair] = config
                self.sync_status[pair] = DataSyncStatus(
                    pair=pair,
                    last_sync=datetime.now(),
                    sync_status='initialized',
                    records_synced=0
                )
                
                # åˆå§‹åŒ–æ•¸æ“šç·©å­˜å’Œé–
                self.data_buffers[pair] = {tf: [] for tf in config.timeframes}
                self.sync_locks[pair] = threading.Lock()
            
            logger.info(f"ğŸ“Š åˆå§‹åŒ– {len(active_pairs)} å€‹äº¤æ˜“å°çš„æ•¸æ“šæµé…ç½®")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–æ•¸æ“šæµé…ç½®å¤±æ•—: {e}")
    
    async def start_data_streams(self):
        """å•Ÿå‹•æ‰€æœ‰æ•¸æ“šæµ"""
        if self.is_running:
            logger.warning("âš ï¸ æ•¸æ“šæµå·²åœ¨é‹è¡Œä¸­")
            return
        
        self.is_running = True
        logger.info("ğŸš€ å•Ÿå‹•å¤šäº¤æ˜“å°æ•¸æ“šæµ...")
        
        try:
            # å‰µå»ºä¸¦ç™¼ä»»å‹™
            tasks = []
            
            # å¯¦æ™‚æ•¸æ“šç²å–ä»»å‹™
            tasks.append(asyncio.create_task(self._real_time_data_loop()))
            
            # æ­·å²æ•¸æ“šåŒæ­¥ä»»å‹™
            tasks.append(asyncio.create_task(self._historical_sync_loop()))
            
            # æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥ä»»å‹™
            tasks.append(asyncio.create_task(self._consistency_check_loop()))
            
            # ç­‰å¾…æ‰€æœ‰ä»»å‹™
            await asyncio.gather(*tasks)
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šæµé‹è¡Œå¤±æ•—: {e}")
        finally:
            self.is_running = False
    
    async def _real_time_data_loop(self):
        """å¯¦æ™‚æ•¸æ“šç²å–å¾ªç’°"""
        logger.info("ğŸ“¡ å•Ÿå‹•å¯¦æ™‚æ•¸æ“šç²å–å¾ªç’°")
        
        while self.is_running:
            try:
                # ç²å–æ‰€æœ‰äº¤æ˜“å°çš„å¯¦æ™‚æ•¸æ“š
                market_data = await self.max_client.get_multi_pair_market_data()
                
                if market_data:
                    # ä¸¦è¡Œè™•ç†æ¯å€‹äº¤æ˜“å°çš„æ•¸æ“š
                    tasks = []
                    for pair, data in market_data.items():
                        task = self._process_real_time_data(pair, data)
                        tasks.append(task)
                    
                    await asyncio.gather(*tasks, return_exceptions=True)
                
                # ç­‰å¾…ä¸‹ä¸€æ¬¡æ›´æ–°
                await asyncio.sleep(30)  # 30ç§’æ›´æ–°ä¸€æ¬¡å¯¦æ™‚æ•¸æ“š
                
            except Exception as e:
                logger.error(f"âŒ å¯¦æ™‚æ•¸æ“šå¾ªç’°éŒ¯èª¤: {e}")
                await asyncio.sleep(60)  # éŒ¯èª¤æ™‚ç­‰å¾…æ›´é•·æ™‚é–“
    
    async def _process_real_time_data(self, pair: str, data: Dict[str, Any]):
        """è™•ç†å–®å€‹äº¤æ˜“å°çš„å¯¦æ™‚æ•¸æ“š"""
        try:
            if pair not in self.stream_configs:
                return
            
            # æå–é—œéµæ•¸æ“š
            timestamp = int(data.get('timestamp', datetime.now()).timestamp())
            price = data.get('current_price', 0)
            volume = data.get('volume_24h', 0)
            bid = data.get('bid', price)
            ask = data.get('ask', price)
            
            # æŠ€è¡“æŒ‡æ¨™æ•¸æ“š
            technical_data = {
                'rsi': data.get('rsi', 50),
                'macd': data.get('macd', 0),
                'bollinger_position': data.get('bollinger_position', 0.5),
                'volume_ratio': data.get('volume_ratio', 1.0),
                'volatility': data.get('volatility', 0.02)
            }
            
            # ä¿å­˜å¯¦æ™‚æ•¸æ“š
            await self._save_real_time_data(pair, timestamp, price, volume, 
                                          bid, ask, technical_data)
            
            # æ›´æ–°åŒæ­¥ç‹€æ…‹
            if pair in self.sync_status:
                self.sync_status[pair].last_sync = datetime.now()
                self.sync_status[pair].sync_status = 'synced'
                self.sync_status[pair].records_synced += 1
            
        except Exception as e:
            logger.error(f"âŒ è™•ç† {pair} å¯¦æ™‚æ•¸æ“šå¤±æ•—: {e}")
            if pair in self.sync_status:
                self.sync_status[pair].sync_status = 'error'
                self.sync_status[pair].error_message = str(e)
    
    async def _save_real_time_data(self, pair: str, timestamp: int, price: float,
                                 volume: float, bid: float, ask: float,
                                 technical_data: Dict[str, Any]):
        """ä¿å­˜å¯¦æ™‚æ•¸æ“šåˆ°æ•¸æ“šåº«"""
        try:
            def save_to_db():
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute('''
                        INSERT OR REPLACE INTO real_time_data 
                        (pair, timestamp, price, volume, bid, ask, technical_data)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (pair, timestamp, price, volume, bid, ask, 
                          json.dumps(technical_data)))
                    conn.commit()
            
            # åœ¨ç·šç¨‹æ± ä¸­åŸ·è¡Œæ•¸æ“šåº«æ“ä½œ
            await asyncio.get_event_loop().run_in_executor(
                self.executor, save_to_db
            )
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ {pair} å¯¦æ™‚æ•¸æ“šå¤±æ•—: {e}")
    
    async def _historical_sync_loop(self):
        """æ­·å²æ•¸æ“šåŒæ­¥å¾ªç’°"""
        logger.info("ğŸ“š å•Ÿå‹•æ­·å²æ•¸æ“šåŒæ­¥å¾ªç’°")
        
        while self.is_running:
            try:
                # ç‚ºæ¯å€‹äº¤æ˜“å°åŒæ­¥æ­·å²æ•¸æ“š
                for pair in self.stream_configs:
                    if self.stream_configs[pair].enabled:
                        await self._sync_pair_historical_data(pair)
                        
                        # é¿å…APIé™åˆ¶
                        await asyncio.sleep(2)
                
                # ç­‰å¾…ä¸‹ä¸€æ¬¡åŒæ­¥
                await asyncio.sleep(self.sync_interval)
                
            except Exception as e:
                logger.error(f"âŒ æ­·å²æ•¸æ“šåŒæ­¥å¾ªç’°éŒ¯èª¤: {e}")
                await asyncio.sleep(300)  # éŒ¯èª¤æ™‚ç­‰å¾…5åˆ†é˜    

    async def _sync_pair_historical_data(self, pair: str):
        """åŒæ­¥å–®å€‹äº¤æ˜“å°çš„æ­·å²æ•¸æ“š"""
        try:
            config = self.stream_configs[pair]
            
            # è¨˜éŒ„åŒæ­¥é–‹å§‹
            sync_id = await self._log_sync_start(pair, 'historical')
            
            total_synced = 0
            
            for timeframe in config.timeframes:
                # æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                if await self._needs_historical_update(pair, timeframe):
                    logger.info(f"ğŸ”„ åŒæ­¥ {pair} {timeframe} æ­·å²æ•¸æ“š...")
                    
                    # ç²å–æ­·å²æ•¸æ“š
                    synced_count = await self._fetch_and_save_historical(pair, timeframe)
                    total_synced += synced_count
                    
                    logger.debug(f"âœ… {pair} {timeframe} åŒæ­¥å®Œæˆ: {synced_count} æ¢")
            
            # è¨˜éŒ„åŒæ­¥å®Œæˆ
            await self._log_sync_complete(sync_id, total_synced, 'success')
            
            # æ›´æ–°ç‹€æ…‹
            if pair in self.sync_status:
                self.sync_status[pair].sync_status = 'synced'
                self.sync_status[pair].records_synced += total_synced
                self.sync_status[pair].last_sync = datetime.now()
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥ {pair} æ­·å²æ•¸æ“šå¤±æ•—: {e}")
            await self._log_sync_complete(sync_id, 0, 'error', str(e))
            
            if pair in self.sync_status:
                self.sync_status[pair].sync_status = 'error'
                self.sync_status[pair].error_message = str(e)
    
    async def _needs_historical_update(self, pair: str, timeframe: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ­·å²æ•¸æ“š"""
        try:
            def check_db():
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    
                    # ç²å–æœ€æ–°æ•¸æ“šæ™‚é–“
                    cursor.execute('''
                        SELECT MAX(timestamp) FROM multi_pair_klines 
                        WHERE pair = ? AND timeframe = ?
                    ''', (pair, timeframe))
                    
                    result = cursor.fetchone()
                    if not result or not result[0]:
                        return True  # æ²’æœ‰æ•¸æ“šï¼Œéœ€è¦æ›´æ–°
                    
                    latest_timestamp = result[0]
                    latest_time = datetime.fromtimestamp(latest_timestamp)
                    
                    # æª¢æŸ¥æ•¸æ“šæ˜¯å¦éæœŸ
                    config = self.stream_configs[pair]
                    update_interval = config.update_intervals.get(timeframe, 3600)
                    
                    time_diff = (datetime.now() - latest_time).total_seconds()
                    return time_diff > update_interval
            
            return await asyncio.get_event_loop().run_in_executor(
                self.executor, check_db
            )
            
        except Exception as e:
            logger.error(f"âŒ æª¢æŸ¥ {pair} {timeframe} æ›´æ–°éœ€æ±‚å¤±æ•—: {e}")
            return True  # éŒ¯èª¤æ™‚å¼·åˆ¶æ›´æ–°
    
    async def _fetch_and_save_historical(self, pair: str, timeframe: str) -> int:
        """ç²å–ä¸¦ä¿å­˜æ­·å²æ•¸æ“š"""
        try:
            # æ™‚é–“æ¡†æ¶æ˜ å°„
            period_map = {'1m': 1, '5m': 5, '1h': 60, '1d': 1440}
            period = period_map.get(timeframe, 5)
            
            # ç²å–æ•¸æ“šé™åˆ¶
            limit_map = {'1m': 1440, '5m': 2016, '1h': 720, '1d': 365}
            limit = limit_map.get(timeframe, 100)
            
            # ç¢ºä¿sessionå·²åˆå§‹åŒ–
            if not self.max_client.session:
                import aiohttp
                self.max_client.session = aiohttp.ClientSession()
            
            # å¾APIç²å–æ•¸æ“š
            klines_df = await self.max_client._get_klines(pair, period, limit)
            
            if klines_df is None or klines_df.empty:
                logger.warning(f"âš ï¸ {pair} {timeframe} æ²’æœ‰ç²å–åˆ°æ•¸æ“š")
                return 0
            
            # ä¿å­˜åˆ°æ•¸æ“šåº«
            def save_to_db():
                with sqlite3.connect(self.db_path) as conn:
                    saved_count = 0
                    
                    for _, row in klines_df.iterrows():
                        try:
                            timestamp = int(row['timestamp'].timestamp()) if isinstance(row['timestamp'], pd.Timestamp) else int(row['timestamp'])
                            
                            conn.execute('''
                                INSERT OR REPLACE INTO multi_pair_klines 
                                (pair, timeframe, timestamp, open, high, low, close, volume)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                            ''', (
                                pair, timeframe, timestamp,
                                float(row['open']), float(row['high']),
                                float(row['low']), float(row['close']),
                                float(row['volume'])
                            ))
                            saved_count += 1
                            
                        except Exception as e:
                            logger.warning(f"âš ï¸ ä¿å­˜å–®æ¢è¨˜éŒ„å¤±æ•—: {e}")
                            continue
                    
                    conn.commit()
                    return saved_count
            
            saved_count = await asyncio.get_event_loop().run_in_executor(
                self.executor, save_to_db
            )
            
            return saved_count
            
        except Exception as e:
            logger.error(f"âŒ ç²å–ä¿å­˜ {pair} {timeframe} æ­·å²æ•¸æ“šå¤±æ•—: {e}")
            return 0
    
    async def _consistency_check_loop(self):
        """æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥å¾ªç’°"""
        logger.info("ğŸ” å•Ÿå‹•æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥å¾ªç’°")
        
        while self.is_running:
            try:
                # æ¯å°æ™‚é€²è¡Œä¸€æ¬¡ä¸€è‡´æ€§æª¢æŸ¥
                await asyncio.sleep(3600)
                
                for pair in self.stream_configs:
                    await self._check_pair_data_consistency(pair)
                
            except Exception as e:
                logger.error(f"âŒ æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥éŒ¯èª¤: {e}")
    
    async def _check_pair_data_consistency(self, pair: str):
        """æª¢æŸ¥å–®å€‹äº¤æ˜“å°çš„æ•¸æ“šä¸€è‡´æ€§"""
        try:
            def check_consistency():
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    
                    issues = []
                    
                    # æª¢æŸ¥æ•¸æ“šç¼ºå£
                    for timeframe in ['1m', '5m', '1h']:
                        cursor.execute('''
                            SELECT COUNT(*) as count,
                                   MIN(timestamp) as earliest,
                                   MAX(timestamp) as latest
                            FROM multi_pair_klines 
                            WHERE pair = ? AND timeframe = ?
                        ''', (pair, timeframe))
                        
                        result = cursor.fetchone()
                        if result and result[0] > 0:
                            count, earliest, latest = result
                            expected_count = self._calculate_expected_records(
                                timeframe, earliest, latest
                            )
                            
                            if count < expected_count * 0.9:  # å…è¨±10%çš„å®¹å·®
                                issues.append(f"{timeframe}: æ•¸æ“šç¼ºå¤± ({count}/{expected_count})")
                    
                    return issues
            
            issues = await asyncio.get_event_loop().run_in_executor(
                self.executor, check_consistency
            )
            
            if issues:
                logger.warning(f"âš ï¸ {pair} æ•¸æ“šä¸€è‡´æ€§å•é¡Œ: {issues}")
                # è§¸ç™¼æ•¸æ“šä¿®å¾©
                await self._repair_data_gaps(pair, issues)
            
        except Exception as e:
            logger.error(f"âŒ æª¢æŸ¥ {pair} æ•¸æ“šä¸€è‡´æ€§å¤±æ•—: {e}")
    
    def _calculate_expected_records(self, timeframe: str, earliest: int, latest: int) -> int:
        """è¨ˆç®—é æœŸè¨˜éŒ„æ•¸"""
        time_diff = latest - earliest
        
        if timeframe == '1m':
            return time_diff // 60
        elif timeframe == '5m':
            return time_diff // 300
        elif timeframe == '1h':
            return time_diff // 3600
        else:
            return 0
    
    async def _repair_data_gaps(self, pair: str, issues: List[str]):
        """ä¿®å¾©æ•¸æ“šç¼ºå£"""
        try:
            logger.info(f"ğŸ”§ ä¿®å¾© {pair} æ•¸æ“šç¼ºå£: {issues}")
            
            # å¼·åˆ¶é‡æ–°åŒæ­¥æœ‰å•é¡Œçš„æ™‚é–“æ¡†æ¶
            for issue in issues:
                timeframe = issue.split(':')[0]
                await self._fetch_and_save_historical(pair, timeframe)
                await asyncio.sleep(1)  # é¿å…APIé™åˆ¶
            
        except Exception as e:
            logger.error(f"âŒ ä¿®å¾© {pair} æ•¸æ“šç¼ºå£å¤±æ•—: {e}")
    
    async def _log_sync_start(self, pair: str, sync_type: str) -> int:
        """è¨˜éŒ„åŒæ­¥é–‹å§‹"""
        try:
            def log_to_db():
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute('''
                        INSERT INTO sync_log 
                        (pair, sync_type, start_time, status)
                        VALUES (?, ?, ?, ?)
                    ''', (pair, sync_type, datetime.now().isoformat(), 'started'))
                    conn.commit()
                    return cursor.lastrowid
            
            return await asyncio.get_event_loop().run_in_executor(
                self.executor, log_to_db
            )
            
        except Exception as e:
            logger.error(f"âŒ è¨˜éŒ„åŒæ­¥é–‹å§‹å¤±æ•—: {e}")
            return 0
    
    async def _log_sync_complete(self, sync_id: int, records_processed: int, 
                               status: str, error_message: str = None):
        """è¨˜éŒ„åŒæ­¥å®Œæˆ"""
        try:
            def log_to_db():
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute('''
                        UPDATE sync_log 
                        SET end_time = ?, records_processed = ?, 
                            status = ?, error_message = ?
                        WHERE id = ?
                    ''', (datetime.now().isoformat(), records_processed, 
                          status, error_message, sync_id))
                    conn.commit()
            
            await asyncio.get_event_loop().run_in_executor(
                self.executor, log_to_db
            )
            
        except Exception as e:
            logger.error(f"âŒ è¨˜éŒ„åŒæ­¥å®Œæˆå¤±æ•—: {e}")
    
    def get_multi_pair_historical_data(self, pairs: List[str] = None, 
                                     timeframe: str = '5m', 
                                     limit: int = 100) -> Dict[str, pd.DataFrame]:
        """ç²å–å¤šå€‹äº¤æ˜“å°çš„æ­·å²æ•¸æ“š"""
        if pairs is None:
            pairs = list(self.stream_configs.keys())
        
        results = {}
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                for pair in pairs:
                    query = '''
                        SELECT timestamp, open, high, low, close, volume
                        FROM multi_pair_klines 
                        WHERE pair = ? AND timeframe = ?
                        ORDER BY timestamp DESC
                        LIMIT ?
                    '''
                    
                    df = pd.read_sql_query(query, conn, params=(pair, timeframe, limit))
                    
                    if not df.empty:
                        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
                        df = df.sort_values('timestamp').reset_index(drop=True)
                        results[pair] = df
            
            logger.info(f"âœ… ç²å– {len(results)} å€‹äº¤æ˜“å°çš„æ­·å²æ•¸æ“š")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ç²å–å¤šäº¤æ˜“å°æ­·å²æ•¸æ“šå¤±æ•—: {e}")
            return {}
    
    def get_real_time_data_summary(self, pairs: List[str] = None) -> Dict[str, Dict[str, Any]]:
        """ç²å–å¯¦æ™‚æ•¸æ“šæ‘˜è¦"""
        if pairs is None:
            pairs = list(self.stream_configs.keys())
        
        results = {}
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                for pair in pairs:
                    cursor = conn.cursor()
                    cursor.execute('''
                        SELECT price, volume, bid, ask, technical_data, timestamp
                        FROM real_time_data 
                        WHERE pair = ?
                        ORDER BY timestamp DESC
                        LIMIT 1
                    ''', (pair,))
                    
                    result = cursor.fetchone()
                    if result:
                        price, volume, bid, ask, technical_data_json, timestamp = result
                        
                        technical_data = json.loads(technical_data_json) if technical_data_json else {}
                        
                        results[pair] = {
                            'price': price,
                            'volume': volume,
                            'bid': bid,
                            'ask': ask,
                            'spread': ask - bid,
                            'timestamp': datetime.fromtimestamp(timestamp),
                            'technical_indicators': technical_data
                        }
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ç²å–å¯¦æ™‚æ•¸æ“šæ‘˜è¦å¤±æ•—: {e}")
            return {}
    
    def get_sync_status_summary(self) -> Dict[str, Any]:
        """ç²å–åŒæ­¥ç‹€æ…‹æ‘˜è¦"""
        try:
            active_pairs = []
            syncing_pairs = []
            error_pairs = []
            
            for pair, status in self.sync_status.items():
                if status.sync_status == 'synced':
                    active_pairs.append(pair)
                elif status.sync_status == 'syncing':
                    syncing_pairs.append(pair)
                elif status.sync_status == 'error':
                    error_pairs.append(pair)
            
            return {
                'total_pairs': len(self.sync_status),
                'active_pairs': active_pairs,
                'syncing_pairs': syncing_pairs,
                'error_pairs': error_pairs,
                'active_count': len(active_pairs),
                'syncing_count': len(syncing_pairs),
                'error_count': len(error_pairs),
                'is_running': self.is_running,
                'database_path': str(self.db_path)
            }
            
        except Exception as e:
            logger.error(f"âŒ ç²å–åŒæ­¥ç‹€æ…‹æ‘˜è¦å¤±æ•—: {e}")
            return {}
    
    async def stop_data_streams(self):
        """åœæ­¢æ‰€æœ‰æ•¸æ“šæµ"""
        logger.info("ğŸ›‘ åœæ­¢å¤šäº¤æ˜“å°æ•¸æ“šæµ...")
        self.is_running = False
        
        # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
        await asyncio.sleep(2)
        
        # é—œé–‰ç·šç¨‹æ± 
        self.executor.shutdown(wait=True)
        
        logger.info("âœ… æ•¸æ“šæµå·²åœæ­¢")
    
    async def close(self):
        """é—œé–‰æ•¸æ“šç®¡ç†ç³»çµ±"""
        try:
            await self.stop_data_streams()
            await self.max_client.close()
            logger.info("âœ… å¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ±å·²é—œé–‰")
        except Exception as e:
            logger.error(f"âŒ é—œé–‰æ•¸æ“šç®¡ç†ç³»çµ±å¤±æ•—: {e}")


# å‰µå»ºå…¨å±€å¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†å™¨å¯¦ä¾‹
def create_multi_pair_data_manager() -> MultiPairDataManager:
    """å‰µå»ºå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†å™¨å¯¦ä¾‹"""
    return MultiPairDataManager()


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    async def test_multi_pair_data_manager():
        """æ¸¬è©¦å¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†å™¨"""
        print("ğŸ§ª æ¸¬è©¦å¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†å™¨...")
        
        manager = create_multi_pair_data_manager()
        
        try:
            # ç²å–åŒæ­¥ç‹€æ…‹
            status = manager.get_sync_status_summary()
            print(f"ğŸ“Š åŒæ­¥ç‹€æ…‹: {status}")
            
            # æ¸¬è©¦æ­·å²æ•¸æ“šç²å–
            print("ğŸ“š æ¸¬è©¦æ­·å²æ•¸æ“šç²å–...")
            historical_data = manager.get_multi_pair_historical_data(
                pairs=['BTCTWD', 'ETHTWD'], 
                timeframe='5m', 
                limit=10
            )
            
            for pair, df in historical_data.items():
                print(f"   {pair}: {len(df)} æ¢æ­·å²è¨˜éŒ„")
            
            # æ¸¬è©¦å¯¦æ™‚æ•¸æ“šæ‘˜è¦
            print("ğŸ“¡ æ¸¬è©¦å¯¦æ™‚æ•¸æ“šæ‘˜è¦...")
            real_time_summary = manager.get_real_time_data_summary()
            
            for pair, data in real_time_summary.items():
                print(f"   {pair}: {data['price']:.0f} TWD")
            
            print("âœ… å¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†å™¨æ¸¬è©¦å®Œæˆ")
            
        finally:
            await manager.close()
    
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(test_multi_pair_data_manager())