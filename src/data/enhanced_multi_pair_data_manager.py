#!/usr/bin/env python3
"""
å¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ±
å¯¦ç¾ä»»å‹™1.2: å»ºç«‹å¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ±
"""

import asyncio
import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import logging
import json
from dataclasses import dataclass, asdict
import threading
from concurrent.futures import ThreadPoolExecutor

try:
    from .multi_pair_max_client import MultiPairMAXClient, create_multi_pair_client
    from .trading_pair_manager import TradingPairManager, create_trading_pair_manager
    from .historical_data_manager import HistoricalDataManager
except ImportError:
    # ç”¨æ–¼ç›´æ¥é‹è¡Œæ¸¬è©¦
    from multi_pair_max_client import MultiPairMAXClient, create_multi_pair_client
    from trading_pair_manager import TradingPairManager, create_trading_pair_manager
    from historical_data_manager import HistoricalDataManager

logger = logging.getLogger(__name__)

@dataclass
class DataStreamConfig:
    """æ•¸æ“šæµé…ç½®"""
    pair: str
    timeframes: List[str]
    update_intervals: Dict[str, int]  # ç§’
    buffer_size: int = 1000
    enabled: bool = True

@dataclass
class DataSyncStatus:
    """æ•¸æ“šåŒæ­¥ç‹€æ…‹"""
    pair: str
    last_sync: datetime
    sync_status: str  # 'synced', 'syncing', 'error'
    records_synced: int
    error_message: Optional[str] = None

class DataSyncCoordinator:
    """æ•¸æ“šåŒæ­¥å”èª¿å™¨ - ç®¡ç†å¤šäº¤æ˜“å°é–“çš„æ•¸æ“šåŒæ­¥å’Œä¸€è‡´æ€§"""
    
    def __init__(self):
        self.sync_queue = asyncio.Queue()
        self.sync_locks = {}
        self.cross_pair_correlations = {}
        self.sync_priorities = {}
        
        logger.info("ğŸ”„ æ•¸æ“šåŒæ­¥å”èª¿å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def coordinate_sync(self, pairs: List[str], sync_type: str = 'incremental'):
        """å”èª¿å¤šäº¤æ˜“å°åŒæ­¥"""
        try:
            # æ ¹æ“šå„ªå…ˆç´šæ’åº
            sorted_pairs = self._sort_pairs_by_priority(pairs)
            
            # ä¸¦è¡ŒåŒæ­¥ï¼Œä½†æ§åˆ¶ä¸¦ç™¼æ•¸
            semaphore = asyncio.Semaphore(3)  # æœ€å¤š3å€‹ä¸¦ç™¼åŒæ­¥
            
            tasks = []
            for pair in sorted_pairs:
                task = self._sync_with_semaphore(semaphore, pair, sync_type)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # æª¢æŸ¥åŒæ­¥çµæœ
            success_count = sum(1 for r in results if not isinstance(r, Exception))
            logger.info(f"âœ… å”èª¿åŒæ­¥å®Œæˆ: {success_count}/{len(pairs)} æˆåŠŸ")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ å”èª¿åŒæ­¥å¤±æ•—: {e}")
            return []
    
    async def _sync_with_semaphore(self, semaphore: asyncio.Semaphore, 
                                 pair: str, sync_type: str):
        """ä½¿ç”¨ä¿¡è™Ÿé‡æ§åˆ¶çš„åŒæ­¥"""
        async with semaphore:
            return await self._sync_single_pair(pair, sync_type)
    
    async def _sync_single_pair(self, pair: str, sync_type: str):
        """åŒæ­¥å–®å€‹äº¤æ˜“å°"""
        try:
            # ç²å–åŒæ­¥é–
            if pair not in self.sync_locks:
                self.sync_locks[pair] = asyncio.Lock()
            
            async with self.sync_locks[pair]:
                logger.debug(f"ğŸ”„ é–‹å§‹åŒæ­¥ {pair} ({sync_type})")
                
                # æ¨¡æ“¬åŒæ­¥éç¨‹
                await asyncio.sleep(1)
                
                logger.debug(f"âœ… å®ŒæˆåŒæ­¥ {pair}")
                return {'pair': pair, 'status': 'success', 'type': sync_type}
                
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥ {pair} å¤±æ•—: {e}")
            return {'pair': pair, 'status': 'error', 'error': str(e)}
    
    def _sort_pairs_by_priority(self, pairs: List[str]) -> List[str]:
        """æ ¹æ“šå„ªå…ˆç´šæ’åºäº¤æ˜“å°"""
        # é»˜èªå„ªå…ˆç´šï¼šä¸»æµå¹£ç¨®å„ªå…ˆ
        priority_map = {
            'BTCTWD': 1,
            'ETHTWD': 2,
            'LTCTWD': 3,
            'BCHTWD': 4,
            'ADATWD': 5,
            'DOTTWD': 6
        }
        
        return sorted(pairs, key=lambda p: priority_map.get(p, 999))
    
    def update_cross_pair_correlations(self, correlations: Dict[str, Dict[str, float]]):
        """æ›´æ–°äº¤æ˜“å°é–“ç›¸é—œæ€§"""
        self.cross_pair_correlations = correlations
        logger.debug(f"ğŸ“Š æ›´æ–°äº¤æ˜“å°ç›¸é—œæ€§: {len(correlations)} å°")
    
    def get_sync_recommendations(self, pair: str) -> Dict[str, Any]:
        """ç²å–åŒæ­¥å»ºè­°"""
        try:
            recommendations = {
                'priority': self.sync_priorities.get(pair, 'normal'),
                'correlated_pairs': [],
                'sync_frequency': 'normal'
            }
            
            # æŸ¥æ‰¾é«˜ç›¸é—œæ€§çš„äº¤æ˜“å°
            if pair in self.cross_pair_correlations:
                for other_pair, correlation in self.cross_pair_correlations[pair].items():
                    if abs(correlation) > 0.7:  # é«˜ç›¸é—œæ€§é–¾å€¼
                        recommendations['correlated_pairs'].append({
                            'pair': other_pair,
                            'correlation': correlation
                        })
            
            return recommendations
            
        except Exception as e:
            logger.error(f"âŒ ç²å– {pair} åŒæ­¥å»ºè­°å¤±æ•—: {e}")
            return {}


class EnhancedMultiPairDataManager:
    """å¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ±"""
    
    def __init__(self, db_path: str = "data/multi_pair_market.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ ¸å¿ƒçµ„ä»¶
        self.max_client = create_multi_pair_client()
        self.pair_manager = create_trading_pair_manager()
        
        # ç‚ºæ¯å€‹äº¤æ˜“å°å‰µå»ºç¨ç«‹çš„æ­·å²æ•¸æ“šç®¡ç†å™¨
        self.pair_data_managers: Dict[str, HistoricalDataManager] = {}
        
        # æ•¸æ“šæµé…ç½®
        self.stream_configs: Dict[str, DataStreamConfig] = {}
        self.sync_status: Dict[str, DataSyncStatus] = {}
        
        # æ•¸æ“šç·©å­˜å’ŒåŒæ­¥ - å¢å¼·ç‰ˆ
        self.data_buffers: Dict[str, Dict[str, List]] = {}
        self.sync_locks: Dict[str, threading.Lock] = {}
        self.data_consistency_cache: Dict[str, Dict[str, Any]] = {}
        
        # ä¸¦ç™¼æ§åˆ¶ - å„ªåŒ–
        self.executor = ThreadPoolExecutor(max_workers=8)  # å¢åŠ ç·šç¨‹æ•¸
        self.sync_interval = 300  # 5åˆ†é˜åŒæ­¥ä¸€æ¬¡
        self.is_running = False
        
        # æ•¸æ“šåŒæ­¥å”èª¿å™¨
        self.sync_coordinator = DataSyncCoordinator()
        
        # åˆå§‹åŒ–
        self._init_database()
        self._initialize_stream_configs()
        self._initialize_pair_data_managers()
        
        logger.info("ğŸš€ å¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    def _init_database(self):
        """åˆå§‹åŒ–å¤šäº¤æ˜“å°æ•¸æ“šåº«"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # å‰µå»ºå¤šäº¤æ˜“å°Kç·šæ•¸æ“šè¡¨
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS multi_pair_klines (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        pair TEXT NOT NULL,
                        timeframe TEXT NOT NULL,
                        timestamp INTEGER NOT NULL,
                        open REAL NOT NULL,
                        high REAL NOT NULL,
                        low REAL NOT NULL,
                        close REAL NOT NULL,
                        volume REAL NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(pair, timeframe, timestamp)
                    )
                ''')
                
                # å‰µå»ºå¯¦æ™‚æ•¸æ“šè¡¨
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS real_time_data (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        pair TEXT NOT NULL,
                        timestamp INTEGER NOT NULL,
                        price REAL NOT NULL,
                        volume REAL NOT NULL,
                        bid REAL,
                        ask REAL,
                        technical_data TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(pair, timestamp)
                    )
                ''')
                
                # å‰µå»ºæ•¸æ“šåŒæ­¥æ—¥èªŒè¡¨
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS sync_log (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        pair TEXT NOT NULL,
                        sync_type TEXT NOT NULL,
                        start_time TIMESTAMP NOT NULL,
                        end_time TIMESTAMP,
                        records_processed INTEGER DEFAULT 0,
                        status TEXT NOT NULL,
                        error_message TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
                
                # å‰µå»ºç´¢å¼•
                cursor.execute('''
                    CREATE INDEX IF NOT EXISTS idx_multi_klines_pair_timeframe_timestamp 
                    ON multi_pair_klines(pair, timeframe, timestamp)
                ''')
                
                cursor.execute('''
                    CREATE INDEX IF NOT EXISTS idx_realtime_pair_timestamp 
                    ON real_time_data(pair, timestamp)
                ''')
                
                conn.commit()
                logger.info("âœ… å¤šäº¤æ˜“å°æ•¸æ“šåº«åˆå§‹åŒ–å®Œæˆ")
                
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šåº«åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def _initialize_stream_configs(self):
        """åˆå§‹åŒ–æ•¸æ“šæµé…ç½®"""
        try:
            # ç²å–æ‰€æœ‰æ´»èºäº¤æ˜“å°
            active_pairs = self.max_client.get_active_pairs()
            
            for pair in active_pairs:
                # ç‚ºæ¯å€‹äº¤æ˜“å°å‰µå»ºæ•¸æ“šæµé…ç½®
                config = DataStreamConfig(
                    pair=pair,
                    timeframes=['1m', '5m', '1h'],
                    update_intervals={'1m': 60, '5m': 300, '1h': 3600},
                    buffer_size=1000,
                    enabled=True
                )
                
                self.stream_configs[pair] = config
                self.sync_status[pair] = DataSyncStatus(
                    pair=pair,
                    last_sync=datetime.now(),
                    sync_status='initialized',
                    records_synced=0
                )
                
                # åˆå§‹åŒ–æ•¸æ“šç·©å­˜å’Œé–
                self.data_buffers[pair] = {tf: [] for tf in config.timeframes}
                self.sync_locks[pair] = threading.Lock()
            
            logger.info(f"ğŸ“Š åˆå§‹åŒ– {len(active_pairs)} å€‹äº¤æ˜“å°çš„æ•¸æ“šæµé…ç½®")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–æ•¸æ“šæµé…ç½®å¤±æ•—: {e}")
    
    def _initialize_pair_data_managers(self):
        """ç‚ºæ¯å€‹äº¤æ˜“å°å‰µå»ºç¨ç«‹çš„æ­·å²æ•¸æ“šç®¡ç†å™¨"""
        try:
            for pair in self.stream_configs.keys():
                # ç‚ºæ¯å€‹äº¤æ˜“å°å‰µå»ºå°ˆç”¨çš„æ•¸æ“šåº«è·¯å¾‘
                pair_db_path = f"data/{pair.lower()}_history.db"
                
                # å‰µå»ºç¨ç«‹çš„æ­·å²æ•¸æ“šç®¡ç†å™¨
                try:
                    self.pair_data_managers[pair] = HistoricalDataManager(
                        db_path=pair_db_path
                    )
                except Exception as e:
                    # å¦‚æœHistoricalDataManagerä¸æ”¯æŒpairåƒæ•¸ï¼Œä½¿ç”¨åŸºæœ¬åˆå§‹åŒ–
                    logger.warning(f"âš ï¸ ä½¿ç”¨åŸºæœ¬åˆå§‹åŒ–ç‚º {pair} å‰µå»ºæ•¸æ“šç®¡ç†å™¨: {e}")
                    self.pair_data_managers[pair] = HistoricalDataManager(
                        db_path=pair_db_path
                    )
                
                logger.debug(f"âœ… ç‚º {pair} å‰µå»ºç¨ç«‹æ•¸æ“šç®¡ç†å™¨")
            
            logger.info(f"ğŸ“Š ç‚º {len(self.pair_data_managers)} å€‹äº¤æ˜“å°å‰µå»ºç¨ç«‹æ•¸æ“šç®¡ç†å™¨")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–äº¤æ˜“å°æ•¸æ“šç®¡ç†å™¨å¤±æ•—: {e}")
    
    def get_multi_pair_historical_data(self, pairs: List[str] = None, 
                                     timeframe: str = '5m', 
                                     limit: int = 100) -> Dict[str, pd.DataFrame]:
        """ç²å–å¤šå€‹äº¤æ˜“å°çš„æ­·å²æ•¸æ“š"""
        if pairs is None:
            pairs = list(self.stream_configs.keys())
        
        results = {}
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                for pair in pairs:
                    query = '''
                        SELECT timestamp, open, high, low, close, volume
                        FROM multi_pair_klines 
                        WHERE pair = ? AND timeframe = ?
                        ORDER BY timestamp DESC
                        LIMIT ?
                    '''
                    
                    df = pd.read_sql_query(query, conn, params=(pair, timeframe, limit))
                    
                    if not df.empty:
                        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
                        df = df.sort_values('timestamp').reset_index(drop=True)
                        results[pair] = df
            
            logger.info(f"âœ… ç²å– {len(results)} å€‹äº¤æ˜“å°çš„æ­·å²æ•¸æ“š")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ç²å–å¤šäº¤æ˜“å°æ­·å²æ•¸æ“šå¤±æ•—: {e}")
            return {}
    
    def get_real_time_data_summary(self, pairs: List[str] = None) -> Dict[str, Dict[str, Any]]:
        """ç²å–å¯¦æ™‚æ•¸æ“šæ‘˜è¦"""
        if pairs is None:
            pairs = list(self.stream_configs.keys())
        
        results = {}
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                for pair in pairs:
                    cursor = conn.cursor()
                    cursor.execute('''
                        SELECT price, volume, bid, ask, technical_data, timestamp
                        FROM real_time_data 
                        WHERE pair = ?
                        ORDER BY timestamp DESC
                        LIMIT 1
                    ''', (pair,))
                    
                    result = cursor.fetchone()
                    if result:
                        price, volume, bid, ask, technical_data_json, timestamp = result
                        
                        technical_data = json.loads(technical_data_json) if technical_data_json else {}
                        
                        results[pair] = {
                            'price': price,
                            'volume': volume,
                            'bid': bid,
                            'ask': ask,
                            'spread': ask - bid if ask and bid else 0,
                            'timestamp': datetime.fromtimestamp(timestamp),
                            'technical_indicators': technical_data
                        }
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ç²å–å¯¦æ™‚æ•¸æ“šæ‘˜è¦å¤±æ•—: {e}")
            return {}
    
    def get_sync_status_summary(self) -> Dict[str, Any]:
        """ç²å–åŒæ­¥ç‹€æ…‹æ‘˜è¦"""
        try:
            active_pairs = []
            syncing_pairs = []
            error_pairs = []
            
            for pair, status in self.sync_status.items():
                if status.sync_status == 'synced':
                    active_pairs.append(pair)
                elif status.sync_status == 'syncing':
                    syncing_pairs.append(pair)
                elif status.sync_status == 'error':
                    error_pairs.append(pair)
            
            return {
                'total_pairs': len(self.sync_status),
                'active_pairs': active_pairs,
                'syncing_pairs': syncing_pairs,
                'error_pairs': error_pairs,
                'active_count': len(active_pairs),
                'syncing_count': len(syncing_pairs),
                'error_count': len(error_pairs),
                'is_running': self.is_running,
                'database_path': str(self.db_path)
            }
            
        except Exception as e:
            logger.error(f"âŒ ç²å–åŒæ­¥ç‹€æ…‹æ‘˜è¦å¤±æ•—: {e}")
            return {}
    
    async def start_parallel_data_streams(self):
        """å•Ÿå‹•ä¸¦è¡Œæ•¸æ“šæµè™•ç†"""
        if self.is_running:
            logger.warning("âš ï¸ æ•¸æ“šæµå·²åœ¨é‹è¡Œä¸­")
            return
        
        self.is_running = True
        logger.info("ğŸš€ å•Ÿå‹•å¤šäº¤æ˜“å°ä¸¦è¡Œæ•¸æ“šæµ...")
        
        try:
            # ä½¿ç”¨åŒæ­¥å”èª¿å™¨å”èª¿å¤šäº¤æ˜“å°åŒæ­¥
            pairs = list(self.stream_configs.keys())
            sync_results = await self.sync_coordinator.coordinate_sync(pairs, 'parallel_start')
            
            logger.info(f"âœ… ä¸¦è¡Œæ•¸æ“šæµå•Ÿå‹•å®Œæˆ: {len(sync_results)} å€‹äº¤æ˜“å°")
            
        except Exception as e:
            logger.error(f"âŒ ä¸¦è¡Œæ•¸æ“šæµå•Ÿå‹•å¤±æ•—: {e}")
        finally:
            self.is_running = False
    
    def check_data_consistency(self, pairs: List[str] = None) -> Dict[str, Any]:
        """æª¢æŸ¥äº¤æ˜“å°é–“æ•¸æ“šåŒæ­¥å’Œä¸€è‡´æ€§"""
        if pairs is None:
            pairs = list(self.stream_configs.keys())
        
        consistency_report = {
            'check_time': datetime.now().isoformat(),
            'pairs_checked': pairs,
            'consistency_issues': [],
            'recommendations': []
        }
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                for pair in pairs:
                    # æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§
                    cursor.execute('''
                        SELECT COUNT(*) as count,
                               MIN(timestamp) as earliest,
                               MAX(timestamp) as latest
                        FROM multi_pair_klines 
                        WHERE pair = ? AND timeframe = '5m'
                    ''', (pair,))
                    
                    result = cursor.fetchone()
                    if result and result[0] > 0:
                        count, earliest, latest = result
                        
                        # è¨ˆç®—é æœŸè¨˜éŒ„æ•¸
                        if earliest and latest:
                            time_diff = latest - earliest
                            expected_count = time_diff // 300  # 5åˆ†é˜é–“éš”
                            
                            if count < expected_count * 0.8:  # å…è¨±20%çš„å®¹å·®
                                consistency_report['consistency_issues'].append({
                                    'pair': pair,
                                    'issue': 'data_gaps',
                                    'actual_count': count,
                                    'expected_count': expected_count,
                                    'completeness': count / expected_count if expected_count > 0 else 0
                                })
                    else:
                        consistency_report['consistency_issues'].append({
                            'pair': pair,
                            'issue': 'no_data',
                            'actual_count': 0,
                            'expected_count': 0,
                            'completeness': 0
                        })
            
            # ç”Ÿæˆå»ºè­°
            if consistency_report['consistency_issues']:
                consistency_report['recommendations'].append(
                    "å»ºè­°é‡æ–°åŒæ­¥æœ‰æ•¸æ“šç¼ºå£çš„äº¤æ˜“å°"
                )
            else:
                consistency_report['recommendations'].append(
                    "æ•¸æ“šä¸€è‡´æ€§è‰¯å¥½ï¼Œç„¡éœ€ç‰¹æ®Šè™•ç†"
                )
            
            logger.info(f"âœ… æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥å®Œæˆ: {len(consistency_report['consistency_issues'])} å€‹å•é¡Œ")
            return consistency_report
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥å¤±æ•—: {e}")
            consistency_report['error'] = str(e)
            return consistency_report
    
    def optimize_storage_structure(self) -> Dict[str, Any]:
        """å„ªåŒ–æ•¸æ“šå­˜å„²çµæ§‹ä»¥æ”¯æŒå¤šäº¤æ˜“å°æŸ¥è©¢"""
        optimization_report = {
            'optimization_time': datetime.now().isoformat(),
            'optimizations_applied': [],
            'performance_improvements': []
        }
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # æª¢æŸ¥ä¸¦å‰µå»ºç¼ºå¤±çš„ç´¢å¼•
                indexes_to_create = [
                    ('idx_multi_klines_pair_timestamp', 'multi_pair_klines', 'pair, timestamp'),
                    ('idx_multi_klines_timeframe', 'multi_pair_klines', 'timeframe'),
                    ('idx_realtime_pair', 'real_time_data', 'pair'),
                    ('idx_sync_log_pair_status', 'sync_log', 'pair, status')
                ]
                
                for index_name, table_name, columns in indexes_to_create:
                    try:
                        cursor.execute(f'''
                            CREATE INDEX IF NOT EXISTS {index_name} 
                            ON {table_name}({columns})
                        ''')
                        optimization_report['optimizations_applied'].append(
                            f"å‰µå»ºç´¢å¼•: {index_name}"
                        )
                    except Exception as e:
                        logger.warning(f"âš ï¸ å‰µå»ºç´¢å¼• {index_name} å¤±æ•—: {e}")
                
                # åˆ†æè¡¨çµ±è¨ˆä¿¡æ¯
                cursor.execute('ANALYZE')
                optimization_report['optimizations_applied'].append("æ›´æ–°è¡¨çµ±è¨ˆä¿¡æ¯")
                
                # æª¢æŸ¥å­˜å„²ä½¿ç”¨æƒ…æ³
                cursor.execute('''
                    SELECT 
                        COUNT(*) as total_records,
                        COUNT(DISTINCT pair) as unique_pairs,
                        MIN(timestamp) as earliest_data,
                        MAX(timestamp) as latest_data
                    FROM multi_pair_klines
                ''')
                
                result = cursor.fetchone()
                if result:
                    total_records, unique_pairs, earliest_data, latest_data = result
                    optimization_report['storage_stats'] = {
                        'total_records': total_records,
                        'unique_pairs': unique_pairs,
                        'earliest_data': datetime.fromtimestamp(earliest_data).isoformat() if earliest_data else None,
                        'latest_data': datetime.fromtimestamp(latest_data).isoformat() if latest_data else None
                    }
                
                conn.commit()
                
                optimization_report['performance_improvements'].append(
                    "æŸ¥è©¢æ€§èƒ½å·²å„ªåŒ–ï¼Œæ”¯æŒé«˜æ•ˆçš„å¤šäº¤æ˜“å°æŸ¥è©¢"
                )
                
                logger.info("âœ… å­˜å„²çµæ§‹å„ªåŒ–å®Œæˆ")
                return optimization_report
                
        except Exception as e:
            logger.error(f"âŒ å­˜å„²çµæ§‹å„ªåŒ–å¤±æ•—: {e}")
            optimization_report['error'] = str(e)
            return optimization_report
    
    async def close(self):
        """é—œé–‰æ•¸æ“šç®¡ç†ç³»çµ±"""
        try:
            self.is_running = False
            
            # é—œé–‰ç·šç¨‹æ± 
            self.executor.shutdown(wait=True)
            
            # é—œé–‰MAXå®¢æˆ¶ç«¯
            await self.max_client.close()
            
            logger.info("âœ… å¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†ç³»çµ±å·²é—œé–‰")
        except Exception as e:
            logger.error(f"âŒ é—œé–‰æ•¸æ“šç®¡ç†ç³»çµ±å¤±æ•—: {e}")


# å‰µå»ºå…¨å±€å¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†å™¨å¯¦ä¾‹
def create_enhanced_multi_pair_data_manager() -> EnhancedMultiPairDataManager:
    """å‰µå»ºå¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†å™¨å¯¦ä¾‹"""
    return EnhancedMultiPairDataManager()


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    async def test_enhanced_multi_pair_data_manager():
        """æ¸¬è©¦å¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†å™¨"""
        print("ğŸ§ª æ¸¬è©¦å¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†å™¨...")
        
        manager = create_enhanced_multi_pair_data_manager()
        
        try:
            # æ¸¬è©¦åŸºæœ¬åŠŸèƒ½
            print("ğŸ“Š æ¸¬è©¦åŒæ­¥ç‹€æ…‹...")
            status = manager.get_sync_status_summary()
            print(f"   åŒæ­¥ç‹€æ…‹: {status}")
            
            # æ¸¬è©¦æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥
            print("ğŸ” æ¸¬è©¦æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥...")
            consistency = manager.check_data_consistency()
            print(f"   ä¸€è‡´æ€§å ±å‘Š: {len(consistency.get('consistency_issues', []))} å€‹å•é¡Œ")
            
            # æ¸¬è©¦å­˜å„²å„ªåŒ–
            print("ğŸ—„ï¸ æ¸¬è©¦å­˜å„²å„ªåŒ–...")
            optimization = manager.optimize_storage_structure()
            print(f"   å„ªåŒ–å®Œæˆ: {len(optimization.get('optimizations_applied', []))} é …å„ªåŒ–")
            
            # æ¸¬è©¦ä¸¦è¡Œæ•¸æ“šæµ
            print("ğŸ“¡ æ¸¬è©¦ä¸¦è¡Œæ•¸æ“šæµ...")
            await manager.start_parallel_data_streams()
            
            print("âœ… å¢å¼·ç‰ˆå¤šäº¤æ˜“å°æ•¸æ“šç®¡ç†å™¨æ¸¬è©¦å®Œæˆ")
            
        finally:
            await manager.close()
    
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(test_enhanced_multi_pair_data_manager())