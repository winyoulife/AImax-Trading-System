#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ­·å²æ•¸æ“šç®¡ç†å™¨ - æœ¬åœ°æ•¸æ“šåº«ç·©å­˜ç³»çµ±
é¿å…æ¯æ¬¡éƒ½é‡æ–°ç²å–æ­·å²æ•¸æ“šï¼Œæå‡ç³»çµ±æ•ˆç‡
"""

import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import logging
import asyncio
from pathlib import Path
import json

from .max_client import create_max_client

logger = logging.getLogger(__name__)

class HistoricalDataManager:
    """æ­·å²æ•¸æ“šç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "data/market_history.db"):
        """
        åˆå§‹åŒ–æ­·å²æ•¸æ“šç®¡ç†å™¨
        
        Args:
            db_path: æ•¸æ“šåº«æ–‡ä»¶è·¯å¾‘
        """
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        self.max_client = create_max_client()
        
        # æ•¸æ“šæ›´æ–°é…ç½®
        self.update_config = {
            '1m': {'limit': 1440, 'update_interval': 60},      # 1å¤©æ•¸æ“šï¼Œæ¯åˆ†é˜æ›´æ–°
            '5m': {'limit': 2016, 'update_interval': 300},     # 7å¤©æ•¸æ“šï¼Œæ¯5åˆ†é˜æ›´æ–°
            '1h': {'limit': 720, 'update_interval': 3600},     # 30å¤©æ•¸æ“šï¼Œæ¯å°æ™‚æ›´æ–°
            '1d': {'limit': 365, 'update_interval': 86400}     # 1å¹´æ•¸æ“šï¼Œæ¯å¤©æ›´æ–°
        }
        
        # åˆå§‹åŒ–æ•¸æ“šåº«
        self._init_database()
        
        logger.info(f"ğŸ“Š æ­·å²æ•¸æ“šç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ•¸æ“šåº«: {self.db_path}")
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•¸æ“šåº«è¡¨çµæ§‹"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # å‰µå»ºKç·šæ•¸æ“šè¡¨
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS klines (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        market TEXT NOT NULL,
                        timeframe TEXT NOT NULL,
                        timestamp INTEGER NOT NULL,
                        open REAL NOT NULL,
                        high REAL NOT NULL,
                        low REAL NOT NULL,
                        close REAL NOT NULL,
                        volume REAL NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(market, timeframe, timestamp)
                    )
                ''')
                
                # å‰µå»ºç´¢å¼•
                cursor.execute('''
                    CREATE INDEX IF NOT EXISTS idx_klines_market_timeframe_timestamp 
                    ON klines(market, timeframe, timestamp)
                ''')
                
                # å‰µå»ºæ•¸æ“šæ›´æ–°è¨˜éŒ„è¡¨
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS update_log (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        market TEXT NOT NULL,
                        timeframe TEXT NOT NULL,
                        last_update TIMESTAMP NOT NULL,
                        records_count INTEGER NOT NULL,
                        status TEXT NOT NULL,
                        UNIQUE(market, timeframe)
                    )
                ''')
                
                # å‰µå»ºç³»çµ±é…ç½®è¡¨
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS system_config (
                        key TEXT PRIMARY KEY,
                        value TEXT NOT NULL,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
                
                conn.commit()
                logger.info("âœ… æ•¸æ“šåº«è¡¨çµæ§‹åˆå§‹åŒ–å®Œæˆ")
                
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šåº«åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    async def ensure_historical_data(self, market: str = "btctwd", 
                                   timeframes: List[str] = None) -> Dict[str, bool]:
        """
        ç¢ºä¿æ­·å²æ•¸æ“šå®Œæ•´æ€§ï¼Œå¦‚æœç¼ºå¤±å‰‡è‡ªå‹•ç²å–
        
        Args:
            market: äº¤æ˜“å°
            timeframes: æ™‚é–“æ¡†æ¶åˆ—è¡¨ï¼Œé»˜èªç‚º ['1m', '5m', '1h']
            
        Returns:
            å„æ™‚é–“æ¡†æ¶çš„æ•¸æ“šç‹€æ…‹
        """
        if timeframes is None:
            timeframes = ['1m', '5m', '1h']
        
        results = {}
        
        try:
            logger.info(f"ğŸ” æª¢æŸ¥æ­·å²æ•¸æ“šå®Œæ•´æ€§: {market}")
            
            for timeframe in timeframes:
                logger.info(f"ğŸ“Š æª¢æŸ¥ {timeframe} æ•¸æ“š...")
                
                # æª¢æŸ¥æ•¸æ“šæ˜¯å¦éœ€è¦æ›´æ–°
                needs_update, reason = self._check_data_freshness(market, timeframe)
                
                if needs_update:
                    logger.info(f"ğŸ”„ {reason}ï¼Œé–‹å§‹æ›´æ–° {timeframe} æ•¸æ“š...")
                    success = await self._update_timeframe_data(market, timeframe)
                    results[timeframe] = success
                else:
                    logger.info(f"âœ… {timeframe} æ•¸æ“šå·²æ˜¯æœ€æ–°")
                    results[timeframe] = True
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ç¢ºä¿æ­·å²æ•¸æ“šå¤±æ•—: {e}")
            return {tf: False for tf in timeframes}
    
    def _check_data_freshness(self, market: str, timeframe: str) -> Tuple[bool, str]:
        """æª¢æŸ¥æ•¸æ“šæ–°é®®åº¦"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # æª¢æŸ¥æ›´æ–°è¨˜éŒ„
                cursor.execute('''
                    SELECT last_update, records_count FROM update_log 
                    WHERE market = ? AND timeframe = ?
                ''', (market, timeframe))
                
                result = cursor.fetchone()
                
                if not result:
                    return True, "é¦–æ¬¡ç²å–æ•¸æ“š"
                
                last_update, records_count = result
                last_update_time = datetime.fromisoformat(last_update)
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                update_interval = self.update_config[timeframe]['update_interval']
                time_since_update = (datetime.now() - last_update_time).total_seconds()
                
                if time_since_update > update_interval:
                    return True, f"æ•¸æ“šéæœŸ ({time_since_update/60:.1f}åˆ†é˜å‰)"
                
                # æª¢æŸ¥æ•¸æ“šé‡æ˜¯å¦è¶³å¤ 
                expected_records = self.update_config[timeframe]['limit']
                if records_count < expected_records * 0.8:  # å…è¨±20%çš„å®¹å·®
                    return True, f"æ•¸æ“šé‡ä¸è¶³ ({records_count}/{expected_records})"
                
                return False, "æ•¸æ“šå·²æ˜¯æœ€æ–°"
                
        except Exception as e:
            logger.error(f"âŒ æª¢æŸ¥æ•¸æ“šæ–°é®®åº¦å¤±æ•—: {e}")
            return True, "æª¢æŸ¥å¤±æ•—ï¼Œå¼·åˆ¶æ›´æ–°"
    
    async def _update_timeframe_data(self, market: str, timeframe: str) -> bool:
        """æ›´æ–°ç‰¹å®šæ™‚é–“æ¡†æ¶çš„æ•¸æ“š"""
        try:
            # ç²å–é…ç½®
            config = self.update_config[timeframe]
            limit = config['limit']
            
            # è½‰æ›æ™‚é–“æ¡†æ¶æ ¼å¼
            period_map = {'1m': 1, '5m': 5, '1h': 60, '1d': 1440}
            period = period_map.get(timeframe, 1)
            
            logger.info(f"ğŸ“¥ å¾MAX APIç²å– {timeframe} æ•¸æ“š (limit: {limit})...")
            
            # ç²å–æ•¸æ“š
            klines_df = await self.max_client._get_recent_klines(market, period, limit)
            
            if klines_df is None or klines_df.empty:
                logger.error(f"âŒ ç²å– {timeframe} æ•¸æ“šå¤±æ•—")
                return False
            
            # ä¿å­˜åˆ°æ•¸æ“šåº«
            saved_count = self._save_klines_data(market, timeframe, klines_df)
            
            # æ›´æ–°è¨˜éŒ„
            self._update_log_record(market, timeframe, saved_count, "success")
            
            logger.info(f"âœ… {timeframe} æ•¸æ“šæ›´æ–°å®Œæˆï¼Œä¿å­˜ {saved_count} æ¢è¨˜éŒ„")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–° {timeframe} æ•¸æ“šå¤±æ•—: {e}")
            self._update_log_record(market, timeframe, 0, "failed")
            return False
    
    def _save_klines_data(self, market: str, timeframe: str, df: pd.DataFrame) -> int:
        """ä¿å­˜Kç·šæ•¸æ“šåˆ°æ•¸æ“šåº«"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                saved_count = 0
                
                for _, row in df.iterrows():
                    try:
                        # è½‰æ›æ™‚é–“æˆ³
                        if isinstance(row['timestamp'], pd.Timestamp):
                            timestamp = int(row['timestamp'].timestamp())
                        else:
                            timestamp = int(row['timestamp'])
                        
                        # æ’å…¥æ•¸æ“šï¼ˆä½¿ç”¨ INSERT OR REPLACE é¿å…é‡è¤‡ï¼‰
                        conn.execute('''
                            INSERT OR REPLACE INTO klines 
                            (market, timeframe, timestamp, open, high, low, close, volume)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                        ''', (
                            market, timeframe, timestamp,
                            float(row['open']), float(row['high']), 
                            float(row['low']), float(row['close']), 
                            float(row['volume'])
                        ))
                        saved_count += 1
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ ä¿å­˜å–®æ¢è¨˜éŒ„å¤±æ•—: {e}")
                        continue
                
                conn.commit()
                return saved_count
                
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜Kç·šæ•¸æ“šå¤±æ•—: {e}")
            return 0
    
    def _update_log_record(self, market: str, timeframe: str, count: int, status: str):
        """æ›´æ–°æ—¥èªŒè¨˜éŒ„"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('''
                    INSERT OR REPLACE INTO update_log 
                    (market, timeframe, last_update, records_count, status)
                    VALUES (?, ?, ?, ?, ?)
                ''', (market, timeframe, datetime.now().isoformat(), count, status))
                conn.commit()
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ—¥èªŒè¨˜éŒ„å¤±æ•—: {e}")
    
    def get_historical_data(self, market: str = "btctwd", 
                          timeframe: str = "5m", 
                          limit: int = 100) -> Optional[pd.DataFrame]:
        """
        å¾æ•¸æ“šåº«ç²å–æ­·å²æ•¸æ“š
        
        Args:
            market: äº¤æ˜“å°
            timeframe: æ™‚é–“æ¡†æ¶
            limit: æ•¸æ“šæ¢æ•¸
            
        Returns:
            æ­·å²æ•¸æ“šDataFrame
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                query = '''
                    SELECT timestamp, open, high, low, close, volume
                    FROM klines 
                    WHERE market = ? AND timeframe = ?
                    ORDER BY timestamp DESC
                    LIMIT ?
                '''
                
                df = pd.read_sql_query(query, conn, params=(market, timeframe, limit))
                
                if df.empty:
                    logger.warning(f"âš ï¸ æ²’æœ‰æ‰¾åˆ° {market} {timeframe} çš„æ­·å²æ•¸æ“š")
                    return None
                
                # è½‰æ›æ™‚é–“æˆ³
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
                
                # æŒ‰æ™‚é–“æ­£åºæ’åˆ—
                df = df.sort_values('timestamp').reset_index(drop=True)
                
                logger.info(f"âœ… ç²å–æ­·å²æ•¸æ“š: {len(df)} æ¢ {timeframe} è¨˜éŒ„")
                return df
                
        except Exception as e:
            logger.error(f"âŒ ç²å–æ­·å²æ•¸æ“šå¤±æ•—: {e}")
            return None
    
    def get_multiple_timeframes(self, market: str = "btctwd", 
                              timeframes: Dict[str, int] = None) -> Dict[str, pd.DataFrame]:
        """
        ç²å–å¤šå€‹æ™‚é–“æ¡†æ¶çš„æ­·å²æ•¸æ“š
        
        Args:
            market: äº¤æ˜“å°
            timeframes: æ™‚é–“æ¡†æ¶å’Œæ•¸æ“šé‡é…ç½®ï¼Œå¦‚ {'1m': 100, '5m': 50, '1h': 24}
            
        Returns:
            å„æ™‚é–“æ¡†æ¶çš„æ•¸æ“šå­—å…¸
        """
        if timeframes is None:
            timeframes = {'1m': 100, '5m': 50, '1h': 24}
        
        results = {}
        
        for timeframe, limit in timeframes.items():
            df = self.get_historical_data(market, timeframe, limit)
            if df is not None:
                results[timeframe] = df
        
        logger.info(f"âœ… ç²å–å¤šæ™‚é–“æ¡†æ¶æ•¸æ“š: {list(results.keys())}")
        return results
    
    def get_data_statistics(self, market: str = "btctwd") -> Dict[str, Any]:
        """ç²å–æ•¸æ“šçµ±è¨ˆä¿¡æ¯"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # ç²å–å„æ™‚é–“æ¡†æ¶çš„æ•¸æ“šçµ±è¨ˆ
                cursor.execute('''
                    SELECT timeframe, COUNT(*) as count, 
                           MIN(timestamp) as earliest, 
                           MAX(timestamp) as latest
                    FROM klines 
                    WHERE market = ?
                    GROUP BY timeframe
                ''', (market,))
                
                timeframe_stats = {}
                for row in cursor.fetchall():
                    timeframe, count, earliest, latest = row
                    timeframe_stats[timeframe] = {
                        'count': count,
                        'earliest': datetime.fromtimestamp(earliest).isoformat(),
                        'latest': datetime.fromtimestamp(latest).isoformat(),
                        'coverage_hours': (latest - earliest) / 3600
                    }
                
                # ç²å–æ›´æ–°è¨˜éŒ„
                cursor.execute('''
                    SELECT timeframe, last_update, status
                    FROM update_log 
                    WHERE market = ?
                ''', (market,))
                
                update_stats = {}
                for row in cursor.fetchall():
                    timeframe, last_update, status = row
                    update_stats[timeframe] = {
                        'last_update': last_update,
                        'status': status
                    }
                
                return {
                    'market': market,
                    'timeframe_stats': timeframe_stats,
                    'update_stats': update_stats,
                    'database_path': str(self.db_path),
                    'database_size_mb': self.db_path.stat().st_size / (1024 * 1024) if self.db_path.exists() else 0
                }
                
        except Exception as e:
            logger.error(f"âŒ ç²å–æ•¸æ“šçµ±è¨ˆå¤±æ•—: {e}")
            return {}
    
    async def initialize_full_dataset(self, market: str = "btctwd") -> bool:
        """åˆå§‹åŒ–å®Œæ•´æ•¸æ“šé›†ï¼ˆé¦–æ¬¡ä½¿ç”¨æ™‚èª¿ç”¨ï¼‰"""
        try:
            logger.info(f"ğŸš€ é–‹å§‹åˆå§‹åŒ–å®Œæ•´æ•¸æ“šé›†: {market}")
            
            # æŒ‰å„ªå…ˆç´šé †åºåˆå§‹åŒ–
            timeframes = ['1h', '5m', '1m']  # å¾é•·æœŸåˆ°çŸ­æœŸ
            
            for timeframe in timeframes:
                logger.info(f"ğŸ“Š åˆå§‹åŒ– {timeframe} æ•¸æ“š...")
                success = await self._update_timeframe_data(market, timeframe)
                
                if not success:
                    logger.error(f"âŒ åˆå§‹åŒ– {timeframe} æ•¸æ“šå¤±æ•—")
                    return False
                
                # çŸ­æš«å»¶é²é¿å…APIé™åˆ¶
                await asyncio.sleep(1)
            
            logger.info("âœ… å®Œæ•´æ•¸æ“šé›†åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å®Œæ•´æ•¸æ“šé›†å¤±æ•—: {e}")
            return False
    
    def cleanup_old_data(self, market: str = "btctwd", days_to_keep: int = 30):
        """æ¸…ç†èˆŠæ•¸æ“š"""
        try:
            cutoff_timestamp = int((datetime.now() - timedelta(days=days_to_keep)).timestamp())
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # åˆªé™¤èˆŠæ•¸æ“š
                cursor.execute('''
                    DELETE FROM klines 
                    WHERE market = ? AND timestamp < ?
                ''', (market, cutoff_timestamp))
                
                deleted_count = cursor.rowcount
                conn.commit()
                
                logger.info(f"ğŸ—‘ï¸ æ¸…ç†äº† {deleted_count} æ¢èˆŠæ•¸æ“š")
                
                # å„ªåŒ–æ•¸æ“šåº«
                cursor.execute('VACUUM')
                
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†èˆŠæ•¸æ“šå¤±æ•—: {e}")
    
    async def close(self):
        """é—œé–‰é€£æ¥"""
        try:
            await self.max_client.close()
            logger.info("âœ… æ­·å²æ•¸æ“šç®¡ç†å™¨å·²é—œé–‰")
        except Exception as e:
            logger.error(f"âŒ é—œé–‰æ­·å²æ•¸æ“šç®¡ç†å™¨å¤±æ•—: {e}")


# å‰µå»ºå…¨å±€æ­·å²æ•¸æ“šç®¡ç†å™¨å¯¦ä¾‹
def create_historical_manager(db_path: str = "data/market_history.db") -> HistoricalDataManager:
    """å‰µå»ºæ­·å²æ•¸æ“šç®¡ç†å™¨å¯¦ä¾‹"""
    return HistoricalDataManager(db_path)


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    import asyncio
    
    async def test_historical_manager():
        """æ¸¬è©¦æ­·å²æ•¸æ“šç®¡ç†å™¨"""
        print("ğŸ§ª æ¸¬è©¦æ­·å²æ•¸æ“šç®¡ç†å™¨...")
        
        manager = create_historical_manager()
        
        try:
            # æª¢æŸ¥ä¸¦ç¢ºä¿æ•¸æ“šå®Œæ•´æ€§
            print("ğŸ” æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§...")
            results = await manager.ensure_historical_data("btctwd", ['1m', '5m', '1h'])
            
            for timeframe, success in results.items():
                print(f"   {timeframe}: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±æ•—'}")
            
            # ç²å–æ­·å²æ•¸æ“š
            print("\nğŸ“Š ç²å–æ­·å²æ•¸æ“š...")
            data = manager.get_multiple_timeframes("btctwd", {'1m': 50, '5m': 30, '1h': 12})
            
            for timeframe, df in data.items():
                print(f"   {timeframe}: {len(df)} æ¢è¨˜éŒ„")
            
            # é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
            print("\nğŸ“ˆ æ•¸æ“šçµ±è¨ˆ:")
            stats = manager.get_data_statistics("btctwd")
            
            for timeframe, info in stats.get('timeframe_stats', {}).items():
                print(f"   {timeframe}: {info['count']} æ¢è¨˜éŒ„ï¼Œè¦†è“‹ {info['coverage_hours']:.1f} å°æ™‚")
            
            print(f"\nğŸ’¾ æ•¸æ“šåº«å¤§å°: {stats.get('database_size_mb', 0):.2f} MB")
            
        finally:
            await manager.close()
    
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(test_historical_manager())