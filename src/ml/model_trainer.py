#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ©Ÿå™¨å­¸ç¿’æ¨¡å‹è¨“ç·´å™¨ - ä½¿ç”¨æå–çš„è¨“ç·´æ•¸æ“šè¨“ç·´å¤šç¨®MLæ¨¡å‹
"""

import logging
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import pickle
import json

# æ©Ÿå™¨å­¸ç¿’åº«
try:
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support
    import joblib
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ scikit-learnæœªå®‰è£ï¼Œè«‹é‹è¡Œ: pip install scikit-learn")

from .training_data_extractor import create_training_data_extractor

logger = logging.getLogger(__name__)

class ModelTrainer:
    """æ©Ÿå™¨å­¸ç¿’æ¨¡å‹è¨“ç·´å™¨"""
    
    def __init__(self, model_dir: str = "AImax/models"):
        """
        åˆå§‹åŒ–æ¨¡å‹è¨“ç·´å™¨
        
        Args:
            model_dir: æ¨¡å‹ä¿å­˜ç›®éŒ„
        """
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        self.data_extractor = create_training_data_extractor()
        
        # æ¨¡å‹é…ç½®
        self.model_configs = {
            'random_forest': {
                'model': RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    random_state=42
                ),
                'param_grid': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [5, 10, 15],
                    'min_samples_split': [2, 5, 10]
                }
            },
            'gradient_boosting': {
                'model': GradientBoostingClassifier(
                    n_estimators=100,
                    learning_rate=0.1,
                    max_depth=6,
                    random_state=42
                ),
                'param_grid': {
                    'n_estimators': [50, 100, 200],
                    'learning_rate': [0.05, 0.1, 0.2],
                    'max_depth': [3, 6, 9]
                }
            },
            'logistic_regression': {
                'model': LogisticRegression(
                    random_state=42,
                    max_iter=1000
                ),
                'param_grid': {
                    'C': [0.1, 1.0, 10.0],
                    'penalty': ['l1', 'l2'],
                    'solver': ['liblinear']
                }
            }
        } if SKLEARN_AVAILABLE else {}
        
        # ç‰¹å¾µé¸æ“‡é…ç½®
        self.feature_groups = {
            'price_features': ['open', 'high', 'low', 'close', 'volume'],
            'technical_features': [
                'rsi_7', 'rsi_14', 'rsi_21',
                'sma_5', 'sma_10', 'sma_20', 'sma_50',
                'ema_12', 'ema_26',
                'macd', 'macd_signal', 'macd_histogram',
                'bb_width', 'volume_ratio'
            ],
            'derived_features': [
                'price_change_1', 'price_change_5', 'price_change_15',
                'volatility_5', 'volatility_20',
                'momentum_5', 'momentum_10',
                'price_position_bb', 'price_vs_sma20'
            ],
            'time_features': ['hour', 'day_of_week', 'is_weekend']
        }
        
        logger.info("ğŸ¤– æ©Ÿå™¨å­¸ç¿’æ¨¡å‹è¨“ç·´å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def train_all_models(self, 
                             market: str = "btctwd",
                             days: int = 30,
                             target_column: str = "signal_3class") -> Dict[str, Any]:
        """
        è¨“ç·´æ‰€æœ‰æ¨¡å‹
        
        Args:
            market: äº¤æ˜“å°
            days: è¨“ç·´æ•¸æ“šå¤©æ•¸
            target_column: ç›®æ¨™åˆ—å
            
        Returns:
            è¨“ç·´çµæœå­—å…¸
        """
        if not SKLEARN_AVAILABLE:
            logger.error("âŒ scikit-learnæœªå®‰è£ï¼Œç„¡æ³•é€²è¡Œæ¨¡å‹è¨“ç·´")
            return {}
        
        try:
            logger.info(f"ğŸš€ é–‹å§‹è¨“ç·´æ‰€æœ‰æ¨¡å‹: {market}, {days}å¤©æ•¸æ“š")
            
            # æ­¥é©Ÿ1: æå–è¨“ç·´æ•¸æ“š
            training_data = await self._prepare_training_data(market, days)
            if training_data is None:
                logger.error("âŒ ç„¡æ³•ç²å–è¨“ç·´æ•¸æ“š")
                return {}
            
            # æ­¥é©Ÿ2: æº–å‚™ç‰¹å¾µå’Œæ¨™ç±¤
            X, y, feature_names = self._prepare_features_and_labels(training_data, target_column)
            if X is None:
                logger.error("âŒ ç‰¹å¾µæº–å‚™å¤±æ•—")
                return {}
            
            # æ­¥é©Ÿ3: åˆ†å‰²è¨“ç·´å’Œæ¸¬è©¦æ•¸æ“š
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # æ­¥é©Ÿ4: ç‰¹å¾µæ¨™æº–åŒ–
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # æ­¥é©Ÿ5: è¨“ç·´æ‰€æœ‰æ¨¡å‹
            results = {}
            for model_name, config in self.model_configs.items():
                logger.info(f"ğŸ”„ è¨“ç·´æ¨¡å‹: {model_name}")
                
                model_result = self._train_single_model(
                    model_name, config,
                    X_train_scaled, X_test_scaled, y_train, y_test,
                    feature_names
                )
                
                if model_result:
                    results[model_name] = model_result
            
            # æ­¥é©Ÿ6: é¸æ“‡æœ€ä½³æ¨¡å‹
            best_model_name = self._select_best_model(results)
            results['best_model'] = best_model_name
            results['scaler'] = scaler
            results['feature_names'] = feature_names
            results['training_info'] = {
                'market': market,
                'days': days,
                'target_column': target_column,
                'training_samples': len(X_train),
                'test_samples': len(X_test),
                'features_count': len(feature_names),
                'timestamp': datetime.now().isoformat()
            }
            
            # æ­¥é©Ÿ7: ä¿å­˜è¨“ç·´çµæœ
            self._save_training_results(results, market)
            
            logger.info(f"âœ… æ‰€æœ‰æ¨¡å‹è¨“ç·´å®Œæˆï¼Œæœ€ä½³æ¨¡å‹: {best_model_name}")
            return results
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è¨“ç·´å¤±æ•—: {e}")
            return {}
    
    async def _prepare_training_data(self, market: str, days: int) -> Optional[pd.DataFrame]:
        """æº–å‚™è¨“ç·´æ•¸æ“š"""
        try:
            logger.info("ğŸ“Š æº–å‚™è¨“ç·´æ•¸æ“š...")
            
            training_data = await self.data_extractor.extract_training_dataset(
                market=market,
                days=days,
                timeframe="5m"
            )
            
            if training_data is None or training_data.empty:
                logger.error("âŒ ç„¡æ³•æå–è¨“ç·´æ•¸æ“š")
                return None
            
            logger.info(f"âœ… è¨“ç·´æ•¸æ“šæº–å‚™å®Œæˆ: {len(training_data)} æ¢è¨˜éŒ„")
            return training_data
            
        except Exception as e:
            logger.error(f"âŒ æº–å‚™è¨“ç·´æ•¸æ“šå¤±æ•—: {e}")
            return None
    
    def _prepare_features_and_labels(self, 
                                   df: pd.DataFrame, 
                                   target_column: str) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[List[str]]]:
        """æº–å‚™ç‰¹å¾µå’Œæ¨™ç±¤"""
        try:
            logger.info("ğŸ”§ æº–å‚™ç‰¹å¾µå’Œæ¨™ç±¤...")
            
            # é¸æ“‡ç‰¹å¾µåˆ—
            feature_columns = []
            for group_name, features in self.feature_groups.items():
                available_features = [f for f in features if f in df.columns]
                feature_columns.extend(available_features)
                logger.info(f"   {group_name}: {len(available_features)} å€‹ç‰¹å¾µ")
            
            # æª¢æŸ¥ç›®æ¨™åˆ—
            if target_column not in df.columns:
                logger.error(f"âŒ ç›®æ¨™åˆ— {target_column} ä¸å­˜åœ¨")
                return None, None, None
            
            # æå–ç‰¹å¾µå’Œæ¨™ç±¤
            X = df[feature_columns].values
            y = df[target_column].values
            
            # ç§»é™¤åŒ…å«NaNçš„æ¨£æœ¬
            valid_mask = ~(np.isnan(X).any(axis=1) | np.isnan(y))
            X = X[valid_mask]
            y = y[valid_mask]
            
            logger.info(f"âœ… ç‰¹å¾µæº–å‚™å®Œæˆ: {X.shape[0]} æ¨£æœ¬, {X.shape[1]} ç‰¹å¾µ")
            logger.info(f"ğŸ“Š æ¨™ç±¤åˆ†ä½ˆ: {np.bincount(y.astype(int))}")
            
            return X, y, feature_columns
            
        except Exception as e:
            logger.error(f"âŒ æº–å‚™ç‰¹å¾µå’Œæ¨™ç±¤å¤±æ•—: {e}")
            return None, None, None
    
    def _train_single_model(self,
                          model_name: str,
                          config: Dict[str, Any],
                          X_train: np.ndarray,
                          X_test: np.ndarray,
                          y_train: np.ndarray,
                          y_test: np.ndarray,
                          feature_names: List[str]) -> Optional[Dict[str, Any]]:
        """è¨“ç·´å–®å€‹æ¨¡å‹"""
        try:
            model = config['model']
            
            # è¨“ç·´æ¨¡å‹
            model.fit(X_train, y_train)
            
            # é æ¸¬
            y_pred_train = model.predict(X_train)
            y_pred_test = model.predict(X_test)
            
            # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
            train_accuracy = accuracy_score(y_train, y_pred_train)
            test_accuracy = accuracy_score(y_test, y_pred_test)
            
            # äº¤å‰é©—è­‰
            cv_scores = cross_val_score(model, X_train, y_train, cv=5)
            
            # åˆ†é¡å ±å‘Š
            classification_rep = classification_report(y_test, y_pred_test, output_dict=True)
            
            # ç‰¹å¾µé‡è¦æ€§ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
            feature_importance = None
            if hasattr(model, 'feature_importances_'):
                feature_importance = dict(zip(feature_names, model.feature_importances_))
                # æ’åºç‰¹å¾µé‡è¦æ€§
                feature_importance = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True))
            
            result = {
                'model': model,
                'train_accuracy': train_accuracy,
                'test_accuracy': test_accuracy,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'classification_report': classification_rep,
                'confusion_matrix': confusion_matrix(y_test, y_pred_test).tolist(),
                'feature_importance': feature_importance
            }
            
            logger.info(f"âœ… {model_name} è¨“ç·´å®Œæˆ - æ¸¬è©¦æº–ç¢ºç‡: {test_accuracy:.4f}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ è¨“ç·´ {model_name} å¤±æ•—: {e}")
            return None
    
    def _select_best_model(self, results: Dict[str, Any]) -> str:
        """é¸æ“‡æœ€ä½³æ¨¡å‹"""
        try:
            best_model = None
            best_score = -1
            
            for model_name, result in results.items():
                if isinstance(result, dict) and 'test_accuracy' in result:
                    # ç¶œåˆè©•åˆ†ï¼šæ¸¬è©¦æº–ç¢ºç‡ * 0.6 + äº¤å‰é©—è­‰å¹³å‡åˆ† * 0.4
                    score = result['test_accuracy'] * 0.6 + result['cv_mean'] * 0.4
                    
                    if score > best_score:
                        best_score = score
                        best_model = model_name
            
            logger.info(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model} (ç¶œåˆè©•åˆ†: {best_score:.4f})")
            return best_model or 'random_forest'
            
        except Exception as e:
            logger.error(f"âŒ é¸æ“‡æœ€ä½³æ¨¡å‹å¤±æ•—: {e}")
            return 'random_forest'
    
    def _save_training_results(self, results: Dict[str, Any], market: str):
        """ä¿å­˜è¨“ç·´çµæœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜æ¨¡å‹
            for model_name, result in results.items():
                if isinstance(result, dict) and 'model' in result:
                    model_file = self.model_dir / f"{market}_{model_name}_{timestamp}.joblib"
                    joblib.dump(result['model'], model_file)
                    logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_file}")
            
            # ä¿å­˜æ¨™æº–åŒ–å™¨
            if 'scaler' in results:
                scaler_file = self.model_dir / f"{market}_scaler_{timestamp}.joblib"
                joblib.dump(results['scaler'], scaler_file)
            
            # ä¿å­˜è¨“ç·´å ±å‘Š
            report = {
                'training_info': results.get('training_info', {}),
                'best_model': results.get('best_model'),
                'feature_names': results.get('feature_names', []),
                'model_performance': {}
            }
            
            for model_name, result in results.items():
                if isinstance(result, dict) and 'test_accuracy' in result:
                    report['model_performance'][model_name] = {
                        'test_accuracy': result['test_accuracy'],
                        'cv_mean': result['cv_mean'],
                        'cv_std': result['cv_std'],
                        'classification_report': result['classification_report']
                    }
            
            report_file = self.model_dir / f"{market}_training_report_{timestamp}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“Š è¨“ç·´å ±å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜è¨“ç·´çµæœå¤±æ•—: {e}")
    
    def load_trained_model(self, market: str, model_name: str = None) -> Optional[Dict[str, Any]]:
        """è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹"""
        try:
            # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
            if model_name is None:
                # è¼‰å…¥æœ€æ–°çš„æœ€ä½³æ¨¡å‹
                model_files = list(self.model_dir.glob(f"{market}_*_*.joblib"))
                if not model_files:
                    logger.warning(f"âš ï¸ æ²’æœ‰æ‰¾åˆ° {market} çš„æ¨¡å‹æ–‡ä»¶")
                    return None
                
                latest_file = max(model_files, key=lambda x: x.stat().st_mtime)
                model = joblib.load(latest_file)
                
                # è¼‰å…¥å°æ‡‰çš„æ¨™æº–åŒ–å™¨
                timestamp = latest_file.stem.split('_')[-1]
                scaler_file = self.model_dir / f"{market}_scaler_{timestamp}.joblib"
                scaler = joblib.load(scaler_file) if scaler_file.exists() else None
                
                return {
                    'model': model,
                    'scaler': scaler,
                    'model_file': str(latest_file)
                }
            else:
                # è¼‰å…¥æŒ‡å®šæ¨¡å‹
                model_files = list(self.model_dir.glob(f"{market}_{model_name}_*.joblib"))
                if not model_files:
                    logger.warning(f"âš ï¸ æ²’æœ‰æ‰¾åˆ° {market} {model_name} çš„æ¨¡å‹æ–‡ä»¶")
                    return None
                
                latest_file = max(model_files, key=lambda x: x.stat().st_mtime)
                model = joblib.load(latest_file)
                
                return {'model': model, 'model_file': str(latest_file)}
                
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
            return None
    
    def predict(self, model_data: Dict[str, Any], features: np.ndarray) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œé æ¸¬"""
        try:
            model = model_data['model']
            scaler = model_data.get('scaler')
            
            # æ¨™æº–åŒ–ç‰¹å¾µ
            if scaler is not None:
                features_scaled = scaler.transform(features.reshape(1, -1))
            else:
                features_scaled = features.reshape(1, -1)
            
            # é æ¸¬
            prediction = model.predict(features_scaled)[0]
            
            # é æ¸¬æ¦‚ç‡ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
            prediction_proba = None
            if hasattr(model, 'predict_proba'):
                prediction_proba = model.predict_proba(features_scaled)[0]
            
            return {
                'prediction': int(prediction),
                'prediction_proba': prediction_proba.tolist() if prediction_proba is not None else None,
                'confidence': float(max(prediction_proba)) if prediction_proba is not None else 0.5,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹é æ¸¬å¤±æ•—: {e}")
            return None


# å‰µå»ºå…¨å±€å¯¦ä¾‹
def create_model_trainer(model_dir: str = "AImax/models") -> ModelTrainer:
    """å‰µå»ºæ¨¡å‹è¨“ç·´å™¨å¯¦ä¾‹"""
    return ModelTrainer(model_dir)


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    import asyncio
    
    async def test_model_trainer():
        """æ¸¬è©¦æ¨¡å‹è¨“ç·´å™¨"""
        print("ğŸ§ª æ¸¬è©¦æ¨¡å‹è¨“ç·´å™¨...")
        
        if not SKLEARN_AVAILABLE:
            print("âŒ scikit-learnæœªå®‰è£ï¼Œç„¡æ³•æ¸¬è©¦")
            return
        
        trainer = create_model_trainer()
        
        try:
            # è¨“ç·´æ¨¡å‹
            results = await trainer.train_all_models(
                market="btctwd",
                days=7,  # æ¸¬è©¦ç”¨è¼ƒå°‘å¤©æ•¸
                target_column="signal_3class"
            )
            
            if results:
                print(f"âœ… æ¨¡å‹è¨“ç·´æˆåŠŸ!")
                print(f"ğŸ† æœ€ä½³æ¨¡å‹: {results.get('best_model')}")
                
                # é¡¯ç¤ºå„æ¨¡å‹æ€§èƒ½
                for model_name, result in results.items():
                    if isinstance(result, dict) and 'test_accuracy' in result:
                        print(f"   {model_name}: æ¸¬è©¦æº–ç¢ºç‡ {result['test_accuracy']:.4f}")
                
                # æ¸¬è©¦æ¨¡å‹è¼‰å…¥å’Œé æ¸¬
                model_data = trainer.load_trained_model("btctwd")
                if model_data:
                    print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
                    
                    # æ¨¡æ“¬ç‰¹å¾µé€²è¡Œé æ¸¬æ¸¬è©¦
                    feature_count = len(results.get('feature_names', []))
                    if feature_count > 0:
                        test_features = np.random.randn(feature_count)
                        prediction = trainer.predict(model_data, test_features)
                        if prediction:
                            print(f"âœ… é æ¸¬æ¸¬è©¦æˆåŠŸ: {prediction}")
                
            else:
                print("âŒ æ¨¡å‹è¨“ç·´å¤±æ•—")
                
        except Exception as e:
            print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
    
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(test_model_trainer())