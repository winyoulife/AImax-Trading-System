#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LSTMåƒ¹æ ¼é æ¸¬å™¨ - ä½¿ç”¨æ·±åº¦å­¸ç¿’é€²è¡Œæ™‚é–“åºåˆ—åƒ¹æ ¼é æ¸¬
"""

import logging
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import json
import pickle

# æ·±åº¦å­¸ç¿’åº«
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, load_model
    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("âš ï¸ TensorFlowæœªå®‰è£ï¼Œè«‹é‹è¡Œ: pip install tensorflow")

logger = logging.getLogger(__name__)

class LSTMPricePredictor:
    """LSTMåƒ¹æ ¼é æ¸¬å™¨"""
    
    def __init__(self, model_dir: str = "AImax/models"):
        """
        åˆå§‹åŒ–LSTMåƒ¹æ ¼é æ¸¬å™¨
        
        Args:
            model_dir: æ¨¡å‹ä¿å­˜ç›®éŒ„
        """
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹é…ç½®
        self.config = {
            'sequence_length': 60,  # ä½¿ç”¨60å€‹æ™‚é–“é»é æ¸¬ä¸‹ä¸€å€‹
            'features': ['close', 'volume', 'high', 'low'],  # ä½¿ç”¨çš„ç‰¹å¾µ
            'prediction_horizon': 1,  # é æ¸¬æœªä¾†1å€‹æ™‚é–“é»
            'lstm_units': [50, 50],  # LSTMå±¤å–®å…ƒæ•¸
            'dropout_rate': 0.2,
            'batch_size': 32,
            'epochs': 100,
            'validation_split': 0.2,
            'learning_rate': 0.001
        }
        
        # æ¨¡å‹çµ„ä»¶
        self.model = None
        self.scaler = None
        self.feature_scalers = {}
        self.training_history = None
        
        # é©—è­‰æŒ‡æ¨™
        self.validation_metrics = {}
        self.prediction_errors = []
        self.confidence_intervals = {}
        
        logger.info("ğŸ§  LSTMåƒ¹æ ¼é æ¸¬å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def prepare_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        æº–å‚™LSTMè¨“ç·´æ•¸æ“š
        
        Args:
            df: åŒ…å«åƒ¹æ ¼æ•¸æ“šçš„DataFrame
            
        Returns:
            X_train, X_test, y_train, y_test
        """
        try:
            logger.info("ğŸ“Š æº–å‚™LSTMè¨“ç·´æ•¸æ“š...")
            
            # é¸æ“‡ç‰¹å¾µåˆ—
            feature_data = df[self.config['features']].values
            
            # æ¨™æº–åŒ–ç‰¹å¾µ
            if self.scaler is None:
                self.scaler = MinMaxScaler()
                scaled_data = self.scaler.fit_transform(feature_data)
            else:
                scaled_data = self.scaler.transform(feature_data)
            
            # å‰µå»ºåºåˆ—æ•¸æ“š
            X, y = self._create_sequences(scaled_data)
            
            # åˆ†å‰²è¨“ç·´å’Œæ¸¬è©¦æ•¸æ“š
            split_idx = int(len(X) * 0.8)
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]
            
            logger.info(f"âœ… æ•¸æ“šæº–å‚™å®Œæˆ: è¨“ç·´é›† {X_train.shape}, æ¸¬è©¦é›† {X_test.shape}")
            return X_train, X_test, y_train, y_test
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šæº–å‚™å¤±æ•—: {e}")
            return None, None, None, None
    
    def _create_sequences(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """å‰µå»ºLSTMåºåˆ—æ•¸æ“š"""
        try:
            X, y = [], []
            seq_len = self.config['sequence_length']
            
            for i in range(seq_len, len(data)):
                X.append(data[i-seq_len:i])
                y.append(data[i, 0])  # é æ¸¬æ”¶ç›¤åƒ¹
            
            return np.array(X), np.array(y)
            
        except Exception as e:
            logger.error(f"âŒ å‰µå»ºåºåˆ—æ•¸æ“šå¤±æ•—: {e}")
            return np.array([]), np.array([])
    
    def build_model(self) -> bool:
        """æ§‹å»ºLSTMæ¨¡å‹"""
        try:
            if not TENSORFLOW_AVAILABLE:
                logger.error("âŒ TensorFlowæœªå®‰è£ï¼Œç„¡æ³•æ§‹å»ºæ¨¡å‹")
                return False
            
            logger.info("ğŸ—ï¸ æ§‹å»ºLSTMæ¨¡å‹...")
            
            model = Sequential()
            
            # ç¬¬ä¸€å€‹LSTMå±¤
            model.add(LSTM(
                units=self.config['lstm_units'][0],
                return_sequences=True,
                input_shape=(self.config['sequence_length'], len(self.config['features']))
            ))
            model.add(Dropout(self.config['dropout_rate']))
            model.add(BatchNormalization())
            
            # ç¬¬äºŒå€‹LSTMå±¤
            model.add(LSTM(
                units=self.config['lstm_units'][1],
                return_sequences=False
            ))
            model.add(Dropout(self.config['dropout_rate']))
            model.add(BatchNormalization())
            
            # è¼¸å‡ºå±¤
            model.add(Dense(units=1))
            
            # ç·¨è­¯æ¨¡å‹
            model.compile(
                optimizer=Adam(learning_rate=self.config['learning_rate']),
                loss='mse',
                metrics=['mae']
            )
            
            self.model = model
            logger.info("âœ… LSTMæ¨¡å‹æ§‹å»ºå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ§‹å»ºLSTMæ¨¡å‹å¤±æ•—: {e}")
            return False
    
    def train(self, X_train: np.ndarray, y_train: np.ndarray, 
              X_val: np.ndarray = None, y_val: np.ndarray = None) -> bool:
        """
        è¨“ç·´LSTMæ¨¡å‹
        
        Args:
            X_train: è¨“ç·´ç‰¹å¾µ
            y_train: è¨“ç·´æ¨™ç±¤
            X_val: é©—è­‰ç‰¹å¾µ
            y_val: é©—è­‰æ¨™ç±¤
            
        Returns:
            è¨“ç·´æ˜¯å¦æˆåŠŸ
        """
        try:
            if not TENSORFLOW_AVAILABLE or self.model is None:
                logger.error("âŒ æ¨¡å‹æœªæº–å‚™å¥½ï¼Œç„¡æ³•è¨“ç·´")
                return False
            
            logger.info("ğŸš€ é–‹å§‹è¨“ç·´LSTMæ¨¡å‹...")
            
            # æº–å‚™å›èª¿å‡½æ•¸
            callbacks = [
                EarlyStopping(
                    monitor='val_loss',
                    patience=10,
                    restore_best_weights=True
                ),
                ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=5,
                    min_lr=1e-7
                )
            ]
            
            # è¨“ç·´æ¨¡å‹
            if X_val is not None and y_val is not None:
                validation_data = (X_val, y_val)
            else:
                validation_data = None
            
            history = self.model.fit(
                X_train, y_train,
                batch_size=self.config['batch_size'],
                epochs=self.config['epochs'],
                validation_split=self.config['validation_split'] if validation_data is None else 0,
                validation_data=validation_data,
                callbacks=callbacks,
                verbose=1
            )
            
            self.training_history = history.history
            logger.info("âœ… LSTMæ¨¡å‹è¨“ç·´å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ LSTMæ¨¡å‹è¨“ç·´å¤±æ•—: {e}")
            return False
    
    def predict(self, X: np.ndarray) -> Optional[np.ndarray]:
        """
        ä½¿ç”¨LSTMæ¨¡å‹é€²è¡Œé æ¸¬
        
        Args:
            X: è¼¸å…¥ç‰¹å¾µ
            
        Returns:
            é æ¸¬çµæœ
        """
        try:
            if not TENSORFLOW_AVAILABLE or self.model is None:
                logger.error("âŒ æ¨¡å‹æœªæº–å‚™å¥½ï¼Œç„¡æ³•é æ¸¬")
                return None
            
            # é€²è¡Œé æ¸¬
            predictions = self.model.predict(X, verbose=0)
            
            # åæ¨™æº–åŒ–é æ¸¬çµæœ
            if self.scaler is not None:
                # å‰µå»ºèˆ‡åŸå§‹ç‰¹å¾µç›¸åŒå½¢ç‹€çš„æ•¸çµ„é€²è¡Œåæ¨™æº–åŒ–
                dummy_features = np.zeros((len(predictions), len(self.config['features'])))
                dummy_features[:, 0] = predictions.flatten()  # æ”¶ç›¤åƒ¹åœ¨ç¬¬ä¸€åˆ—
                
                inverse_scaled = self.scaler.inverse_transform(dummy_features)
                predictions = inverse_scaled[:, 0].reshape(-1, 1)
            
            return predictions
            
        except Exception as e:
            logger.error(f"âŒ LSTMé æ¸¬å¤±æ•—: {e}")
            return None
    
    def predict_single(self, sequence: np.ndarray) -> Optional[Dict[str, Any]]:
        """
        é æ¸¬å–®å€‹åºåˆ—çš„ä¸‹ä¸€å€‹åƒ¹æ ¼
        
        Args:
            sequence: è¼¸å…¥åºåˆ—
            
        Returns:
            é æ¸¬çµæœå­—å…¸
        """
        try:
            if sequence.shape != (self.config['sequence_length'], len(self.config['features'])):
                logger.error(f"âŒ è¼¸å…¥åºåˆ—å½¢ç‹€éŒ¯èª¤: {sequence.shape}")
                return None
            
            # æ¨™æº–åŒ–è¼¸å…¥
            if self.scaler is not None:
                sequence_scaled = self.scaler.transform(sequence)
            else:
                sequence_scaled = sequence
            
            # é æ¸¬
            X_input = sequence_scaled.reshape(1, self.config['sequence_length'], len(self.config['features']))
            prediction = self.predict(X_input)
            
            if prediction is not None:
                predicted_price = float(prediction[0, 0])
                
                # è¨ˆç®—ä¿¡å¿ƒåº¦ï¼ˆåŸºæ–¼è¨“ç·´æ­·å²ï¼‰
                confidence = self._calculate_prediction_confidence(sequence_scaled)
                
                return {
                    'predicted_price': predicted_price,
                    'confidence': confidence,
                    'timestamp': datetime.now().isoformat(),
                    'model_type': 'LSTM'
                }
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ å–®æ¬¡é æ¸¬å¤±æ•—: {e}")
            return None
    
    def _calculate_prediction_confidence(self, sequence: np.ndarray) -> float:
        """è¨ˆç®—é æ¸¬ä¿¡å¿ƒåº¦"""
        try:
            # åŸºæ–¼è¨“ç·´æ­·å²è¨ˆç®—ä¿¡å¿ƒåº¦
            if self.training_history is None:
                return 0.5
            
            # ä½¿ç”¨é©—è­‰æå¤±ä½œç‚ºä¿¡å¿ƒåº¦æŒ‡æ¨™
            val_loss = self.training_history.get('val_loss', [1.0])
            final_val_loss = val_loss[-1] if val_loss else 1.0
            
            # å°‡æå¤±è½‰æ›ç‚ºä¿¡å¿ƒåº¦ (æå¤±è¶Šå°ï¼Œä¿¡å¿ƒåº¦è¶Šé«˜)
            confidence = max(0.1, min(0.9, 1.0 / (1.0 + final_val_loss)))
            
            return confidence
            
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—é æ¸¬ä¿¡å¿ƒåº¦å¤±æ•—: {e}")
            return 0.5
    
    # ==================== æ¨¡å‹é©—è­‰åŠŸèƒ½ ====================
    
    def validate_model(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Any]:
        """
        é©—è­‰LSTMæ¨¡å‹æ€§èƒ½
        
        Args:
            X_test: æ¸¬è©¦ç‰¹å¾µ
            y_test: æ¸¬è©¦æ¨™ç±¤
            
        Returns:
            é©—è­‰çµæœå­—å…¸
        """
        try:
            if not TENSORFLOW_AVAILABLE or self.model is None:
                logger.error("âŒ æ¨¡å‹æœªæº–å‚™å¥½ï¼Œç„¡æ³•é©—è­‰")
                return {'success': False, 'error': 'Model not ready'}
            
            logger.info("ğŸ” é–‹å§‹LSTMæ¨¡å‹é©—è­‰...")
            
            # é€²è¡Œé æ¸¬
            predictions = self.predict(X_test)
            if predictions is None:
                return {'success': False, 'error': 'Prediction failed'}
            
            # åæ¨™æº–åŒ–çœŸå¯¦å€¼
            y_true = self._inverse_transform_target(y_test)
            y_pred = predictions.flatten()
            
            # è¨ˆç®—åŸºæœ¬æŒ‡æ¨™
            mse = mean_squared_error(y_true, y_pred)
            mae = mean_absolute_error(y_true, y_pred)
            rmse = np.sqrt(mse)
            r2 = r2_score(y_true, y_pred)
            
            # è¨ˆç®—æ–¹å‘æº–ç¢ºç‡
            direction_accuracy = self._calculate_direction_accuracy(y_true, y_pred)
            
            # è¨ˆç®—é æ¸¬ç©©å®šæ€§
            stability_score = self._calculate_prediction_stability(predictions)
            
            # è¨ˆç®—ä¸åŒæ™‚é–“çª—å£çš„æœ‰æ•ˆæ€§
            time_window_effectiveness = self._test_time_window_effectiveness(X_test, y_test)
            
            # è¨ˆç®—ä¿¡å¿ƒå€é–“
            confidence_intervals = self._calculate_confidence_intervals(y_true, y_pred)
            
            # æ¨¡å‹æ”¶æ–‚æ€§æª¢æŸ¥
            convergence_check = self._check_model_convergence()
            
            # ä¿å­˜é©—è­‰æŒ‡æ¨™
            self.validation_metrics = {
                'mse': float(mse),
                'mae': float(mae),
                'rmse': float(rmse),
                'r2_score': float(r2),
                'direction_accuracy': direction_accuracy,
                'stability_score': stability_score,
                'time_window_effectiveness': time_window_effectiveness,
                'confidence_intervals': confidence_intervals,
                'convergence_check': convergence_check,
                'validation_timestamp': datetime.now().isoformat()
            }
            
            logger.info(f"âœ… LSTMæ¨¡å‹é©—è­‰å®Œæˆ")
            logger.info(f"   RMSE: {rmse:.2f}")
            logger.info(f"   MAE: {mae:.2f}")
            logger.info(f"   RÂ²: {r2:.4f}")
            logger.info(f"   æ–¹å‘æº–ç¢ºç‡: {direction_accuracy:.2%}")
            logger.info(f"   ç©©å®šæ€§åˆ†æ•¸: {stability_score:.4f}")
            
            return {
                'success': True,
                'metrics': self.validation_metrics,
                'summary': {
                    'overall_score': self._calculate_overall_score(),
                    'strengths': self._identify_model_strengths(),
                    'weaknesses': self._identify_model_weaknesses(),
                    'recommendations': self._generate_improvement_recommendations()
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ LSTMæ¨¡å‹é©—è­‰å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}
    
    def _inverse_transform_target(self, y: np.ndarray) -> np.ndarray:
        """åæ¨™æº–åŒ–ç›®æ¨™å€¼"""
        try:
            if self.scaler is None:
                return y
            
            # å‰µå»ºèˆ‡åŸå§‹ç‰¹å¾µç›¸åŒå½¢ç‹€çš„æ•¸çµ„é€²è¡Œåæ¨™æº–åŒ–
            dummy_features = np.zeros((len(y), len(self.config['features'])))
            dummy_features[:, 0] = y.flatten()  # æ”¶ç›¤åƒ¹åœ¨ç¬¬ä¸€åˆ—
            
            inverse_scaled = self.scaler.inverse_transform(dummy_features)
            return inverse_scaled[:, 0]
            
        except Exception as e:
            logger.error(f"âŒ åæ¨™æº–åŒ–ç›®æ¨™å€¼å¤±æ•—: {e}")
            return y
    
    def _calculate_direction_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """è¨ˆç®—æ–¹å‘é æ¸¬æº–ç¢ºç‡"""
        try:
            if len(y_true) < 2 or len(y_pred) < 2:
                return 0.0
            
            # è¨ˆç®—çœŸå¯¦å’Œé æ¸¬çš„æ–¹å‘è®ŠåŒ–
            true_directions = np.diff(y_true) > 0
            pred_directions = np.diff(y_pred) > 0
            
            # è¨ˆç®—æ–¹å‘ä¸€è‡´æ€§
            correct_directions = np.sum(true_directions == pred_directions)
            total_directions = len(true_directions)
            
            return correct_directions / total_directions if total_directions > 0 else 0.0
            
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—æ–¹å‘æº–ç¢ºç‡å¤±æ•—: {e}")
            return 0.0
    
    def _calculate_prediction_stability(self, predictions: np.ndarray) -> float:
        """è¨ˆç®—é æ¸¬ç©©å®šæ€§"""
        try:
            if len(predictions) < 2:
                return 0.0
            
            # è¨ˆç®—é æ¸¬å€¼çš„è®ŠåŒ–ç‡
            pred_changes = np.diff(predictions.flatten())
            
            # è¨ˆç®—ç©©å®šæ€§åˆ†æ•¸ï¼ˆè®ŠåŒ–ç‡çš„æ¨™æº–å·®è¶Šå°ï¼Œç©©å®šæ€§è¶Šé«˜ï¼‰
            stability = 1.0 / (1.0 + np.std(pred_changes))
            
            return float(stability)
            
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—é æ¸¬ç©©å®šæ€§å¤±æ•—: {e}")
            return 0.0
    
    def _test_time_window_effectiveness(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:
        """æ¸¬è©¦ä¸åŒæ™‚é–“çª—å£çš„é æ¸¬æœ‰æ•ˆæ€§"""
        try:
            effectiveness = {}
            
            # æ¸¬è©¦ä¸åŒé•·åº¦çš„æ™‚é–“çª—å£
            window_sizes = [10, 30, 60, 100]
            
            for window_size in window_sizes:
                if len(X_test) > window_size:
                    # å–æœ€è¿‘çš„window_sizeå€‹æ¨£æœ¬
                    X_window = X_test[-window_size:]
                    y_window = y_test[-window_size:]
                    
                    # é æ¸¬
                    pred_window = self.predict(X_window)
                    if pred_window is not None:
                        y_true_window = self._inverse_transform_target(y_window)
                        y_pred_window = pred_window.flatten()
                        
                        # è¨ˆç®—è©²çª—å£çš„RÂ²åˆ†æ•¸
                        r2_window = r2_score(y_true_window, y_pred_window)
                        effectiveness[f'window_{window_size}'] = float(r2_window)
            
            return effectiveness
            
        except Exception as e:
            logger.error(f"âŒ æ¸¬è©¦æ™‚é–“çª—å£æœ‰æ•ˆæ€§å¤±æ•—: {e}")
            return {}
    
    def _calculate_confidence_intervals(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """è¨ˆç®—é æ¸¬ä¿¡å¿ƒå€é–“"""
        try:
            # è¨ˆç®—é æ¸¬èª¤å·®
            errors = y_true - y_pred
            
            # è¨ˆç®—ä¸åŒç™¾åˆ†ä½çš„èª¤å·®
            percentiles = [5, 25, 50, 75, 95]
            intervals = {}
            
            for p in percentiles:
                intervals[f'percentile_{p}'] = float(np.percentile(np.abs(errors), p))
            
            # è¨ˆç®—æ¨™æº–èª¤å·®
            intervals['standard_error'] = float(np.std(errors))
            intervals['mean_absolute_error'] = float(np.mean(np.abs(errors)))
            
            return intervals
            
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—ä¿¡å¿ƒå€é–“å¤±æ•—: {e}")
            return {}
    
    def _check_model_convergence(self) -> Dict[str, Any]:
        """æª¢æŸ¥æ¨¡å‹æ”¶æ–‚æ€§"""
        try:
            if self.training_history is None:
                return {'converged': False, 'reason': 'No training history'}
            
            train_loss = self.training_history.get('loss', [])
            val_loss = self.training_history.get('val_loss', [])
            
            if not train_loss or not val_loss:
                return {'converged': False, 'reason': 'Incomplete training history'}
            
            # æª¢æŸ¥æå¤±æ˜¯å¦æ”¶æ–‚
            recent_train_loss = train_loss[-10:] if len(train_loss) >= 10 else train_loss
            recent_val_loss = val_loss[-10:] if len(val_loss) >= 10 else val_loss
            
            # è¨ˆç®—æœ€è¿‘æå¤±çš„è®ŠåŒ–è¶¨å‹¢
            train_trend = np.polyfit(range(len(recent_train_loss)), recent_train_loss, 1)[0]
            val_trend = np.polyfit(range(len(recent_val_loss)), recent_val_loss, 1)[0]
            
            # æª¢æŸ¥éæ“¬åˆ
            final_train_loss = train_loss[-1]
            final_val_loss = val_loss[-1]
            overfitting_ratio = final_val_loss / final_train_loss if final_train_loss > 0 else float('inf')
            
            converged = (
                abs(train_trend) < 0.001 and  # è¨“ç·´æå¤±è¶¨æ–¼ç©©å®š
                abs(val_trend) < 0.001 and    # é©—è­‰æå¤±è¶¨æ–¼ç©©å®š
                overfitting_ratio < 2.0       # æ²’æœ‰åš´é‡éæ“¬åˆ
            )
            
            return {
                'converged': converged,
                'train_trend': float(train_trend),
                'val_trend': float(val_trend),
                'overfitting_ratio': float(overfitting_ratio),
                'final_train_loss': float(final_train_loss),
                'final_val_loss': float(final_val_loss),
                'epochs_trained': len(train_loss)
            }
            
        except Exception as e:
            logger.error(f"âŒ æª¢æŸ¥æ¨¡å‹æ”¶æ–‚æ€§å¤±æ•—: {e}")
            return {'converged': False, 'error': str(e)}
    
    def _calculate_overall_score(self) -> float:
        """è¨ˆç®—æ¨¡å‹æ•´é«”è©•åˆ†"""
        try:
            if not self.validation_metrics:
                return 0.0
            
            # æ¬Šé‡é…ç½®
            weights = {
                'r2_score': 0.3,
                'direction_accuracy': 0.25,
                'stability_score': 0.2,
                'convergence': 0.15,
                'mae_normalized': 0.1
            }
            
            score = 0.0
            
            # RÂ²åˆ†æ•¸ (0-1, è¶Šé«˜è¶Šå¥½)
            r2 = self.validation_metrics.get('r2_score', 0)
            score += max(0, r2) * weights['r2_score']
            
            # æ–¹å‘æº–ç¢ºç‡ (0-1, è¶Šé«˜è¶Šå¥½)
            direction_acc = self.validation_metrics.get('direction_accuracy', 0)
            score += direction_acc * weights['direction_accuracy']
            
            # ç©©å®šæ€§åˆ†æ•¸ (0-1, è¶Šé«˜è¶Šå¥½)
            stability = self.validation_metrics.get('stability_score', 0)
            score += stability * weights['stability_score']
            
            # æ”¶æ–‚æ€§ (å¸ƒçˆ¾å€¼è½‰æ›ç‚ºåˆ†æ•¸)
            convergence = self.validation_metrics.get('convergence_check', {})
            convergence_score = 1.0 if convergence.get('converged', False) else 0.0
            score += convergence_score * weights['convergence']
            
            # æ¨™æº–åŒ–MAE (è¶Šå°è¶Šå¥½ï¼Œéœ€è¦åè½‰)
            mae = self.validation_metrics.get('mae', float('inf'))
            mae_normalized = max(0, 1.0 - mae / 10000)  # å‡è¨­10000ç‚ºæœ€å¤§å¯æ¥å—MAE
            score += mae_normalized * weights['mae_normalized']
            
            return min(1.0, max(0.0, score))
            
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—æ•´é«”è©•åˆ†å¤±æ•—: {e}")
            return 0.0
    
    def _identify_model_strengths(self) -> List[str]:
        """è­˜åˆ¥æ¨¡å‹å„ªå‹¢"""
        try:
            strengths = []
            
            if not self.validation_metrics:
                return strengths
            
            # æª¢æŸ¥å„é …æŒ‡æ¨™
            r2 = self.validation_metrics.get('r2_score', 0)
            if r2 > 0.7:
                strengths.append(f"å„ªç§€çš„æ“¬åˆåº¦ (RÂ² = {r2:.3f})")
            elif r2 > 0.5:
                strengths.append(f"è‰¯å¥½çš„æ“¬åˆåº¦ (RÂ² = {r2:.3f})")
            
            direction_acc = self.validation_metrics.get('direction_accuracy', 0)
            if direction_acc > 0.6:
                strengths.append(f"è‰¯å¥½çš„æ–¹å‘é æ¸¬èƒ½åŠ› ({direction_acc:.1%})")
            
            stability = self.validation_metrics.get('stability_score', 0)
            if stability > 0.8:
                strengths.append(f"é æ¸¬çµæœç©©å®š (ç©©å®šæ€§: {stability:.3f})")
            
            convergence = self.validation_metrics.get('convergence_check', {})
            if convergence.get('converged', False):
                strengths.append("æ¨¡å‹æ”¶æ–‚è‰¯å¥½")
            
            mae = self.validation_metrics.get('mae', float('inf'))
            if mae < 1000:
                strengths.append(f"é æ¸¬èª¤å·®è¼ƒå° (MAE: {mae:.0f})")
            
            return strengths
            
        except Exception as e:
            logger.error(f"âŒ è­˜åˆ¥æ¨¡å‹å„ªå‹¢å¤±æ•—: {e}")
            return []
    
    def _identify_model_weaknesses(self) -> List[str]:
        """è­˜åˆ¥æ¨¡å‹å¼±é»"""
        try:
            weaknesses = []
            
            if not self.validation_metrics:
                return weaknesses
            
            # æª¢æŸ¥å„é …æŒ‡æ¨™
            r2 = self.validation_metrics.get('r2_score', 0)
            if r2 < 0.3:
                weaknesses.append(f"æ“¬åˆåº¦è¼ƒå·® (RÂ² = {r2:.3f})")
            
            direction_acc = self.validation_metrics.get('direction_accuracy', 0)
            if direction_acc < 0.5:
                weaknesses.append(f"æ–¹å‘é æ¸¬èƒ½åŠ›ä¸è¶³ ({direction_acc:.1%})")
            
            stability = self.validation_metrics.get('stability_score', 0)
            if stability < 0.5:
                weaknesses.append(f"é æ¸¬çµæœä¸ç©©å®š (ç©©å®šæ€§: {stability:.3f})")
            
            convergence = self.validation_metrics.get('convergence_check', {})
            if not convergence.get('converged', False):
                weaknesses.append("æ¨¡å‹æ”¶æ–‚æ€§å•é¡Œ")
            
            overfitting_ratio = convergence.get('overfitting_ratio', 1.0)
            if overfitting_ratio > 2.0:
                weaknesses.append(f"å¯èƒ½å­˜åœ¨éæ“¬åˆ (æ¯”ç‡: {overfitting_ratio:.2f})")
            
            mae = self.validation_metrics.get('mae', 0)
            if mae > 5000:
                weaknesses.append(f"é æ¸¬èª¤å·®è¼ƒå¤§ (MAE: {mae:.0f})")
            
            return weaknesses
            
        except Exception as e:
            logger.error(f"âŒ è­˜åˆ¥æ¨¡å‹å¼±é»å¤±æ•—: {e}")
            return []
    
    def _generate_improvement_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        try:
            recommendations = []
            
            if not self.validation_metrics:
                return recommendations
            
            # åŸºæ–¼å¼±é»ç”Ÿæˆå»ºè­°
            weaknesses = self._identify_model_weaknesses()
            
            if any("æ“¬åˆåº¦è¼ƒå·®" in w for w in weaknesses):
                recommendations.append("è€ƒæ…®å¢åŠ LSTMå±¤æ•¸æˆ–å–®å…ƒæ•¸")
                recommendations.append("å˜—è©¦èª¿æ•´åºåˆ—é•·åº¦åƒæ•¸")
                recommendations.append("å¢åŠ æ›´å¤šç›¸é—œç‰¹å¾µ")
            
            if any("æ–¹å‘é æ¸¬èƒ½åŠ›ä¸è¶³" in w for w in weaknesses):
                recommendations.append("è€ƒæ…®ä½¿ç”¨åˆ†é¡æ¨¡å‹è¼”åŠ©æ–¹å‘é æ¸¬")
                recommendations.append("å¢åŠ æŠ€è¡“æŒ‡æ¨™ç‰¹å¾µ")
            
            if any("ä¸ç©©å®š" in w for w in weaknesses):
                recommendations.append("å¢åŠ Dropoutå±¤æˆ–èª¿æ•´Dropoutç‡")
                recommendations.append("ä½¿ç”¨BatchNormalization")
                recommendations.append("æ¸›å°‘å­¸ç¿’ç‡")
            
            if any("æ”¶æ–‚æ€§å•é¡Œ" in w for w in weaknesses):
                recommendations.append("èª¿æ•´å­¸ç¿’ç‡å’Œå„ªåŒ–å™¨åƒæ•¸")
                recommendations.append("å¢åŠ è¨“ç·´epochs")
                recommendations.append("ä½¿ç”¨å­¸ç¿’ç‡èª¿åº¦å™¨")
            
            if any("éæ“¬åˆ" in w for w in weaknesses):
                recommendations.append("å¢åŠ æ­£å‰‡åŒ–")
                recommendations.append("ä½¿ç”¨æ›´å¤šè¨“ç·´æ•¸æ“š")
                recommendations.append("æ¸›å°‘æ¨¡å‹è¤‡é›œåº¦")
            
            if any("èª¤å·®è¼ƒå¤§" in w for w in weaknesses):
                recommendations.append("æª¢æŸ¥æ•¸æ“šè³ªé‡å’Œé è™•ç†")
                recommendations.append("å˜—è©¦ä¸åŒçš„ç‰¹å¾µå·¥ç¨‹æ–¹æ³•")
                recommendations.append("è€ƒæ…®é›†æˆå¤šå€‹æ¨¡å‹")
            
            # é€šç”¨å»ºè­°
            if not recommendations:
                recommendations.append("æ¨¡å‹è¡¨ç¾è‰¯å¥½ï¼Œå¯è€ƒæ…®å¾®èª¿è¶…åƒæ•¸é€²ä¸€æ­¥å„ªåŒ–")
            
            return recommendations
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ”¹é€²å»ºè­°å¤±æ•—: {e}")
            return []
    
    def get_validation_report(self) -> Dict[str, Any]:
        """ç²å–å®Œæ•´çš„é©—è­‰å ±å‘Š"""
        try:
            if not self.validation_metrics:
                return {'error': 'No validation metrics available'}
            
            overall_score = self._calculate_overall_score()
            strengths = self._identify_model_strengths()
            weaknesses = self._identify_model_weaknesses()
            recommendations = self._generate_improvement_recommendations()
            
            return {
                'model_type': 'LSTM',
                'validation_timestamp': self.validation_metrics.get('validation_timestamp'),
                'overall_score': overall_score,
                'performance_grade': self._get_performance_grade(overall_score),
                'detailed_metrics': self.validation_metrics,
                'analysis': {
                    'strengths': strengths,
                    'weaknesses': weaknesses,
                    'recommendations': recommendations
                },
                'model_config': self.config.copy()
            }
            
        except Exception as e:
            logger.error(f"âŒ ç²å–é©—è­‰å ±å‘Šå¤±æ•—: {e}")
            return {'error': str(e)}
    
    def _get_performance_grade(self, score: float) -> str:
        """æ ¹æ“šåˆ†æ•¸ç²å–æ€§èƒ½ç­‰ç´š"""
        if score >= 0.9:
            return "å„ªç§€ (A)"
        elif score >= 0.8:
            return "è‰¯å¥½ (B)"
        elif score >= 0.7:
            return "ä¸­ç­‰ (C)"
        elif score >= 0.6:
            return "åŠæ ¼ (D)"
        else:
            return "éœ€è¦æ”¹é€² (F)"
    
    def save_model(self, market: str) -> bool:
        """ä¿å­˜æ¨¡å‹"""
        try:
            if not TENSORFLOW_AVAILABLE or self.model is None:
                logger.error("âŒ æ¨¡å‹æœªæº–å‚™å¥½ï¼Œç„¡æ³•ä¿å­˜")
                return False
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜Kerasæ¨¡å‹
            model_file = self.model_dir / f"{market}_lstm_model_{timestamp}.h5"
            self.model.save(model_file)
            
            # ä¿å­˜æ¨™æº–åŒ–å™¨
            scaler_file = self.model_dir / f"{market}_lstm_scaler_{timestamp}.pkl"
            with open(scaler_file, 'wb') as f:
                pickle.dump(self.scaler, f)
            
            # ä¿å­˜é…ç½®å’Œæ­·å²
            config_file = self.model_dir / f"{market}_lstm_config_{timestamp}.json"
            config_data = {
                'config': self.config,
                'training_history': self.training_history,
                'validation_metrics': self.validation_metrics,
                'timestamp': timestamp
            }
            
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"ğŸ’¾ LSTMæ¨¡å‹å·²ä¿å­˜: {model_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜LSTMæ¨¡å‹å¤±æ•—: {e}")
            return False
    
    def load_model(self, market: str) -> bool:
        """è¼‰å…¥æ¨¡å‹"""
        try:
            if not TENSORFLOW_AVAILABLE:
                logger.error("âŒ TensorFlowæœªå®‰è£ï¼Œç„¡æ³•è¼‰å…¥æ¨¡å‹")
                return False
            
            # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
            model_files = list(self.model_dir.glob(f"{market}_lstm_model_*.h5"))
            if not model_files:
                logger.warning(f"âš ï¸ æ²’æœ‰æ‰¾åˆ° {market} çš„LSTMæ¨¡å‹æ–‡ä»¶")
                return False
            
            latest_model_file = max(model_files, key=lambda x: x.stat().st_mtime)
            timestamp = latest_model_file.stem.split('_')[-1]
            
            # è¼‰å…¥æ¨¡å‹
            self.model = load_model(latest_model_file)
            
            # è¼‰å…¥æ¨™æº–åŒ–å™¨
            scaler_file = self.model_dir / f"{market}_lstm_scaler_{timestamp}.pkl"
            if scaler_file.exists():
                with open(scaler_file, 'rb') as f:
                    self.scaler = pickle.load(f)
            
            # è¼‰å…¥é…ç½®
            config_file = self.model_dir / f"{market}_lstm_config_{timestamp}.json"
            if config_file.exists():
                with open(config_file, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                    self.config.update(config_data.get('config', {}))
                    self.training_history = config_data.get('training_history')
                    self.validation_metrics = config_data.get('validation_metrics', {})
            
            logger.info(f"âœ… LSTMæ¨¡å‹è¼‰å…¥æˆåŠŸ: {latest_model_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥LSTMæ¨¡å‹å¤±æ•—: {e}")
            return False


def create_lstm_predictor(model_dir: str = "AImax/models") -> LSTMPricePredictor:
    """å‰µå»ºLSTMåƒ¹æ ¼é æ¸¬å™¨å¯¦ä¾‹"""
    return LSTMPricePredictor(model_dir)


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    import asyncio
    
    async def test_lstm_predictor():
        """æ¸¬è©¦LSTMåƒ¹æ ¼é æ¸¬å™¨"""
        print("ğŸ§ª æ¸¬è©¦LSTMåƒ¹æ ¼é æ¸¬å™¨...")
        
        if not TENSORFLOW_AVAILABLE:
            print("âŒ TensorFlowæœªå®‰è£ï¼Œç„¡æ³•æ¸¬è©¦")
            return
        
        predictor = create_lstm_predictor()
        
        try:
            # ç”Ÿæˆæ¨¡æ“¬æ•¸æ“šé€²è¡Œæ¸¬è©¦
            dates = pd.date_range(start='2024-01-01', periods=1000, freq='5T')
            np.random.seed(42)
            
            # æ¨¡æ“¬åƒ¹æ ¼æ•¸æ“š
            base_price = 1500000
            price_changes = np.random.normal(0, 0.001, 1000)
            prices = [base_price]
            
            for change in price_changes[1:]:
                new_price = prices[-1] * (1 + change)
                prices.append(new_price)
            
            # å‰µå»ºDataFrame
            df = pd.DataFrame({
                'timestamp': dates,
                'close': prices,
                'high': [p * 1.001 for p in prices],
                'low': [p * 0.999 for p in prices],
                'volume': np.random.uniform(100, 1000, 1000)
            })
            
            print(f"âœ… ç”Ÿæˆæ¸¬è©¦æ•¸æ“š: {len(df)} æ¢è¨˜éŒ„")
            
            # æº–å‚™æ•¸æ“š
            X_train, X_test, y_train, y_test = predictor.prepare_data(df)
            
            if X_train is not None:
                print(f"âœ… æ•¸æ“šæº–å‚™æˆåŠŸ")
                
                # æ§‹å»ºæ¨¡å‹
                if predictor.build_model():
                    print("âœ… æ¨¡å‹æ§‹å»ºæˆåŠŸ")
                    
                    # è¨“ç·´æ¨¡å‹ï¼ˆä½¿ç”¨è¼ƒå°‘çš„epochsé€²è¡Œæ¸¬è©¦ï¼‰
                    predictor.config['epochs'] = 5
                    if predictor.train(X_train, y_train, X_test, y_test):
                        print("âœ… æ¨¡å‹è¨“ç·´æˆåŠŸ")
                        
                        # æ¸¬è©¦é æ¸¬
                        predictions = predictor.predict(X_test[:5])
                        if predictions is not None:
                            print(f"âœ… é æ¸¬æ¸¬è©¦æˆåŠŸ: {predictions.flatten()[:3]}")
                        
                        # æ¸¬è©¦å–®æ¬¡é æ¸¬
                        test_sequence = X_test[0]
                        single_pred = predictor.predict_single(test_sequence)
                        if single_pred:
                            print(f"âœ… å–®æ¬¡é æ¸¬æˆåŠŸ: {single_pred}")
                        
                        # æ¸¬è©¦ä¿å­˜æ¨¡å‹
                        if predictor.save_model("test"):
                            print("âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ")
                
            else:
                print("âŒ æ•¸æ“šæº–å‚™å¤±æ•—")
                
        except Exception as e:
            print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
    
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(test_lstm_predictor())