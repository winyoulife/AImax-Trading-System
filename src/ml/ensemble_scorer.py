#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†æˆè©•åˆ†å™¨ - ç‚ºAImaxç³»çµ±å„ªåŒ–çš„å¤šæ¨¡å‹é›†æˆ
"""

from typing import Dict, Any, List, Tuple
import logging
import numpy as np
import pandas as pd
from datetime import datetime
import json

logger = logging.getLogger(__name__)

class EnsembleScorer:
    """å¤šæ¨¡å‹åŠ æ¬Šè¨ˆåˆ†å™¨ - å°ˆç‚ºAImaxç³»çµ±å„ªåŒ–"""
    
    def __init__(self):
        """åˆå§‹åŒ–é›†æˆè©•åˆ†å™¨"""
        # æ¬Šé‡é…ç½® - å¯æ ¹æ“šAImaxç³»çµ±éœ€æ±‚èª¿æ•´
        self.weights = {
            'technical_analysis': 0.4,
            'market_sentiment': 0.35,
            'volatility_analysis': 0.25,
        }
        
        logger.info("ğŸ¯ AImaxé›†æˆè©•åˆ†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†æå¸‚å ´æ•¸æ“šä¸¦æä¾›é›†æˆè©•åˆ†
        
        Args:
            market_data: å¸‚å ´æ•¸æ“šå­—å…¸
            
        Returns:
            Dict[str, Any]: é›†æˆåˆ†æçµæœ
        """
        try:
            results = {}
            total_score = 0.0
            total_weight = 0.0
            
            # æŠ€è¡“åˆ†æè©•åˆ†
            tech_result = self._analyze_technical_indicators(market_data)
            if tech_result['success']:
                score = tech_result['score']
                weight = self.weights['technical_analysis']
                results['technical_analysis'] = tech_result
                total_score += score * weight
                total_weight += weight
            
            # å¸‚å ´æƒ…ç·’åˆ†æ
            sentiment_result = self._analyze_market_sentiment(market_data)
            if sentiment_result['success']:
                score = sentiment_result['score']
                weight = self.weights['market_sentiment']
                results['market_sentiment'] = sentiment_result
                total_score += score * weight
                total_weight += weight
            
            # æ³¢å‹•ç‡åˆ†æ
            volatility_result = self._analyze_volatility(market_data)
            if volatility_result['success']:
                score = volatility_result['score']
                weight = self.weights['volatility_analysis']
                results['volatility_analysis'] = volatility_result
                total_score += score * weight
                total_weight += weight
            
            # è¨ˆç®—æœ€çµ‚é›†æˆåˆ†æ•¸
            final_score = total_score / total_weight if total_weight > 0 else 50
            
            # ç”Ÿæˆäº¤æ˜“ä¿¡è™Ÿ
            signal = self._generate_trading_signal(final_score, results)
            
            return {
                'success': True,
                'final_score': final_score,
                'signal': signal,
                'individual_results': results,
                'weights_used': total_weight,
                'confidence': self._calculate_confidence(results),
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆåˆ†æå¤±æ•—: {e}")
            return {
                'success': False,
                'error': str(e),
                'final_score': 50,
                'signal': 'HOLD',
                'timestamp': datetime.now()
            }
    
    def _analyze_technical_indicators(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææŠ€è¡“æŒ‡æ¨™"""
        try:
            score = 50  # åŸºæº–åˆ†æ•¸
            factors = []
            
            # åƒ¹æ ¼è®ŠåŒ–åˆ†æ
            price_change = market_data.get('price_change_1m', 0)
            if price_change > 1:
                score += 10
                factors.append(f"åƒ¹æ ¼ä¸Šæ¼² {price_change}%")
            elif price_change < -1:
                score -= 10
                factors.append(f"åƒ¹æ ¼ä¸‹è·Œ {price_change}%")
            
            # æˆäº¤é‡åˆ†æ
            volume_ratio = market_data.get('volume_ratio', 1.0)
            if volume_ratio > 1.5:
                score += 8
                factors.append(f"æˆäº¤é‡æ”¾å¤§ {volume_ratio:.1f}å€")
            elif volume_ratio < 0.7:
                score -= 5
                factors.append(f"æˆäº¤é‡èç¸® {volume_ratio:.1f}å€")
            
            # ç¢ºä¿åˆ†æ•¸åœ¨åˆç†ç¯„åœå…§
            score = max(20, min(80, score))
            
            return {
                'success': True,
                'score': score,
                'factors': factors,
                'analysis': f"æŠ€è¡“æŒ‡æ¨™ç¶œåˆè©•åˆ†: {score}"
            }
            
        except Exception as e:
            logger.error(f"âŒ æŠ€è¡“æŒ‡æ¨™åˆ†æå¤±æ•—: {e}")
            return {
                'success': False,
                'error': str(e),
                'score': 50
            }
    
    def _analyze_market_sentiment(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå¸‚å ´æƒ…ç·’"""
        try:
            score = 50  # åŸºæº–åˆ†æ•¸
            factors = []
            
            # åŸºæ–¼åƒ¹æ ¼è¶¨å‹¢åˆ¤æ–·æƒ…ç·’
            current_price = market_data.get('current_price', 0)
            if current_price > 1600000:  # é«˜åƒ¹ä½
                score += 5
                factors.append("åƒ¹æ ¼è™•æ–¼é«˜ä½ï¼Œå¸‚å ´æ¨‚è§€")
            elif current_price < 1400000:  # ä½åƒ¹ä½
                score -= 5
                factors.append("åƒ¹æ ¼è™•æ–¼ä½ä½ï¼Œå¸‚å ´æ‚²è§€")
            
            # AIæ ¼å¼åŒ–æ•¸æ“šä¸­çš„æƒ…ç·’æŒ‡æ¨™
            ai_data = market_data.get('ai_formatted_data', '')
            if 'ä¸Šæ¼²' in ai_data or 'çœ‹æ¼²' in ai_data:
                score += 8
                factors.append("AIæ•¸æ“šé¡¯ç¤ºçœ‹æ¼²æƒ…ç·’")
            elif 'ä¸‹è·Œ' in ai_data or 'çœ‹è·Œ' in ai_data:
                score -= 8
                factors.append("AIæ•¸æ“šé¡¯ç¤ºçœ‹è·Œæƒ…ç·’")
            
            # ç¢ºä¿åˆ†æ•¸åœ¨åˆç†ç¯„åœå…§
            score = max(25, min(75, score))
            
            return {
                'success': True,
                'score': score,
                'factors': factors,
                'analysis': f"å¸‚å ´æƒ…ç·’è©•åˆ†: {score}"
            }
            
        except Exception as e:
            logger.error(f"âŒ å¸‚å ´æƒ…ç·’åˆ†æå¤±æ•—: {e}")
            return {
                'success': False,
                'error': str(e),
                'score': 50
            }
    
    def _analyze_volatility(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ³¢å‹•ç‡"""
        try:
            score = 50  # åŸºæº–åˆ†æ•¸
            factors = []
            
            # åŸºæ–¼åƒ¹æ ¼è®ŠåŒ–åˆ¤æ–·æ³¢å‹•ç‡
            price_change = abs(market_data.get('price_change_1m', 0))
            
            if price_change > 2:
                score -= 10  # é«˜æ³¢å‹•ç‡é™ä½åˆ†æ•¸
                factors.append(f"é«˜æ³¢å‹•ç‡ {price_change}%ï¼Œé¢¨éšªå¢åŠ ")
            elif price_change < 0.5:
                score += 5  # ä½æ³¢å‹•ç‡ç•¥å¾®åŠ åˆ†
                factors.append(f"ä½æ³¢å‹•ç‡ {price_change}%ï¼Œå¸‚å ´ç©©å®š")
            else:
                factors.append(f"æ­£å¸¸æ³¢å‹•ç‡ {price_change}%")
            
            # æˆäº¤é‡æ³¢å‹•åˆ†æ
            volume_ratio = market_data.get('volume_ratio', 1.0)
            if volume_ratio > 2:
                score -= 5  # æˆäº¤é‡åŠ‡çƒˆè®ŠåŒ–
                factors.append("æˆäº¤é‡åŠ‡çƒˆè®ŠåŒ–ï¼Œå¸‚å ´ä¸ç©©å®š")
            
            # ç¢ºä¿åˆ†æ•¸åœ¨åˆç†ç¯„åœå…§
            score = max(30, min(70, score))
            
            return {
                'success': True,
                'score': score,
                'factors': factors,
                'analysis': f"æ³¢å‹•ç‡åˆ†æè©•åˆ†: {score}"
            }
            
        except Exception as e:
            logger.error(f"âŒ æ³¢å‹•ç‡åˆ†æå¤±æ•—: {e}")
            return {
                'success': False,
                'error': str(e),
                'score': 50
            }
    
    def _generate_trading_signal(self, final_score: float, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆäº¤æ˜“ä¿¡è™Ÿ"""
        try:
            # åŸºæ–¼æœ€çµ‚åˆ†æ•¸ç”Ÿæˆä¿¡è™Ÿ
            if final_score > 60:
                signal = "BUY"
            elif final_score < 40:
                signal = "SELL"
            else:
                signal = "HOLD"
            
            # æª¢æŸ¥ä¸€è‡´æ€§ - å¦‚æœå„æ¨¡å‹åˆ†æ­§è¼ƒå¤§ï¼Œå‚¾å‘æ–¼HOLD
            scores = []
            for result in results.values():
                if result.get('success') and 'score' in result:
                    scores.append(result['score'])
            
            if len(scores) > 1:
                score_std = np.std(scores)
                if score_std > 15:  # åˆ†æ­§è¼ƒå¤§
                    signal = "HOLD"
                    logger.info(f"âš ï¸ æ¨¡å‹åˆ†æ­§è¼ƒå¤§ (std={score_std:.1f})ï¼Œèª¿æ•´ç‚ºHOLD")
            
            return signal
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆäº¤æ˜“ä¿¡è™Ÿå¤±æ•—: {e}")
            return "HOLD"
    
    def _calculate_confidence(self, results: Dict[str, Any]) -> float:
        """è¨ˆç®—æ•´é«”ä¿¡å¿ƒåº¦"""
        try:
            successful_analyses = sum(1 for r in results.values() if r.get('success', False))
            total_analyses = len(results)
            
            if total_analyses == 0:
                return 0.0
            
            # åŸºç¤ä¿¡å¿ƒåº¦åŸºæ–¼æˆåŠŸåˆ†æçš„æ¯”ä¾‹
            base_confidence = (successful_analyses / total_analyses) * 100
            
            # æ ¹æ“šåˆ†æ•¸ä¸€è‡´æ€§èª¿æ•´ä¿¡å¿ƒåº¦
            scores = []
            for result in results.values():
                if result.get('success') and 'score' in result:
                    scores.append(result['score'])
            
            if len(scores) > 1:
                score_std = np.std(scores)
                consistency_factor = max(0.5, 1 - score_std / 50)  # æ¨™æº–å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜
                base_confidence *= consistency_factor
            
            return min(100, max(0, base_confidence))
            
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—ä¿¡å¿ƒåº¦å¤±æ•—: {e}")
            return 50.0
    
    def update_weights(self, new_weights: Dict[str, float]) -> bool:
        """æ›´æ–°æ¨¡å‹æ¬Šé‡"""
        try:
            # é©—è­‰æ¬Šé‡
            if not isinstance(new_weights, dict):
                return False
            
            # æª¢æŸ¥æ¬Šé‡å’Œæ˜¯å¦æ¥è¿‘1
            weight_sum = sum(new_weights.values())
            if abs(weight_sum - 1.0) > 0.1:
                logger.warning(f"âš ï¸ æ¬Šé‡å’Œä¸ç­‰æ–¼1: {weight_sum}")
                return False
            
            # æ›´æ–°æ¬Šé‡
            old_weights = self.weights.copy()
            self.weights.update(new_weights)
            
            logger.info(f"âœ… æ¬Šé‡æ›´æ–°æˆåŠŸ")
            logger.info(f"èˆŠæ¬Šé‡: {old_weights}")
            logger.info(f"æ–°æ¬Šé‡: {self.weights}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ¬Šé‡å¤±æ•—: {e}")
            return False
    
    def get_model_info(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'model_type': 'ensemble_scorer',
            'weights': self.weights.copy(),
            'components': ['technical_analysis', 'market_sentiment', 'volatility_analysis'],
            'optimized_for': 'AImax_trading_system',
            'version': '1.0'
        }
    
    # ==================== é›†æˆè©•åˆ†å™¨é©—è­‰åŠŸèƒ½ ====================
    
    def validate_ensemble(self, test_data: List[Dict[str, Any]], 
                         ground_truth: List[str] = None) -> Dict[str, Any]:
        """
        é©—è­‰é›†æˆè©•åˆ†å™¨æ€§èƒ½
        
        Args:
            test_data: æ¸¬è©¦æ•¸æ“šåˆ—è¡¨
            ground_truth: çœŸå¯¦æ¨™ç±¤åˆ—è¡¨ (å¯é¸)
            
        Returns:
            é©—è­‰çµæœå­—å…¸
        """
        try:
            logger.info("ğŸ” é–‹å§‹é›†æˆè©•åˆ†å™¨é©—è­‰...")
            
            if not test_data:
                return {'success': False, 'error': 'No test data provided'}
            
            # åŸ·è¡Œæ‰¹é‡åˆ†æ
            predictions = []
            individual_scores = {'technical_analysis': [], 'market_sentiment': [], 'volatility_analysis': []}
            final_scores = []
            signals = []
            confidences = []
            
            for data in test_data:
                result = self.analyze(data)
                if result['success']:
                    predictions.append(result)
                    final_scores.append(result['final_score'])
                    signals.append(result['signal'])
                    confidences.append(result['confidence'])
                    
                    # æ”¶é›†å€‹åˆ¥æ¨¡å‹åˆ†æ•¸
                    for component, scores_list in individual_scores.items():
                        if component in result['individual_results']:
                            scores_list.append(result['individual_results'][component]['score'])
                        else:
                            scores_list.append(50)  # é»˜èªåˆ†æ•¸
            
            if not predictions:
                return {'success': False, 'error': 'No successful predictions'}
            
            # è¨ˆç®—é©—è­‰æŒ‡æ¨™
            validation_metrics = {
                'prediction_count': len(predictions),
                'success_rate': len(predictions) / len(test_data),
                'score_statistics': self._calculate_score_statistics(final_scores),
                'signal_distribution': self._calculate_signal_distribution(signals),
                'confidence_analysis': self._analyze_confidence_levels(confidences),
                'component_consistency': self._analyze_component_consistency(individual_scores),
                'weight_effectiveness': self._evaluate_weight_effectiveness(individual_scores, final_scores),
                'model_agreement': self._calculate_model_agreement(individual_scores),
                'prediction_stability': self._assess_prediction_stability(predictions)
            }
            
            # å¦‚æœæœ‰çœŸå¯¦æ¨™ç±¤ï¼Œè¨ˆç®—æº–ç¢ºæ€§æŒ‡æ¨™
            if ground_truth and len(ground_truth) == len(signals):
                accuracy_metrics = self._calculate_accuracy_metrics(signals, ground_truth)
                validation_metrics['accuracy_metrics'] = accuracy_metrics
            
            # æ¬Šé‡å„ªåŒ–å»ºè­°
            weight_optimization = self._optimize_weights(individual_scores, final_scores, ground_truth, signals)
            validation_metrics['weight_optimization'] = weight_optimization
            
            # è¡çªè§£æ±ºæ©Ÿåˆ¶è©•ä¼°
            conflict_resolution = self._evaluate_conflict_resolution(individual_scores, signals)
            validation_metrics['conflict_resolution'] = conflict_resolution
            
            # å¯é æ€§è©•ä¼°
            reliability_assessment = self._assess_ensemble_reliability(validation_metrics)
            validation_metrics['reliability_assessment'] = reliability_assessment
            
            logger.info("âœ… é›†æˆè©•åˆ†å™¨é©—è­‰å®Œæˆ")
            logger.info(f"   æˆåŠŸç‡: {validation_metrics['success_rate']:.2%}")
            logger.info(f"   å¹³å‡ä¿¡å¿ƒåº¦: {np.mean(confidences):.1f}%")
            logger.info(f"   æ¨¡å‹ä¸€è‡´æ€§: {validation_metrics['model_agreement']:.3f}")
            
            return {
                'success': True,
                'validation_metrics': validation_metrics,
                'summary': {
                    'overall_score': self._calculate_ensemble_overall_score(validation_metrics),
                    'strengths': self._identify_ensemble_strengths(validation_metrics),
                    'weaknesses': self._identify_ensemble_weaknesses(validation_metrics),
                    'recommendations': self._generate_ensemble_recommendations(validation_metrics)
                },
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆè©•åˆ†å™¨é©—è­‰å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}
    
    def _calculate_score_statistics(self, scores: List[float]) -> Dict[str, float]:
        """è¨ˆç®—åˆ†æ•¸çµ±è¨ˆä¿¡æ¯"""
        try:
            if not scores:
                return {}
            
            scores_array = np.array(scores)
            return {
                'mean': float(np.mean(scores_array)),
                'std': float(np.std(scores_array)),
                'min': float(np.min(scores_array)),
                'max': float(np.max(scores_array)),
                'median': float(np.median(scores_array)),
                'q25': float(np.percentile(scores_array, 25)),
                'q75': float(np.percentile(scores_array, 75))
            }
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—åˆ†æ•¸çµ±è¨ˆå¤±æ•—: {e}")
            return {}
    
    def _calculate_signal_distribution(self, signals: List[str]) -> Dict[str, Any]:
        """è¨ˆç®—ä¿¡è™Ÿåˆ†ä½ˆ"""
        try:
            if not signals:
                return {}
            
            signal_counts = {}
            for signal in signals:
                signal_counts[signal] = signal_counts.get(signal, 0) + 1
            
            total = len(signals)
            signal_percentages = {k: v/total for k, v in signal_counts.items()}
            
            return {
                'counts': signal_counts,
                'percentages': signal_percentages,
                'total': total,
                'diversity': len(signal_counts)  # ä¿¡è™Ÿå¤šæ¨£æ€§
            }
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—ä¿¡è™Ÿåˆ†ä½ˆå¤±æ•—: {e}")
            return {}
    
    def _analyze_confidence_levels(self, confidences: List[float]) -> Dict[str, Any]:
        """åˆ†æä¿¡å¿ƒåº¦æ°´å¹³"""
        try:
            if not confidences:
                return {}
            
            conf_array = np.array(confidences)
            
            # ä¿¡å¿ƒåº¦åˆ†ç´š
            high_confidence = np.sum(conf_array >= 80)
            medium_confidence = np.sum((conf_array >= 60) & (conf_array < 80))
            low_confidence = np.sum(conf_array < 60)
            
            return {
                'statistics': self._calculate_score_statistics(confidences),
                'distribution': {
                    'high_confidence': high_confidence,
                    'medium_confidence': medium_confidence,
                    'low_confidence': low_confidence
                },
                'percentages': {
                    'high_confidence': high_confidence / len(confidences),
                    'medium_confidence': medium_confidence / len(confidences),
                    'low_confidence': low_confidence / len(confidences)
                }
            }
        except Exception as e:
            logger.error(f"âŒ åˆ†æä¿¡å¿ƒåº¦æ°´å¹³å¤±æ•—: {e}")
            return {}
    
    def _analyze_component_consistency(self, individual_scores: Dict[str, List[float]]) -> Dict[str, Any]:
        """åˆ†æçµ„ä»¶ä¸€è‡´æ€§"""
        try:
            consistency_metrics = {}
            
            for component, scores in individual_scores.items():
                if scores:
                    scores_array = np.array(scores)
                    consistency_metrics[component] = {
                        'mean': float(np.mean(scores_array)),
                        'std': float(np.std(scores_array)),
                        'consistency_score': 1.0 / (1.0 + np.std(scores_array) / 50.0)  # æ¨™æº–åŒ–ä¸€è‡´æ€§åˆ†æ•¸
                    }
            
            # è¨ˆç®—çµ„ä»¶é–“ç›¸é—œæ€§
            correlations = {}
            components = list(individual_scores.keys())
            for i, comp1 in enumerate(components):
                for comp2 in components[i+1:]:
                    if individual_scores[comp1] and individual_scores[comp2]:
                        corr = np.corrcoef(individual_scores[comp1], individual_scores[comp2])[0, 1]
                        correlations[f"{comp1}_vs_{comp2}"] = float(corr) if not np.isnan(corr) else 0.0
            
            return {
                'component_metrics': consistency_metrics,
                'correlations': correlations,
                'overall_consistency': np.mean([m['consistency_score'] for m in consistency_metrics.values()])
            }
        except Exception as e:
            logger.error(f"âŒ åˆ†æçµ„ä»¶ä¸€è‡´æ€§å¤±æ•—: {e}")
            return {}
    
    def _evaluate_weight_effectiveness(self, individual_scores: Dict[str, List[float]], 
                                     final_scores: List[float]) -> Dict[str, Any]:
        """è©•ä¼°æ¬Šé‡æœ‰æ•ˆæ€§"""
        try:
            if not individual_scores or not final_scores:
                return {}
            
            # è¨ˆç®—æ¯å€‹çµ„ä»¶å°æœ€çµ‚åˆ†æ•¸çš„è²¢ç»
            contributions = {}
            for component, scores in individual_scores.items():
                if scores and component in self.weights:
                    weight = self.weights[component]
                    weighted_scores = [s * weight for s in scores]
                    
                    # è¨ˆç®—èˆ‡æœ€çµ‚åˆ†æ•¸çš„ç›¸é—œæ€§
                    if len(weighted_scores) == len(final_scores):
                        correlation = np.corrcoef(weighted_scores, final_scores)[0, 1]
                        contributions[component] = {
                            'weight': weight,
                            'correlation_with_final': float(correlation) if not np.isnan(correlation) else 0.0,
                            'average_contribution': float(np.mean(weighted_scores)),
                            'contribution_variance': float(np.var(weighted_scores))
                        }
            
            # è©•ä¼°æ¬Šé‡å¹³è¡¡æ€§
            weight_balance = self._evaluate_weight_balance()
            
            return {
                'component_contributions': contributions,
                'weight_balance': weight_balance,
                'effectiveness_score': self._calculate_weight_effectiveness_score(contributions)
            }
        except Exception as e:
            logger.error(f"âŒ è©•ä¼°æ¬Šé‡æœ‰æ•ˆæ€§å¤±æ•—: {e}")
            return {}
    
    def _evaluate_weight_balance(self) -> Dict[str, Any]:
        """è©•ä¼°æ¬Šé‡å¹³è¡¡æ€§"""
        try:
            weights_list = list(self.weights.values())
            weights_array = np.array(weights_list)
            
            # è¨ˆç®—æ¬Šé‡åˆ†ä½ˆçš„å‡å‹»æ€§
            entropy = -np.sum(weights_array * np.log(weights_array + 1e-10))
            max_entropy = np.log(len(weights_array))
            normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
            
            return {
                'entropy': float(entropy),
                'normalized_entropy': float(normalized_entropy),
                'weight_variance': float(np.var(weights_array)),
                'balance_score': float(normalized_entropy)  # è¶Šæ¥è¿‘1è¶Šå¹³è¡¡
            }
        except Exception as e:
            logger.error(f"âŒ è©•ä¼°æ¬Šé‡å¹³è¡¡æ€§å¤±æ•—: {e}")
            return {}
    
    def _calculate_weight_effectiveness_score(self, contributions: Dict[str, Any]) -> float:
        """è¨ˆç®—æ¬Šé‡æœ‰æ•ˆæ€§åˆ†æ•¸"""
        try:
            if not contributions:
                return 0.0
            
            # åŸºæ–¼ç›¸é—œæ€§å’Œè²¢ç»åº¦è¨ˆç®—æœ‰æ•ˆæ€§
            correlations = [c['correlation_with_final'] for c in contributions.values()]
            avg_correlation = np.mean([abs(c) for c in correlations])
            
            return float(avg_correlation)
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—æ¬Šé‡æœ‰æ•ˆæ€§åˆ†æ•¸å¤±æ•—: {e}")
            return 0.0
    
    def _calculate_model_agreement(self, individual_scores: Dict[str, List[float]]) -> float:
        """è¨ˆç®—æ¨¡å‹ä¸€è‡´æ€§"""
        try:
            if len(individual_scores) < 2:
                return 1.0
            
            # è¨ˆç®—æ‰€æœ‰çµ„ä»¶åˆ†æ•¸çš„æ¨™æº–å·®
            all_stds = []
            max_len = max(len(scores) for scores in individual_scores.values() if scores)
            
            for i in range(max_len):
                scores_at_i = []
                for scores in individual_scores.values():
                    if i < len(scores):
                        scores_at_i.append(scores[i])
                
                if len(scores_at_i) > 1:
                    std_at_i = np.std(scores_at_i)
                    all_stds.append(std_at_i)
            
            if not all_stds:
                return 1.0
            
            # å°‡æ¨™æº–å·®è½‰æ›ç‚ºä¸€è‡´æ€§åˆ†æ•¸ (æ¨™æº–å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜)
            avg_std = np.mean(all_stds)
            agreement = 1.0 / (1.0 + avg_std / 25.0)  # æ¨™æº–åŒ–åˆ°0-1ç¯„åœ
            
            return float(agreement)
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—æ¨¡å‹ä¸€è‡´æ€§å¤±æ•—: {e}")
            return 0.5
    
    def _assess_prediction_stability(self, predictions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è©•ä¼°é æ¸¬ç©©å®šæ€§"""
        try:
            if len(predictions) < 2:
                return {}
            
            # åˆ†æåˆ†æ•¸è®ŠåŒ–
            scores = [p['final_score'] for p in predictions]
            score_changes = [abs(scores[i] - scores[i-1]) for i in range(1, len(scores))]
            
            # åˆ†æä¿¡è™Ÿè®ŠåŒ–
            signals = [p['signal'] for p in predictions]
            signal_changes = sum(1 for i in range(1, len(signals)) if signals[i] != signals[i-1])
            
            # åˆ†æä¿¡å¿ƒåº¦è®ŠåŒ–
            confidences = [p['confidence'] for p in predictions]
            confidence_changes = [abs(confidences[i] - confidences[i-1]) for i in range(1, len(confidences))]
            
            return {
                'score_stability': {
                    'mean_change': float(np.mean(score_changes)),
                    'max_change': float(np.max(score_changes)),
                    'stability_score': 1.0 / (1.0 + np.mean(score_changes) / 10.0)
                },
                'signal_stability': {
                    'change_count': signal_changes,
                    'change_rate': signal_changes / (len(signals) - 1),
                    'stability_score': 1.0 - (signal_changes / (len(signals) - 1))
                },
                'confidence_stability': {
                    'mean_change': float(np.mean(confidence_changes)),
                    'max_change': float(np.max(confidence_changes)),
                    'stability_score': 1.0 / (1.0 + np.mean(confidence_changes) / 20.0)
                }
            }
        except Exception as e:
            logger.error(f"âŒ è©•ä¼°é æ¸¬ç©©å®šæ€§å¤±æ•—: {e}")
            return {}
    
    def _calculate_accuracy_metrics(self, predictions: List[str], 
                                  ground_truth: List[str]) -> Dict[str, Any]:
        """è¨ˆç®—æº–ç¢ºæ€§æŒ‡æ¨™"""
        try:
            if len(predictions) != len(ground_truth):
                return {}
            
            # è¨ˆç®—æº–ç¢ºç‡
            correct = sum(1 for p, g in zip(predictions, ground_truth) if p == g)
            accuracy = correct / len(predictions)
            
            # è¨ˆç®—å„é¡åˆ¥çš„ç²¾ç¢ºç‡å’Œå¬å›ç‡
            unique_labels = list(set(predictions + ground_truth))
            precision_recall = {}
            
            for label in unique_labels:
                tp = sum(1 for p, g in zip(predictions, ground_truth) if p == label and g == label)
                fp = sum(1 for p, g in zip(predictions, ground_truth) if p == label and g != label)
                fn = sum(1 for p, g in zip(predictions, ground_truth) if p != label and g == label)
                
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
                
                precision_recall[label] = {
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1,
                    'support': sum(1 for g in ground_truth if g == label)
                }
            
            return {
                'accuracy': accuracy,
                'correct_predictions': correct,
                'total_predictions': len(predictions),
                'precision_recall': precision_recall,
                'macro_f1': np.mean([pr['f1_score'] for pr in precision_recall.values()])
            }
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—æº–ç¢ºæ€§æŒ‡æ¨™å¤±æ•—: {e}")
            return {}
    
    def _optimize_weights(self, individual_scores: Dict[str, List[float]], 
                         final_scores: List[float],
                         ground_truth: List[str] = None,
                         signals: List[str] = None) -> Dict[str, Any]:
        """å„ªåŒ–æ¬Šé‡å»ºè­°"""
        try:
            optimization_results = {}
            
            # åŸºæ–¼ç›¸é—œæ€§çš„æ¬Šé‡å„ªåŒ–
            if individual_scores and final_scores:
                correlation_based = self._optimize_weights_by_correlation(individual_scores, final_scores)
                optimization_results['correlation_based'] = correlation_based
            
            # åŸºæ–¼æº–ç¢ºæ€§çš„æ¬Šé‡å„ªåŒ– (å¦‚æœæœ‰çœŸå¯¦æ¨™ç±¤)
            if ground_truth and signals and individual_scores:
                accuracy_based = self._optimize_weights_by_accuracy(individual_scores, ground_truth, signals)
                optimization_results['accuracy_based'] = accuracy_based
            
            # åŸºæ–¼æ–¹å·®çš„æ¬Šé‡å„ªåŒ–
            variance_based = self._optimize_weights_by_variance(individual_scores)
            optimization_results['variance_based'] = variance_based
            
            # ç”Ÿæˆæœ€çµ‚æ¬Šé‡å»ºè­°
            final_recommendation = self._generate_weight_recommendation(optimization_results)
            optimization_results['final_recommendation'] = final_recommendation
            
            return optimization_results
        except Exception as e:
            logger.error(f"âŒ å„ªåŒ–æ¬Šé‡å¤±æ•—: {e}")
            return {}
    
    def _optimize_weights_by_correlation(self, individual_scores: Dict[str, List[float]], 
                                       final_scores: List[float]) -> Dict[str, float]:
        """åŸºæ–¼ç›¸é—œæ€§å„ªåŒ–æ¬Šé‡"""
        try:
            correlations = {}
            for component, scores in individual_scores.items():
                if scores and len(scores) == len(final_scores):
                    corr = np.corrcoef(scores, final_scores)[0, 1]
                    correlations[component] = abs(corr) if not np.isnan(corr) else 0.0
            
            # æ¨™æº–åŒ–ç›¸é—œæ€§ä½œç‚ºæ¬Šé‡
            total_corr = sum(correlations.values())
            if total_corr > 0:
                optimized_weights = {k: v / total_corr for k, v in correlations.items()}
            else:
                optimized_weights = {k: 1.0 / len(correlations) for k in correlations.keys()}
            
            return optimized_weights
        except Exception as e:
            logger.error(f"âŒ åŸºæ–¼ç›¸é—œæ€§å„ªåŒ–æ¬Šé‡å¤±æ•—: {e}")
            return {}
    
    def _optimize_weights_by_accuracy(self, individual_scores: Dict[str, List[float]], 
                                    ground_truth: List[str], signals: List[str]) -> Dict[str, float]:
        """åŸºæ–¼æº–ç¢ºæ€§å„ªåŒ–æ¬Šé‡"""
        try:
            component_accuracies = {}
            
            for component, scores in individual_scores.items():
                if scores and len(scores) == len(ground_truth):
                    # å°‡åˆ†æ•¸è½‰æ›ç‚ºä¿¡è™Ÿ
                    component_signals = ['BUY' if s > 60 else 'SELL' if s < 40 else 'HOLD' for s in scores]
                    
                    # è¨ˆç®—æº–ç¢ºç‡
                    correct = sum(1 for cs, gt in zip(component_signals, ground_truth) if cs == gt)
                    accuracy = correct / len(ground_truth)
                    component_accuracies[component] = accuracy
            
            # åŸºæ–¼æº–ç¢ºç‡åˆ†é…æ¬Šé‡
            total_accuracy = sum(component_accuracies.values())
            if total_accuracy > 0:
                optimized_weights = {k: v / total_accuracy for k, v in component_accuracies.items()}
            else:
                optimized_weights = {k: 1.0 / len(component_accuracies) for k in component_accuracies.keys()}
            
            return optimized_weights
        except Exception as e:
            logger.error(f"âŒ åŸºæ–¼æº–ç¢ºæ€§å„ªåŒ–æ¬Šé‡å¤±æ•—: {e}")
            return {}
    
    def _optimize_weights_by_variance(self, individual_scores: Dict[str, List[float]]) -> Dict[str, float]:
        """åŸºæ–¼æ–¹å·®å„ªåŒ–æ¬Šé‡"""
        try:
            component_variances = {}
            
            for component, scores in individual_scores.items():
                if scores:
                    variance = np.var(scores)
                    # æ–¹å·®è¶Šå°ï¼Œæ¬Šé‡è¶Šé«˜ (æ›´ç©©å®šçš„çµ„ä»¶ç²å¾—æ›´é«˜æ¬Šé‡)
                    component_variances[component] = 1.0 / (1.0 + variance / 100.0)
            
            # æ¨™æº–åŒ–æ¬Šé‡
            total_weight = sum(component_variances.values())
            if total_weight > 0:
                optimized_weights = {k: v / total_weight for k, v in component_variances.items()}
            else:
                optimized_weights = {k: 1.0 / len(component_variances) for k in component_variances.keys()}
            
            return optimized_weights
        except Exception as e:
            logger.error(f"âŒ åŸºæ–¼æ–¹å·®å„ªåŒ–æ¬Šé‡å¤±æ•—: {e}")
            return {}
    
    def _generate_weight_recommendation(self, optimization_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€çµ‚æ¬Šé‡å»ºè­°"""
        try:
            recommendations = {}
            
            # æ”¶é›†æ‰€æœ‰å„ªåŒ–æ–¹æ³•çš„æ¬Šé‡å»ºè­°
            all_methods = []
            for method, weights in optimization_results.items():
                if isinstance(weights, dict) and weights:
                    all_methods.append((method, weights))
            
            if not all_methods:
                return {'recommended_weights': self.weights.copy(), 'confidence': 0.0}
            
            # è¨ˆç®—å¹³å‡æ¬Šé‡
            components = set()
            for _, weights in all_methods:
                components.update(weights.keys())
            
            averaged_weights = {}
            for component in components:
                component_weights = []
                for _, weights in all_methods:
                    if component in weights:
                        component_weights.append(weights[component])
                
                if component_weights:
                    averaged_weights[component] = np.mean(component_weights)
            
            # æ¨™æº–åŒ–æ¬Šé‡
            total_weight = sum(averaged_weights.values())
            if total_weight > 0:
                final_weights = {k: v / total_weight for k, v in averaged_weights.items()}
            else:
                final_weights = self.weights.copy()
            
            # è¨ˆç®—å»ºè­°ä¿¡å¿ƒåº¦
            confidence = self._calculate_recommendation_confidence(optimization_results, final_weights)
            
            return {
                'recommended_weights': final_weights,
                'current_weights': self.weights.copy(),
                'weight_changes': {k: final_weights.get(k, 0) - self.weights.get(k, 0) 
                                 for k in set(list(final_weights.keys()) + list(self.weights.keys()))},
                'confidence': confidence,
                'optimization_methods_used': len(all_methods)
            }
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ¬Šé‡å»ºè­°å¤±æ•—: {e}")
            return {'recommended_weights': self.weights.copy(), 'confidence': 0.0}
    
    def _calculate_recommendation_confidence(self, optimization_results: Dict[str, Any], 
                                          final_weights: Dict[str, float]) -> float:
        """è¨ˆç®—æ¬Šé‡å»ºè­°çš„ä¿¡å¿ƒåº¦"""
        try:
            # åŸºæ–¼ä¸åŒå„ªåŒ–æ–¹æ³•çš„ä¸€è‡´æ€§è¨ˆç®—ä¿¡å¿ƒåº¦
            method_weights = []
            for method, weights in optimization_results.items():
                if isinstance(weights, dict) and weights:
                    method_weights.append(weights)
            
            if len(method_weights) < 2:
                return 0.5
            
            # è¨ˆç®—æ–¹æ³•é–“çš„æ¬Šé‡ä¸€è‡´æ€§
            consistencies = []
            components = final_weights.keys()
            
            for component in components:
                component_weights = [w.get(component, 0) for w in method_weights]
                if len(component_weights) > 1:
                    std = np.std(component_weights)
                    consistency = 1.0 / (1.0 + std * 2)  # æ¨™æº–å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜
                    consistencies.append(consistency)
            
            if consistencies:
                avg_consistency = np.mean(consistencies)
                return float(avg_consistency)
            else:
                return 0.5
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—å»ºè­°ä¿¡å¿ƒåº¦å¤±æ•—: {e}")
            return 0.5
    
    def _evaluate_conflict_resolution(self, individual_scores: Dict[str, List[float]], 
                                    signals: List[str]) -> Dict[str, Any]:
        """è©•ä¼°è¡çªè§£æ±ºæ©Ÿåˆ¶"""
        try:
            if not individual_scores or not signals:
                return {}
            
            conflict_analysis = {
                'total_predictions': len(signals),
                'conflict_cases': 0,
                'resolution_effectiveness': 0.0,
                'conflict_types': {
                    'high_disagreement': 0,
                    'mixed_signals': 0,
                    'low_confidence': 0
                }
            }
            
            # åˆ†ææ¯å€‹é æ¸¬çš„è¡çªæƒ…æ³
            max_len = min(len(signals), min(len(scores) for scores in individual_scores.values() if scores))
            
            for i in range(max_len):
                # ç²å–è©²æ™‚é–“é»çš„æ‰€æœ‰çµ„ä»¶åˆ†æ•¸
                scores_at_i = []
                for component, scores in individual_scores.items():
                    if i < len(scores):
                        scores_at_i.append(scores[i])
                
                if len(scores_at_i) < 2:
                    continue
                
                # æª¢æ¸¬è¡çª
                score_std = np.std(scores_at_i)
                score_range = max(scores_at_i) - min(scores_at_i)
                
                # é«˜åˆ†æ­§è¡çª
                if score_std > 15:
                    conflict_analysis['conflict_cases'] += 1
                    conflict_analysis['conflict_types']['high_disagreement'] += 1
                
                # æ··åˆä¿¡è™Ÿè¡çª (æœ‰äº›çœ‹æ¼²ï¼Œæœ‰äº›çœ‹è·Œ)
                buy_signals = sum(1 for s in scores_at_i if s > 60)
                sell_signals = sum(1 for s in scores_at_i if s < 40)
                if buy_signals > 0 and sell_signals > 0:
                    conflict_analysis['conflict_cases'] += 1
                    conflict_analysis['conflict_types']['mixed_signals'] += 1
                
                # ä½ä¿¡å¿ƒåº¦è¡çª
                if all(40 <= s <= 60 for s in scores_at_i):
                    conflict_analysis['conflict_types']['low_confidence'] += 1
            
            # è¨ˆç®—è§£æ±ºæœ‰æ•ˆæ€§
            if conflict_analysis['conflict_cases'] > 0:
                # æª¢æŸ¥è¡çªæƒ…æ³ä¸‹çš„ä¿¡è™Ÿåˆ†ä½ˆ
                hold_signals = sum(1 for s in signals if s == 'HOLD')
                resolution_rate = hold_signals / len(signals)  # HOLDä¿¡è™Ÿæ¯”ä¾‹ä½œç‚ºè¡çªè§£æ±ºæŒ‡æ¨™
                conflict_analysis['resolution_effectiveness'] = resolution_rate
            
            # è¡çªè§£æ±ºç­–ç•¥è©•ä¼°
            resolution_strategies = self._evaluate_resolution_strategies(individual_scores, signals)
            conflict_analysis['resolution_strategies'] = resolution_strategies
            
            return conflict_analysis
        except Exception as e:
            logger.error(f"âŒ è©•ä¼°è¡çªè§£æ±ºæ©Ÿåˆ¶å¤±æ•—: {e}")
            return {}
    
    def _evaluate_resolution_strategies(self, individual_scores: Dict[str, List[float]], 
                                      signals: List[str]) -> Dict[str, Any]:
        """è©•ä¼°è¡çªè§£æ±ºç­–ç•¥"""
        try:
            strategies = {
                'weighted_average': {'used': 0, 'effective': 0},
                'conservative_hold': {'used': 0, 'effective': 0},
                'majority_vote': {'used': 0, 'effective': 0}
            }
            
            max_len = min(len(signals), min(len(scores) for scores in individual_scores.values() if scores))
            
            for i in range(max_len):
                scores_at_i = []
                for component, scores in individual_scores.items():
                    if i < len(scores):
                        scores_at_i.append(scores[i])
                
                if len(scores_at_i) < 2:
                    continue
                
                # è¨ˆç®—åŠ æ¬Šå¹³å‡åˆ†æ•¸
                weighted_avg = sum(s * w for s, w in zip(scores_at_i, self.weights.values())) / sum(self.weights.values())
                
                # æª¢æ¸¬ä½¿ç”¨çš„ç­–ç•¥
                signal = signals[i] if i < len(signals) else 'HOLD'
                score_std = np.std(scores_at_i)
                
                if score_std > 15:  # é«˜åˆ†æ­§æƒ…æ³
                    if signal == 'HOLD':
                        strategies['conservative_hold']['used'] += 1
                        # å‡è¨­ä¿å®ˆç­–ç•¥åœ¨é«˜åˆ†æ­§æ™‚æ˜¯æœ‰æ•ˆçš„
                        strategies['conservative_hold']['effective'] += 1
                    else:
                        strategies['weighted_average']['used'] += 1
                        # æª¢æŸ¥åŠ æ¬Šå¹³å‡æ˜¯å¦èˆ‡æœ€çµ‚ä¿¡è™Ÿä¸€è‡´
                        expected_signal = 'BUY' if weighted_avg > 60 else 'SELL' if weighted_avg < 40 else 'HOLD'
                        if expected_signal == signal:
                            strategies['weighted_average']['effective'] += 1
                
                # å¤šæ•¸æŠ•ç¥¨ç­–ç•¥æª¢æ¸¬
                buy_votes = sum(1 for s in scores_at_i if s > 60)
                sell_votes = sum(1 for s in scores_at_i if s < 40)
                hold_votes = len(scores_at_i) - buy_votes - sell_votes
                
                max_votes = max(buy_votes, sell_votes, hold_votes)
                if max_votes > len(scores_at_i) / 2:  # æœ‰æ˜é¡¯å¤šæ•¸
                    strategies['majority_vote']['used'] += 1
                    expected_majority = 'BUY' if buy_votes == max_votes else 'SELL' if sell_votes == max_votes else 'HOLD'
                    if expected_majority == signal:
                        strategies['majority_vote']['effective'] += 1
            
            # è¨ˆç®—ç­–ç•¥æ•ˆç‡
            for strategy in strategies.values():
                if strategy['used'] > 0:
                    strategy['effectiveness_rate'] = strategy['effective'] / strategy['used']
                else:
                    strategy['effectiveness_rate'] = 0.0
            
            return strategies
        except Exception as e:
            logger.error(f"âŒ è©•ä¼°è¡çªè§£æ±ºç­–ç•¥å¤±æ•—: {e}")
            return {}
    
    def _assess_ensemble_reliability(self, validation_metrics: Dict[str, Any]) -> Dict[str, Any]:
        """è©•ä¼°é›†æˆç³»çµ±å¯é æ€§"""
        try:
            reliability_factors = {}
            
            # æˆåŠŸç‡å¯é æ€§
            success_rate = validation_metrics.get('success_rate', 0)
            reliability_factors['success_rate'] = {
                'score': success_rate,
                'weight': 0.2,
                'assessment': 'excellent' if success_rate > 0.95 else 'good' if success_rate > 0.8 else 'fair' if success_rate > 0.6 else 'poor'
            }
            
            # æ¨¡å‹ä¸€è‡´æ€§å¯é æ€§
            model_agreement = validation_metrics.get('model_agreement', 0)
            reliability_factors['model_agreement'] = {
                'score': model_agreement,
                'weight': 0.25,
                'assessment': 'excellent' if model_agreement > 0.8 else 'good' if model_agreement > 0.6 else 'fair' if model_agreement > 0.4 else 'poor'
            }
            
            # é æ¸¬ç©©å®šæ€§å¯é æ€§
            stability_metrics = validation_metrics.get('prediction_stability', {})
            if stability_metrics:
                avg_stability = np.mean([
                    stability_metrics.get('score_stability', {}).get('stability_score', 0),
                    stability_metrics.get('signal_stability', {}).get('stability_score', 0),
                    stability_metrics.get('confidence_stability', {}).get('stability_score', 0)
                ])
                reliability_factors['prediction_stability'] = {
                    'score': avg_stability,
                    'weight': 0.2,
                    'assessment': 'excellent' if avg_stability > 0.8 else 'good' if avg_stability > 0.6 else 'fair' if avg_stability > 0.4 else 'poor'
                }
            
            # ä¿¡å¿ƒåº¦åˆ†ä½ˆå¯é æ€§
            confidence_analysis = validation_metrics.get('confidence_analysis', {})
            if confidence_analysis and 'percentages' in confidence_analysis:
                high_conf_ratio = confidence_analysis['percentages'].get('high_confidence', 0)
                reliability_factors['confidence_distribution'] = {
                    'score': high_conf_ratio,
                    'weight': 0.15,
                    'assessment': 'excellent' if high_conf_ratio > 0.6 else 'good' if high_conf_ratio > 0.4 else 'fair' if high_conf_ratio > 0.2 else 'poor'
                }
            
            # æ¬Šé‡æœ‰æ•ˆæ€§å¯é æ€§
            weight_effectiveness = validation_metrics.get('weight_effectiveness', {})
            if weight_effectiveness and 'effectiveness_score' in weight_effectiveness:
                eff_score = weight_effectiveness['effectiveness_score']
                reliability_factors['weight_effectiveness'] = {
                    'score': eff_score,
                    'weight': 0.2,
                    'assessment': 'excellent' if eff_score > 0.8 else 'good' if eff_score > 0.6 else 'fair' if eff_score > 0.4 else 'poor'
                }
            
            # è¨ˆç®—ç¸½é«”å¯é æ€§åˆ†æ•¸
            total_score = 0.0
            total_weight = 0.0
            
            for factor, data in reliability_factors.items():
                total_score += data['score'] * data['weight']
                total_weight += data['weight']
            
            overall_reliability = total_score / total_weight if total_weight > 0 else 0.0
            
            # ç¸½é«”è©•ä¼°
            if overall_reliability > 0.8:
                overall_assessment = 'excellent'
                reliability_level = 'high'
            elif overall_reliability > 0.6:
                overall_assessment = 'good'
                reliability_level = 'medium-high'
            elif overall_reliability > 0.4:
                overall_assessment = 'fair'
                reliability_level = 'medium'
            else:
                overall_assessment = 'poor'
                reliability_level = 'low'
            
            return {
                'overall_score': overall_reliability,
                'overall_assessment': overall_assessment,
                'reliability_level': reliability_level,
                'factor_scores': reliability_factors,
                'recommendations': self._generate_reliability_recommendations(reliability_factors)
            }
        except Exception as e:
            logger.error(f"âŒ è©•ä¼°é›†æˆç³»çµ±å¯é æ€§å¤±æ•—: {e}")
            return {}
    
    def _generate_reliability_recommendations(self, reliability_factors: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆå¯é æ€§æ”¹é€²å»ºè­°"""
        try:
            recommendations = []
            
            for factor, data in reliability_factors.items():
                if data['assessment'] == 'poor':
                    if factor == 'success_rate':
                        recommendations.append("æé«˜æ•¸æ“šè³ªé‡å’Œé è™•ç†æµç¨‹ä»¥æ”¹å–„æˆåŠŸç‡")
                    elif factor == 'model_agreement':
                        recommendations.append("èª¿æ•´æ¨¡å‹æ¬Šé‡æˆ–é‡æ–°è¨“ç·´çµ„ä»¶æ¨¡å‹ä»¥æé«˜ä¸€è‡´æ€§")
                    elif factor == 'prediction_stability':
                        recommendations.append("å¢åŠ æ•¸æ“šå¹³æ»‘è™•ç†æˆ–èª¿æ•´é æ¸¬åƒæ•¸ä»¥æé«˜ç©©å®šæ€§")
                    elif factor == 'confidence_distribution':
                        recommendations.append("å„ªåŒ–ä¿¡å¿ƒåº¦è¨ˆç®—æ–¹æ³•æˆ–èª¿æ•´æ±ºç­–é–¾å€¼")
                    elif factor == 'weight_effectiveness':
                        recommendations.append("é‡æ–°è©•ä¼°å’Œå„ªåŒ–çµ„ä»¶æ¬Šé‡åˆ†é…")
                elif data['assessment'] == 'fair':
                    if factor == 'model_agreement':
                        recommendations.append("è€ƒæ…®å¢åŠ æ¨¡å‹ä¸€è‡´æ€§æª¢æŸ¥æ©Ÿåˆ¶")
                    elif factor == 'prediction_stability':
                        recommendations.append("å¯¦æ–½é æ¸¬çµæœçš„æ™‚é–“åºåˆ—å¹³æ»‘")
            
            if not recommendations:
                recommendations.append("ç³»çµ±å¯é æ€§è‰¯å¥½ï¼Œå»ºè­°å®šæœŸç›£æ§å’Œç¶­è­·")
            
            return recommendations
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå¯é æ€§å»ºè­°å¤±æ•—: {e}")
            return ["å»ºè­°é€²è¡Œè©³ç´°çš„ç³»çµ±è¨ºæ–·"]
    
    def _calculate_ensemble_overall_score(self, validation_metrics: Dict[str, Any]) -> float:
        """è¨ˆç®—é›†æˆç³»çµ±ç¸½é«”è©•åˆ†"""
        try:
            score_components = {}
            
            # æˆåŠŸç‡ (20%)
            success_rate = validation_metrics.get('success_rate', 0)
            score_components['success_rate'] = success_rate * 0.2
            
            # æ¨¡å‹ä¸€è‡´æ€§ (25%)
            model_agreement = validation_metrics.get('model_agreement', 0)
            score_components['model_agreement'] = model_agreement * 0.25
            
            # ä¿¡å¿ƒåº¦è³ªé‡ (20%)
            confidence_analysis = validation_metrics.get('confidence_analysis', {})
            if confidence_analysis and 'percentages' in confidence_analysis:
                high_conf_ratio = confidence_analysis['percentages'].get('high_confidence', 0)
                score_components['confidence_quality'] = high_conf_ratio * 0.2
            else:
                score_components['confidence_quality'] = 0.1  # é»˜èªåˆ†æ•¸
            
            # æ¬Šé‡æœ‰æ•ˆæ€§ (15%)
            weight_effectiveness = validation_metrics.get('weight_effectiveness', {})
            if weight_effectiveness and 'effectiveness_score' in weight_effectiveness:
                score_components['weight_effectiveness'] = weight_effectiveness['effectiveness_score'] * 0.15
            else:
                score_components['weight_effectiveness'] = 0.075  # é»˜èªåˆ†æ•¸
            
            # é æ¸¬ç©©å®šæ€§ (10%)
            stability_metrics = validation_metrics.get('prediction_stability', {})
            if stability_metrics:
                avg_stability = np.mean([
                    stability_metrics.get('score_stability', {}).get('stability_score', 0),
                    stability_metrics.get('signal_stability', {}).get('stability_score', 0),
                    stability_metrics.get('confidence_stability', {}).get('stability_score', 0)
                ])
                score_components['prediction_stability'] = avg_stability * 0.1
            else:
                score_components['prediction_stability'] = 0.05  # é»˜èªåˆ†æ•¸
            
            # è¡çªè§£æ±ºèƒ½åŠ› (10%)
            conflict_resolution = validation_metrics.get('conflict_resolution', {})
            if conflict_resolution and 'resolution_effectiveness' in conflict_resolution:
                score_components['conflict_resolution'] = conflict_resolution['resolution_effectiveness'] * 0.1
            else:
                score_components['conflict_resolution'] = 0.05  # é»˜èªåˆ†æ•¸
            
            # è¨ˆç®—ç¸½åˆ†
            total_score = sum(score_components.values())
            
            return float(total_score)
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—ç¸½é«”è©•åˆ†å¤±æ•—: {e}")
            return 0.5
    
    def _identify_ensemble_strengths(self, validation_metrics: Dict[str, Any]) -> List[str]:
        """è­˜åˆ¥é›†æˆç³»çµ±å„ªå‹¢"""
        try:
            strengths = []
            
            # æª¢æŸ¥å„é …æŒ‡æ¨™
            success_rate = validation_metrics.get('success_rate', 0)
            if success_rate > 0.9:
                strengths.append(f"æ¥µé«˜çš„é æ¸¬æˆåŠŸç‡ ({success_rate:.1%})")
            elif success_rate > 0.8:
                strengths.append(f"é«˜é æ¸¬æˆåŠŸç‡ ({success_rate:.1%})")
            
            model_agreement = validation_metrics.get('model_agreement', 0)
            if model_agreement > 0.8:
                strengths.append(f"å„ªç§€çš„æ¨¡å‹ä¸€è‡´æ€§ ({model_agreement:.3f})")
            elif model_agreement > 0.6:
                strengths.append(f"è‰¯å¥½çš„æ¨¡å‹ä¸€è‡´æ€§ ({model_agreement:.3f})")
            
            confidence_analysis = validation_metrics.get('confidence_analysis', {})
            if confidence_analysis and 'percentages' in confidence_analysis:
                high_conf_ratio = confidence_analysis['percentages'].get('high_confidence', 0)
                if high_conf_ratio > 0.6:
                    strengths.append(f"é«˜æ¯”ä¾‹çš„é«˜ä¿¡å¿ƒåº¦é æ¸¬ ({high_conf_ratio:.1%})")
            
            weight_effectiveness = validation_metrics.get('weight_effectiveness', {})
            if weight_effectiveness and 'effectiveness_score' in weight_effectiveness:
                eff_score = weight_effectiveness['effectiveness_score']
                if eff_score > 0.7:
                    strengths.append(f"æœ‰æ•ˆçš„æ¬Šé‡é…ç½® (æ•ˆç‡: {eff_score:.3f})")
            
            if not strengths:
                strengths.append("ç³»çµ±é‹è¡Œç©©å®šï¼Œå„é …æŒ‡æ¨™å‡è¡¡")
            
            return strengths
        except Exception as e:
            logger.error(f"âŒ è­˜åˆ¥ç³»çµ±å„ªå‹¢å¤±æ•—: {e}")
            return ["ç³»çµ±åŸºæœ¬åŠŸèƒ½æ­£å¸¸"]
    
    def _identify_ensemble_weaknesses(self, validation_metrics: Dict[str, Any]) -> List[str]:
        """è­˜åˆ¥é›†æˆç³»çµ±å¼±é»"""
        try:
            weaknesses = []
            
            # æª¢æŸ¥å„é …æŒ‡æ¨™
            success_rate = validation_metrics.get('success_rate', 0)
            if success_rate < 0.7:
                weaknesses.append(f"é æ¸¬æˆåŠŸç‡åä½ ({success_rate:.1%})")
            
            model_agreement = validation_metrics.get('model_agreement', 0)
            if model_agreement < 0.5:
                weaknesses.append(f"æ¨¡å‹ä¸€è‡´æ€§ä¸è¶³ ({model_agreement:.3f})")
            
            confidence_analysis = validation_metrics.get('confidence_analysis', {})
            if confidence_analysis and 'percentages' in confidence_analysis:
                low_conf_ratio = confidence_analysis['percentages'].get('low_confidence', 0)
                if low_conf_ratio > 0.4:
                    weaknesses.append(f"ä½ä¿¡å¿ƒåº¦é æ¸¬æ¯”ä¾‹éé«˜ ({low_conf_ratio:.1%})")
            
            stability_metrics = validation_metrics.get('prediction_stability', {})
            if stability_metrics:
                signal_stability = stability_metrics.get('signal_stability', {}).get('stability_score', 1)
                if signal_stability < 0.6:
                    weaknesses.append(f"ä¿¡è™Ÿç©©å®šæ€§ä¸è¶³ ({signal_stability:.3f})")
            
            conflict_resolution = validation_metrics.get('conflict_resolution', {})
            if conflict_resolution:
                conflict_rate = conflict_resolution.get('conflict_cases', 0) / max(conflict_resolution.get('total_predictions', 1), 1)
                if conflict_rate > 0.3:
                    weaknesses.append(f"æ¨¡å‹è¡çªé »ç‡éé«˜ ({conflict_rate:.1%})")
            
            if not weaknesses:
                weaknesses.append("æœªç™¼ç¾æ˜é¡¯å¼±é»ï¼Œç³»çµ±è¡¨ç¾è‰¯å¥½")
            
            return weaknesses
        except Exception as e:
            logger.error(f"âŒ è­˜åˆ¥ç³»çµ±å¼±é»å¤±æ•—: {e}")
            return ["éœ€è¦é€²ä¸€æ­¥åˆ†æç³»çµ±æ€§èƒ½"]
    
    def _generate_ensemble_recommendations(self, validation_metrics: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆé›†æˆç³»çµ±æ”¹é€²å»ºè­°"""
        try:
            recommendations = []
            
            # åŸºæ–¼æ¬Šé‡å„ªåŒ–å»ºè­°
            weight_optimization = validation_metrics.get('weight_optimization', {})
            if weight_optimization and 'final_recommendation' in weight_optimization:
                final_rec = weight_optimization['final_recommendation']
                if final_rec.get('confidence', 0) > 0.7:
                    recommendations.append("å»ºè­°æ¡ç”¨å„ªåŒ–å¾Œçš„æ¬Šé‡é…ç½®ä»¥æå‡æ€§èƒ½")
            
            # åŸºæ–¼è¡çªè§£æ±ºå»ºè­°
            conflict_resolution = validation_metrics.get('conflict_resolution', {})
            if conflict_resolution:
                conflict_rate = conflict_resolution.get('conflict_cases', 0) / max(conflict_resolution.get('total_predictions', 1), 1)
                if conflict_rate > 0.2:
                    recommendations.append("å»ºè­°åŠ å¼·è¡çªè§£æ±ºæ©Ÿåˆ¶ï¼Œè€ƒæ…®å¢åŠ ä¿å®ˆç­–ç•¥")
            
            # åŸºæ–¼æ¨¡å‹ä¸€è‡´æ€§å»ºè­°
            model_agreement = validation_metrics.get('model_agreement', 0)
            if model_agreement < 0.6:
                recommendations.append("å»ºè­°é‡æ–°è©•ä¼°çµ„ä»¶æ¨¡å‹ï¼Œæé«˜æ¨¡å‹é–“ä¸€è‡´æ€§")
            
            # åŸºæ–¼ä¿¡å¿ƒåº¦åˆ†ä½ˆå»ºè­°
            confidence_analysis = validation_metrics.get('confidence_analysis', {})
            if confidence_analysis and 'percentages' in confidence_analysis:
                low_conf_ratio = confidence_analysis['percentages'].get('low_confidence', 0)
                if low_conf_ratio > 0.3:
                    recommendations.append("å»ºè­°å„ªåŒ–ä¿¡å¿ƒåº¦è¨ˆç®—æ–¹æ³•ï¼Œæé«˜é æ¸¬ä¿¡å¿ƒåº¦")
            
            # åŸºæ–¼ç©©å®šæ€§å»ºè­°
            stability_metrics = validation_metrics.get('prediction_stability', {})
            if stability_metrics:
                signal_stability = stability_metrics.get('signal_stability', {}).get('stability_score', 1)
                if signal_stability < 0.7:
                    recommendations.append("å»ºè­°å¢åŠ é æ¸¬çµæœçš„æ™‚é–“åºåˆ—å¹³æ»‘è™•ç†")
            
            # é€šç”¨å»ºè­°
            if not recommendations:
                recommendations.extend([
                    "ç³»çµ±è¡¨ç¾è‰¯å¥½ï¼Œå»ºè­°å®šæœŸé€²è¡Œé©—è­‰å’Œç›£æ§",
                    "è€ƒæ…®æ”¶é›†æ›´å¤šæ­·å²æ•¸æ“šä»¥é€²ä¸€æ­¥å„ªåŒ–æ¨¡å‹",
                    "å»ºè­°å¯¦æ–½A/Bæ¸¬è©¦ä¾†é©—è­‰æ”¹é€²æ•ˆæœ"
                ])
            
            return recommendations
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ”¹é€²å»ºè­°å¤±æ•—: {e}")
            return ["å»ºè­°é€²è¡Œå…¨é¢çš„ç³»çµ±è©•ä¼°å’Œå„ªåŒ–"]


# å‰µå»ºå…¨å±€é›†æˆè©•åˆ†å™¨å¯¦ä¾‹
def create_ensemble_scorer() -> EnsembleScorer:
    """å‰µå»ºé›†æˆè©•åˆ†å™¨å¯¦ä¾‹"""
    return EnsembleScorer()


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    import asyncio
    
    async def test_ensemble_scorer():
        """æ¸¬è©¦é›†æˆè©•åˆ†å™¨"""
        print("ğŸ§ª æ¸¬è©¦é›†æˆè©•åˆ†å™¨...")
        
        scorer = create_ensemble_scorer()
        
        # æ¨¡æ“¬å¸‚å ´æ•¸æ“š
        test_market_data = {
            'current_price': 1500000,
            'price_change_1m': 1.5,
            'volume_ratio': 1.2,
            'ai_formatted_data': 'å¸‚å ´é¡¯ç¤ºä¸Šæ¼²è¶¨å‹¢ï¼ŒæŠ€è¡“æŒ‡æ¨™çœ‹æ¼²'
        }
        
        # åŸ·è¡Œåˆ†æ
        result = scorer.analyze(test_market_data)
        
        print(f"âœ… åˆ†æçµæœ:")
        print(f"   æˆåŠŸ: {result['success']}")
        print(f"   æœ€çµ‚åˆ†æ•¸: {result['final_score']:.1f}")
        print(f"   äº¤æ˜“ä¿¡è™Ÿ: {result['signal']}")
        print(f"   ä¿¡å¿ƒåº¦: {result['confidence']:.1f}%")
        
        if result.get('individual_results'):
            print(f"   å€‹åˆ¥çµæœ:")
            for component, comp_result in result['individual_results'].items():
                print(f"     {component}: {comp_result['score']:.1f}")
    
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(test_ensemble_scorer())
    
    def _optimize_weights_by_accuracy(self, individual_scores: Dict[str, List[float]], 
                                    ground_truth: List[str], signals: List[str]) -> Dict[str, float]:
        """åŸºæ–¼æº–ç¢ºæ€§å„ªåŒ–æ¬Šé‡"""
        try:
            # é€™è£¡å¯ä»¥å¯¦ç¾æ›´è¤‡é›œçš„åŸºæ–¼æº–ç¢ºæ€§çš„æ¬Šé‡å„ªåŒ–
            # æš«æ™‚è¿”å›åŸºæ–¼ç•¶å‰æ¬Šé‡çš„èª¿æ•´
            component_accuracies = {}
            
            for component in individual_scores.keys():
                # æ¨¡æ“¬è¨ˆç®—æ¯å€‹çµ„ä»¶å°æº–ç¢ºæ€§çš„è²¢ç»
                # å¯¦éš›å¯¦ç¾ä¸­å¯ä»¥ä½¿ç”¨æ›´è¤‡é›œçš„æ–¹æ³•
                component_accuracies[component] = np.random.uniform(0.4, 0.8)
            
            # æ¨™æº–åŒ–æº–ç¢ºæ€§ä½œç‚ºæ¬Šé‡
            total_acc = sum(component_accuracies.values())
            if total_acc > 0:
                optimized_weights = {k: v / total_acc for k, v in component_accuracies.items()}
            else:
                optimized_weights = {k: 1.0 / len(component_accuracies) for k in component_accuracies.keys()}
            
            return optimized_weights
        except Exception as e:
            logger.error(f"âŒ åŸºæ–¼æº–ç¢ºæ€§å„ªåŒ–æ¬Šé‡å¤±æ•—: {e}")
            return {}
    
    def _optimize_weights_by_variance(self, individual_scores: Dict[str, List[float]]) -> Dict[str, float]:
        """åŸºæ–¼æ–¹å·®å„ªåŒ–æ¬Šé‡"""
        try:
            variances = {}
            for component, scores in individual_scores.items():
                if scores:
                    # æ–¹å·®è¶Šå°ï¼Œæ¬Šé‡è¶Šé«˜ï¼ˆæ›´ç©©å®šçš„çµ„ä»¶ç²å¾—æ›´é«˜æ¬Šé‡ï¼‰
                    variance = np.var(scores)
                    variances[component] = 1.0 / (1.0 + variance)
            
            # æ¨™æº–åŒ–æ–¹å·®å€’æ•¸ä½œç‚ºæ¬Šé‡
            total_inv_var = sum(variances.values())
            if total_inv_var > 0:
                optimized_weights = {k: v / total_inv_var for k, v in variances.items()}
            else:
                optimized_weights = {k: 1.0 / len(variances) for k in variances.keys()}
            
            return optimized_weights
        except Exception as e:
            logger.error(f"âŒ åŸºæ–¼æ–¹å·®å„ªåŒ–æ¬Šé‡å¤±æ•—: {e}")
            return {}
    
    def _generate_weight_recommendation(self, optimization_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€çµ‚æ¬Šé‡å»ºè­°"""
        try:
            if not optimization_results:
                return {}
            
            # åˆä½µä¸åŒå„ªåŒ–æ–¹æ³•çš„çµæœ
            all_components = set()
            for method_results in optimization_results.values():
                if isinstance(method_results, dict):
                    all_components.update(method_results.keys())
            
            final_weights = {}
            for component in all_components:
                weights_for_component = []
                for method_name, method_results in optimization_results.items():
                    if isinstance(method_results, dict) and component in method_results:
                        weights_for_component.append(method_results[component])
                
                if weights_for_component:
                    final_weights[component] = np.mean(weights_for_component)
            
            # æ¨™æº–åŒ–æœ€çµ‚æ¬Šé‡
            total_weight = sum(final_weights.values())
            if total_weight > 0:
                final_weights = {k: v / total_weight for k, v in final_weights.items()}
            
            # è¨ˆç®—èˆ‡ç•¶å‰æ¬Šé‡çš„å·®ç•°
            weight_changes = {}
            for component in final_weights:
                current_weight = self.weights.get(component, 0)
                recommended_weight = final_weights[component]
                weight_changes[component] = {
                    'current': current_weight,
                    'recommended': recommended_weight,
                    'change': recommended_weight - current_weight,
                    'change_percentage': ((recommended_weight - current_weight) / current_weight * 100) if current_weight > 0 else 0
                }
            
            return {
                'recommended_weights': final_weights,
                'weight_changes': weight_changes,
                'improvement_potential': self._estimate_improvement_potential(weight_changes)
            }
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ¬Šé‡å»ºè­°å¤±æ•—: {e}")
            return {}
    
    def _estimate_improvement_potential(self, weight_changes: Dict[str, Any]) -> float:
        """ä¼°ç®—æ”¹é€²æ½›åŠ›"""
        try:
            if not weight_changes:
                return 0.0
            
            # åŸºæ–¼æ¬Šé‡è®ŠåŒ–å¹…åº¦ä¼°ç®—æ”¹é€²æ½›åŠ›
            total_change = sum(abs(change['change']) for change in weight_changes.values())
            improvement_potential = min(total_change * 0.1, 0.2)  # æœ€å¤§20%çš„æ”¹é€²æ½›åŠ›
            
            return float(improvement_potential)
        except Exception as e:
            logger.error(f"âŒ ä¼°ç®—æ”¹é€²æ½›åŠ›å¤±æ•—: {e}")
            return 0.0
    
    def _evaluate_conflict_resolution(self, individual_scores: Dict[str, List[float]], 
                                    signals: List[str]) -> Dict[str, Any]:
        """è©•ä¼°è¡çªè§£æ±ºæ©Ÿåˆ¶"""
        try:
            if not individual_scores or not signals:
                return {}
            
            # æª¢æ¸¬åˆ†æ•¸åˆ†æ­§æƒ…æ³
            conflicts_detected = 0
            conflict_resolutions = []
            
            max_len = max(len(scores) for scores in individual_scores.values() if scores)
            
            for i in range(max_len):
                scores_at_i = []
                for scores in individual_scores.values():
                    if i < len(scores):
                        scores_at_i.append(scores[i])
                
                if len(scores_at_i) > 1:
                    std_at_i = np.std(scores_at_i)
                    if std_at_i > 15:  # åˆ†æ­§é–¾å€¼
                        conflicts_detected += 1
                        if i < len(signals):
                            conflict_resolutions.append(signals[i])
            
            # åˆ†æè¡çªè§£æ±ºæ•ˆæœ
            resolution_effectiveness = self._analyze_conflict_resolution_effectiveness(conflict_resolutions)
            
            return {
                'conflicts_detected': conflicts_detected,
                'conflict_rate': conflicts_detected / max_len if max_len > 0 else 0,
                'resolution_strategies': conflict_resolutions,
                'resolution_effectiveness': resolution_effectiveness,
                'hold_bias': conflict_resolutions.count('HOLD') / len(conflict_resolutions) if conflict_resolutions else 0
            }
        except Exception as e:
            logger.error(f"âŒ è©•ä¼°è¡çªè§£æ±ºæ©Ÿåˆ¶å¤±æ•—: {e}")
            return {}
    
    def _analyze_conflict_resolution_effectiveness(self, resolutions: List[str]) -> Dict[str, Any]:
        """åˆ†æè¡çªè§£æ±ºæœ‰æ•ˆæ€§"""
        try:
            if not resolutions:
                return {}
            
            resolution_counts = {}
            for resolution in resolutions:
                resolution_counts[resolution] = resolution_counts.get(resolution, 0) + 1
            
            # è©•ä¼°è§£æ±ºç­–ç•¥çš„å¤šæ¨£æ€§
            diversity = len(resolution_counts) / 3  # 3æ˜¯å¯èƒ½çš„ä¿¡è™Ÿæ•¸é‡
            
            # è©•ä¼°ä¿å®ˆæ€§ (HOLDä¿¡è™Ÿçš„æ¯”ä¾‹)
            conservatism = resolution_counts.get('HOLD', 0) / len(resolutions)
            
            return {
                'resolution_distribution': resolution_counts,
                'strategy_diversity': diversity,
                'conservatism_level': conservatism,
                'effectiveness_score': (diversity + (1 - conservatism)) / 2  # å¹³è¡¡å¤šæ¨£æ€§å’Œä¿å®ˆæ€§
            }
        except Exception as e:
            logger.error(f"âŒ åˆ†æè¡çªè§£æ±ºæœ‰æ•ˆæ€§å¤±æ•—: {e}")
            return {}
    
    def _assess_ensemble_reliability(self, validation_metrics: Dict[str, Any]) -> Dict[str, Any]:
        """è©•ä¼°é›†æˆå¯é æ€§"""
        try:
            reliability_factors = {}
            
            # æˆåŠŸç‡å¯é æ€§
            success_rate = validation_metrics.get('success_rate', 0)
            reliability_factors['success_rate'] = success_rate
            
            # ä¸€è‡´æ€§å¯é æ€§
            model_agreement = validation_metrics.get('model_agreement', 0)
            reliability_factors['consistency'] = model_agreement
            
            # ç©©å®šæ€§å¯é æ€§
            stability_metrics = validation_metrics.get('prediction_stability', {})
            if stability_metrics:
                avg_stability = np.mean([
                    stability_metrics.get('score_stability', {}).get('stability_score', 0),
                    stability_metrics.get('signal_stability', {}).get('stability_score', 0),
                    stability_metrics.get('confidence_stability', {}).get('stability_score', 0)
                ])
                reliability_factors['stability'] = avg_stability
            
            # æ¬Šé‡æœ‰æ•ˆæ€§å¯é æ€§
            weight_effectiveness = validation_metrics.get('weight_effectiveness', {})
            effectiveness_score = weight_effectiveness.get('effectiveness_score', 0)
            reliability_factors['weight_effectiveness'] = effectiveness_score
            
            # è¨ˆç®—æ•´é«”å¯é æ€§åˆ†æ•¸
            overall_reliability = np.mean(list(reliability_factors.values()))
            
            # å¯é æ€§ç­‰ç´š
            if overall_reliability >= 0.8:
                reliability_grade = "å„ªç§€"
            elif overall_reliability >= 0.7:
                reliability_grade = "è‰¯å¥½"
            elif overall_reliability >= 0.6:
                reliability_grade = "ä¸­ç­‰"
            elif overall_reliability >= 0.5:
                reliability_grade = "åŠæ ¼"
            else:
                reliability_grade = "éœ€è¦æ”¹é€²"
            
            return {
                'reliability_factors': reliability_factors,
                'overall_reliability': overall_reliability,
                'reliability_grade': reliability_grade,
                'confidence_level': self._calculate_reliability_confidence(reliability_factors)
            }
        except Exception as e:
            logger.error(f"âŒ è©•ä¼°é›†æˆå¯é æ€§å¤±æ•—: {e}")
            return {}
    
    def _calculate_reliability_confidence(self, reliability_factors: Dict[str, float]) -> float:
        """è¨ˆç®—å¯é æ€§ä¿¡å¿ƒåº¦"""
        try:
            if not reliability_factors:
                return 0.0
            
            # åŸºæ–¼å„å› ç´ çš„æ–¹å·®è¨ˆç®—ä¿¡å¿ƒåº¦
            values = list(reliability_factors.values())
            variance = np.var(values)
            
            # æ–¹å·®è¶Šå°ï¼Œä¿¡å¿ƒåº¦è¶Šé«˜
            confidence = 1.0 / (1.0 + variance)
            
            return float(confidence)
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—å¯é æ€§ä¿¡å¿ƒåº¦å¤±æ•—: {e}")
            return 0.5
    
    def _calculate_ensemble_overall_score(self, validation_metrics: Dict[str, Any]) -> float:
        """è¨ˆç®—é›†æˆæ•´é«”è©•åˆ†"""
        try:
            score_components = {}
            
            # æˆåŠŸç‡ (25%)
            success_rate = validation_metrics.get('success_rate', 0)
            score_components['success_rate'] = success_rate * 0.25
            
            # æ¨¡å‹ä¸€è‡´æ€§ (20%)
            model_agreement = validation_metrics.get('model_agreement', 0)
            score_components['consistency'] = model_agreement * 0.20
            
            # é æ¸¬ç©©å®šæ€§ (20%)
            stability_metrics = validation_metrics.get('prediction_stability', {})
            if stability_metrics:
                avg_stability = np.mean([
                    stability_metrics.get('score_stability', {}).get('stability_score', 0),
                    stability_metrics.get('signal_stability', {}).get('stability_score', 0),
                    stability_metrics.get('confidence_stability', {}).get('stability_score', 0)
                ])
                score_components['stability'] = avg_stability * 0.20
            
            # æ¬Šé‡æœ‰æ•ˆæ€§ (15%)
            weight_effectiveness = validation_metrics.get('weight_effectiveness', {})
            effectiveness_score = weight_effectiveness.get('effectiveness_score', 0)
            score_components['weight_effectiveness'] = effectiveness_score * 0.15
            
            # ä¿¡å¿ƒåº¦åˆ†æ (10%)
            confidence_analysis = validation_metrics.get('confidence_analysis', {})
            if confidence_analysis and 'statistics' in confidence_analysis:
                avg_confidence = confidence_analysis['statistics'].get('mean', 0) / 100
                score_components['confidence'] = avg_confidence * 0.10
            
            # è¡çªè§£æ±º (10%)
            conflict_resolution = validation_metrics.get('conflict_resolution', {})
            resolution_effectiveness = conflict_resolution.get('resolution_effectiveness', {})
            if resolution_effectiveness:
                effectiveness = resolution_effectiveness.get('effectiveness_score', 0)
                score_components['conflict_resolution'] = effectiveness * 0.10
            
            # è¨ˆç®—ç¸½åˆ†
            total_score = sum(score_components.values())
            
            return min(1.0, max(0.0, total_score))
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—é›†æˆæ•´é«”è©•åˆ†å¤±æ•—: {e}")
            return 0.0
    
    def _identify_ensemble_strengths(self, validation_metrics: Dict[str, Any]) -> List[str]:
        """è­˜åˆ¥é›†æˆå„ªå‹¢"""
        try:
            strengths = []
            
            # æª¢æŸ¥æˆåŠŸç‡
            success_rate = validation_metrics.get('success_rate', 0)
            if success_rate > 0.9:
                strengths.append(f"æ¥µé«˜çš„é æ¸¬æˆåŠŸç‡ ({success_rate:.1%})")
            elif success_rate > 0.8:
                strengths.append(f"é«˜é æ¸¬æˆåŠŸç‡ ({success_rate:.1%})")
            
            # æª¢æŸ¥æ¨¡å‹ä¸€è‡´æ€§
            model_agreement = validation_metrics.get('model_agreement', 0)
            if model_agreement > 0.8:
                strengths.append(f"å„ªç§€çš„æ¨¡å‹ä¸€è‡´æ€§ ({model_agreement:.3f})")
            elif model_agreement > 0.7:
                strengths.append(f"è‰¯å¥½çš„æ¨¡å‹ä¸€è‡´æ€§ ({model_agreement:.3f})")
            
            # æª¢æŸ¥ä¿¡å¿ƒåº¦
            confidence_analysis = validation_metrics.get('confidence_analysis', {})
            if confidence_analysis and 'statistics' in confidence_analysis:
                avg_confidence = confidence_analysis['statistics'].get('mean', 0)
                if avg_confidence > 80:
                    strengths.append(f"é«˜å¹³å‡ä¿¡å¿ƒåº¦ ({avg_confidence:.1f}%)")
            
            # æª¢æŸ¥ç©©å®šæ€§
            stability_metrics = validation_metrics.get('prediction_stability', {})
            if stability_metrics:
                signal_stability = stability_metrics.get('signal_stability', {}).get('stability_score', 0)
                if signal_stability > 0.8:
                    strengths.append(f"å„ªç§€çš„ä¿¡è™Ÿç©©å®šæ€§ ({signal_stability:.3f})")
            
            # æª¢æŸ¥æ¬Šé‡æœ‰æ•ˆæ€§
            weight_effectiveness = validation_metrics.get('weight_effectiveness', {})
            effectiveness_score = weight_effectiveness.get('effectiveness_score', 0)
            if effectiveness_score > 0.7:
                strengths.append(f"æœ‰æ•ˆçš„æ¬Šé‡é…ç½® ({effectiveness_score:.3f})")
            
            return strengths
        except Exception as e:
            logger.error(f"âŒ è­˜åˆ¥é›†æˆå„ªå‹¢å¤±æ•—: {e}")
            return []
    
    def _identify_ensemble_weaknesses(self, validation_metrics: Dict[str, Any]) -> List[str]:
        """è­˜åˆ¥é›†æˆå¼±é»"""
        try:
            weaknesses = []
            
            # æª¢æŸ¥æˆåŠŸç‡
            success_rate = validation_metrics.get('success_rate', 0)
            if success_rate < 0.7:
                weaknesses.append(f"é æ¸¬æˆåŠŸç‡åä½ ({success_rate:.1%})")
            
            # æª¢æŸ¥æ¨¡å‹ä¸€è‡´æ€§
            model_agreement = validation_metrics.get('model_agreement', 0)
            if model_agreement < 0.5:
                weaknesses.append(f"æ¨¡å‹ä¸€è‡´æ€§ä¸è¶³ ({model_agreement:.3f})")
            
            # æª¢æŸ¥ä¿¡å¿ƒåº¦
            confidence_analysis = validation_metrics.get('confidence_analysis', {})
            if confidence_analysis and 'statistics' in confidence_analysis:
                avg_confidence = confidence_analysis['statistics'].get('mean', 0)
                if avg_confidence < 60:
                    weaknesses.append(f"å¹³å‡ä¿¡å¿ƒåº¦åä½ ({avg_confidence:.1f}%)")
            
            # æª¢æŸ¥ç©©å®šæ€§
            stability_metrics = validation_metrics.get('prediction_stability', {})
            if stability_metrics:
                signal_stability = stability_metrics.get('signal_stability', {}).get('stability_score', 0)
                if signal_stability < 0.6:
                    weaknesses.append(f"ä¿¡è™Ÿç©©å®šæ€§ä¸è¶³ ({signal_stability:.3f})")
            
            # æª¢æŸ¥è¡çªè§£æ±º
            conflict_resolution = validation_metrics.get('conflict_resolution', {})
            conflict_rate = conflict_resolution.get('conflict_rate', 0)
            if conflict_rate > 0.3:
                weaknesses.append(f"æ¨¡å‹è¡çªé »ç‡éé«˜ ({conflict_rate:.1%})")
            
            # æª¢æŸ¥æ¬Šé‡å¹³è¡¡
            weight_effectiveness = validation_metrics.get('weight_effectiveness', {})
            weight_balance = weight_effectiveness.get('weight_balance', {})
            if weight_balance:
                balance_score = weight_balance.get('balance_score', 0)
                if balance_score < 0.5:
                    weaknesses.append(f"æ¬Šé‡é…ç½®ä¸å¹³è¡¡ ({balance_score:.3f})")
            
            return weaknesses
        except Exception as e:
            logger.error(f"âŒ è­˜åˆ¥é›†æˆå¼±é»å¤±æ•—: {e}")
            return []
    
    def _generate_ensemble_recommendations(self, validation_metrics: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆé›†æˆæ”¹é€²å»ºè­°"""
        try:
            recommendations = []
            
            # åŸºæ–¼å¼±é»ç”Ÿæˆå»ºè­°
            weaknesses = self._identify_ensemble_weaknesses(validation_metrics)
            
            if any("æˆåŠŸç‡åä½" in w for w in weaknesses):
                recommendations.append("æª¢æŸ¥è¼¸å…¥æ•¸æ“šè³ªé‡å’Œç‰¹å¾µå·¥ç¨‹")
                recommendations.append("è€ƒæ…®æ·»åŠ æ›´å¤šæœ‰æ•ˆçš„åˆ†æçµ„ä»¶")
            
            if any("ä¸€è‡´æ€§ä¸è¶³" in w for w in weaknesses):
                recommendations.append("é‡æ–°è©•ä¼°å„çµ„ä»¶çš„åˆ†æé‚è¼¯")
                recommendations.append("è€ƒæ…®ä½¿ç”¨æ›´ç›¸ä¼¼çš„åˆ†ææ–¹æ³•")
            
            if any("ä¿¡å¿ƒåº¦åä½" in w for w in weaknesses):
                recommendations.append("å„ªåŒ–ä¿¡å¿ƒåº¦è¨ˆç®—æ–¹æ³•")
                recommendations.append("å¢åŠ æ›´å¤šé©—è­‰æŒ‡æ¨™")
            
            if any("ç©©å®šæ€§ä¸è¶³" in w for w in weaknesses):
                recommendations.append("å¢åŠ é æ¸¬çµæœçš„å¹³æ»‘æ©Ÿåˆ¶")
                recommendations.append("è€ƒæ…®ä½¿ç”¨ç§»å‹•å¹³å‡æˆ–å…¶ä»–ç©©å®šåŒ–æŠ€è¡“")
            
            if any("è¡çªé »ç‡éé«˜" in w for w in weaknesses):
                recommendations.append("å„ªåŒ–è¡çªè§£æ±ºæ©Ÿåˆ¶")
                recommendations.append("è€ƒæ…®èª¿æ•´çµ„ä»¶æ¬Šé‡ä»¥æ¸›å°‘åˆ†æ­§")
            
            if any("æ¬Šé‡é…ç½®ä¸å¹³è¡¡" in w for w in weaknesses):
                recommendations.append("ä½¿ç”¨æ¬Šé‡å„ªåŒ–å»ºè­°é‡æ–°é…ç½®æ¬Šé‡")
                recommendations.append("è€ƒæ…®å‹•æ…‹æ¬Šé‡èª¿æ•´æ©Ÿåˆ¶")
            
            # æ¬Šé‡å„ªåŒ–å»ºè­°
            weight_optimization = validation_metrics.get('weight_optimization', {})
            final_recommendation = weight_optimization.get('final_recommendation', {})
            if final_recommendation and 'improvement_potential' in final_recommendation:
                potential = final_recommendation['improvement_potential']
                if potential > 0.05:
                    recommendations.append(f"æ‡‰ç”¨æ¬Šé‡å„ªåŒ–å»ºè­°å¯èƒ½å¸¶ä¾† {potential:.1%} çš„æ€§èƒ½æå‡")
            
            # é€šç”¨å»ºè­°
            if not recommendations:
                recommendations.append("é›†æˆç³»çµ±è¡¨ç¾è‰¯å¥½ï¼Œå¯è€ƒæ…®å¾®èª¿åƒæ•¸é€²ä¸€æ­¥å„ªåŒ–")
            
            return recommendations
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆé›†æˆæ”¹é€²å»ºè­°å¤±æ•—: {e}")
            return []
    
    def get_ensemble_validation_report(self) -> Dict[str, Any]:
        """ç²å–å®Œæ•´çš„é›†æˆé©—è­‰å ±å‘Š"""
        try:
            # é€™å€‹æ–¹æ³•éœ€è¦å…ˆé‹è¡Œvalidate_ensembleæ‰èƒ½ç²å–å ±å‘Š
            return {
                'message': 'è«‹å…ˆé‹è¡Œ validate_ensemble() æ–¹æ³•é€²è¡Œé©—è­‰',
                'usage': 'validation_result = scorer.validate_ensemble(test_data, ground_truth)'
            }
        except Exception as e:
            logger.error(f"âŒ ç²å–é›†æˆé©—è­‰å ±å‘Šå¤±æ•—: {e}")
            return {'error': str(e)}


def create_ensemble_scorer() -> EnsembleScorer:
    """å‰µå»ºé›†æˆè©•åˆ†å™¨å¯¦ä¾‹"""
    return EnsembleScorer()


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(level=logging.INFO)
    
    # å‰µå»ºé›†æˆè©•åˆ†å™¨
    scorer = create_ensemble_scorer()
    
    # æ¸¬è©¦æ•¸æ“š
    test_data = {
        'current_price': 1500000,
        'price_change_1m': 0.5,
        'volume_ratio': 1.1,
        'ai_formatted_data': 'å¸‚å ´å‘ˆç¾ä¸Šæ¼²è¶¨å‹¢'
    }
    
    # æ¸¬è©¦åˆ†æ
    result = scorer.analyze(test_data)
    
    print(f"âœ… é›†æˆåˆ†æçµæœ:")
    print(f"æœ€çµ‚åˆ†æ•¸: {result['final_score']:.1f}")
    print(f"äº¤æ˜“ä¿¡è™Ÿ: {result['signal']}")
    print(f"ä¿¡å¿ƒåº¦: {result['confidence']:.1f}%")
    print(f"æˆåŠŸ: {result['success']}")