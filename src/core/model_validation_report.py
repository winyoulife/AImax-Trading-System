#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹é©—è­‰å ±å‘Šç³»çµ± - çµ±ä¸€çš„æ¨¡å‹æ€§èƒ½æ¯”è¼ƒå’Œé¸æ“‡å»ºè­°
"""

import logging
import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
import base64

# å°å…¥ç›¸é—œæ¨¡å¡Š
try:
    from ..ml.price_predictor import create_lstm_predictor
    from ..ml.ensemble_scorer import create_ensemble_scorer
    from ..data.historical_data_manager import create_historical_data_manager
    MODULES_AVAILABLE = True
except ImportError:
    MODULES_AVAILABLE = False
    print("âš ï¸ éƒ¨åˆ†æ¨¡å¡Šæœªå®Œå…¨å¯ç”¨")

logger = logging.getLogger(__name__)

@dataclass
class ModelPerformanceMetrics:
    """æ¨¡å‹æ€§èƒ½æŒ‡æ¨™"""
    model_name: str
    model_type: str
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    confidence_avg: float
    confidence_std: float
    prediction_count: int
    success_rate: float
    processing_time: float
    memory_usage: float
    stability_score: float
    reliability_score: float
    last_updated: str

@dataclass
class ModelComparisonResult:
    """æ¨¡å‹æ¯”è¼ƒçµæœ"""
    best_overall_model: str
    best_accuracy_model: str
    best_stability_model: str
    best_speed_model: str
    model_rankings: Dict[str, int]
    performance_matrix: Dict[str, Dict[str, float]]
    recommendations: List[str]
    comparison_summary: str

class ModelValidationReportGenerator:
    """æ¨¡å‹é©—è­‰å ±å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str = "AImax/reports"):
        """
        åˆå§‹åŒ–å ±å‘Šç”Ÿæˆå™¨
        
        Args:
            output_dir: å ±å‘Šè¼¸å‡ºç›®éŒ„
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹æ€§èƒ½æ•¸æ“šå­˜å„²
        self.model_metrics = {}
        self.validation_history = []
        self.comparison_results = None
        
        # å ±å‘Šé…ç½®
        self.report_config = {
            'include_charts': True,
            'include_detailed_analysis': True,
            'include_recommendations': True,
            'chart_style': 'seaborn',
            'output_formats': ['json', 'html', 'pdf']
        }
        
        logger.info("ğŸ“Š æ¨¡å‹é©—è­‰å ±å‘Šç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def validate_all_models(self, test_data: List[Dict[str, Any]], 
                                ground_truth: List[str] = None) -> Dict[str, Any]:
        """
        é©—è­‰æ‰€æœ‰å¯ç”¨æ¨¡å‹
        
        Args:
            test_data: æ¸¬è©¦æ•¸æ“š
            ground_truth: çœŸå¯¦æ¨™ç±¤
            
        Returns:
            æ‰€æœ‰æ¨¡å‹çš„é©—è­‰çµæœ
        """
        try:
            logger.info("ğŸ” é–‹å§‹é©—è­‰æ‰€æœ‰æ¨¡å‹...")
            
            if not MODULES_AVAILABLE:
                logger.warning("âš ï¸ ä½¿ç”¨æ¨¡æ“¬æ¨¡å¼é€²è¡Œé©—è­‰ (éƒ¨åˆ†æ¨¡å¡Šä¸å¯ç”¨)")
            
            validation_results = {}
            
            # 1. é©—è­‰LSTMåƒ¹æ ¼é æ¸¬å™¨
            lstm_result = await self._validate_lstm_model(test_data, ground_truth)
            if lstm_result['success']:
                validation_results['lstm_predictor'] = lstm_result
                logger.info("âœ… LSTMæ¨¡å‹é©—è­‰å®Œæˆ")
            else:
                logger.warning(f"âš ï¸ LSTMæ¨¡å‹é©—è­‰å¤±æ•—: {lstm_result.get('error')}")
            
            # 2. é©—è­‰é›†æˆè©•åˆ†å™¨
            ensemble_result = await self._validate_ensemble_model(test_data, ground_truth)
            if ensemble_result['success']:
                validation_results['ensemble_scorer'] = ensemble_result
                logger.info("âœ… é›†æˆè©•åˆ†å™¨é©—è­‰å®Œæˆ")
            else:
                logger.warning(f"âš ï¸ é›†æˆè©•åˆ†å™¨é©—è­‰å¤±æ•—: {ensemble_result.get('error')}")
            
            # 3. é©—è­‰å…¶ä»–å¯ç”¨æ¨¡å‹ (å¦‚æœæœ‰çš„è©±)
            # é€™è£¡å¯ä»¥æ·»åŠ æ›´å¤šæ¨¡å‹çš„é©—è­‰
            
            if not validation_results:
                return {'success': False, 'error': 'No models successfully validated'}
            
            # 4. ç”Ÿæˆæ¨¡å‹æ¯”è¼ƒåˆ†æ
            comparison_analysis = self._generate_model_comparison(validation_results)
            
            # 5. æ›´æ–°æ¨¡å‹æ€§èƒ½æŒ‡æ¨™
            self._update_model_metrics(validation_results)
            
            # 6. ç”Ÿæˆçµ±ä¸€å ±å‘Š
            unified_report = self._generate_unified_report(validation_results, comparison_analysis)
            
            logger.info(f"âœ… æ‰€æœ‰æ¨¡å‹é©—è­‰å®Œæˆ: {len(validation_results)} å€‹æ¨¡å‹")
            
            return {
                'success': True,
                'validation_results': validation_results,
                'comparison_analysis': comparison_analysis,
                'unified_report': unified_report,
                'models_validated': list(validation_results.keys()),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹é©—è­‰å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}
    
    async def _validate_lstm_model(self, test_data: List[Dict[str, Any]], 
                                 ground_truth: List[str] = None) -> Dict[str, Any]:
        """é©—è­‰LSTMæ¨¡å‹"""
        try:
            if not MODULES_AVAILABLE:
                # æ¨¡æ“¬LSTMæ¨¡å‹é©—è­‰çµæœ
                return self._simulate_lstm_validation(test_data, ground_truth)
            
            # å‰µå»ºLSTMé æ¸¬å™¨
            lstm_predictor = create_lstm_predictor()
            
            # æº–å‚™LSTMæ¸¬è©¦æ•¸æ“š
            lstm_test_data = []
            for data in test_data:
                # è½‰æ›ç‚ºLSTMæ‰€éœ€çš„æ ¼å¼
                lstm_data = {
                    'prices': [data.get('current_price', 1500000)],
                    'volumes': [data.get('volume', 1000)],
                    'timestamps': [datetime.now()]
                }
                lstm_test_data.append(lstm_data)
            
            # åŸ·è¡ŒLSTMé©—è­‰
            start_time = datetime.now()
            predictions = []
            confidences = []
            
            for data in lstm_test_data:
                try:
                    # èª¿ç”¨LSTMé æ¸¬
                    if hasattr(lstm_predictor, 'validate_model'):
                        result = await lstm_predictor.validate_model(data)
                    else:
                        # å¦‚æœæ²’æœ‰é©—è­‰æ–¹æ³•ï¼Œä½¿ç”¨é æ¸¬æ–¹æ³•
                        result = await lstm_predictor.predict_price_trend(data)
                    
                    if result.get('success'):
                        predictions.append(result.get('prediction', 'HOLD'))
                        confidences.append(result.get('confidence', 0.5))
                    else:
                        predictions.append('HOLD')
                        confidences.append(0.0)
                        
                except Exception as e:
                    logger.warning(f"LSTMé æ¸¬å¤±æ•—: {e}")
                    predictions.append('HOLD')
                    confidences.append(0.0)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
            metrics = self._calculate_model_metrics(
                'LSTM Price Predictor', 'deep_learning',
                predictions, ground_truth, confidences, processing_time
            )
            
            return {
                'success': True,
                'model_name': 'LSTM Price Predictor',
                'model_type': 'deep_learning',
                'predictions': predictions,
                'confidences': confidences,
                'metrics': metrics,
                'processing_time': processing_time,
                'validation_details': {
                    'test_samples': len(test_data),
                    'successful_predictions': len([p for p in predictions if p != 'HOLD']),
                    'average_confidence': np.mean(confidences) if confidences else 0.0
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ LSTMæ¨¡å‹é©—è­‰å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}
    
    def _simulate_lstm_validation(self, test_data: List[Dict[str, Any]], 
                                ground_truth: List[str] = None) -> Dict[str, Any]:
        """æ¨¡æ“¬LSTMæ¨¡å‹é©—è­‰çµæœ"""
        try:
            logger.info("ğŸ”„ ä½¿ç”¨æ¨¡æ“¬LSTMæ¨¡å‹é€²è¡Œé©—è­‰...")
            
            predictions = []
            confidences = []
            
            # æ¨¡æ“¬LSTMé æ¸¬é‚è¼¯
            for data in test_data:
                price_change = data.get('price_change_1m', 0)
                
                # åŸºæ–¼åƒ¹æ ¼è®ŠåŒ–çš„ç°¡å–®é æ¸¬é‚è¼¯
                if price_change > 1.5:
                    pred = 'BUY'
                    conf = min(0.9, 0.6 + abs(price_change) * 0.1)
                elif price_change < -1.5:
                    pred = 'SELL'
                    conf = min(0.9, 0.6 + abs(price_change) * 0.1)
                else:
                    pred = 'HOLD'
                    conf = 0.5 + np.random.normal(0, 0.1)
                
                # æ·»åŠ ä¸€äº›éš¨æ©Ÿæ€§
                if np.random.random() < 0.1:  # 10%çš„éš¨æ©Ÿæ€§
                    pred = np.random.choice(['BUY', 'SELL', 'HOLD'])
                    conf = np.random.uniform(0.3, 0.8)
                
                predictions.append(pred)
                confidences.append(max(0.1, min(0.95, conf)))
            
            processing_time = len(test_data) * 0.02  # æ¨¡æ“¬è™•ç†æ™‚é–“
            
            # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
            metrics = self._calculate_model_metrics(
                'LSTM Price Predictor (Simulated)', 'deep_learning',
                predictions, ground_truth, confidences, processing_time
            )
            
            return {
                'success': True,
                'model_name': 'LSTM Price Predictor (Simulated)',
                'model_type': 'deep_learning',
                'predictions': predictions,
                'confidences': confidences,
                'metrics': metrics,
                'processing_time': processing_time,
                'validation_details': {
                    'test_samples': len(test_data),
                    'successful_predictions': len([p for p in predictions if p != 'HOLD']),
                    'average_confidence': np.mean(confidences) if confidences else 0.0,
                    'simulation_note': 'This is a simulated LSTM model for testing purposes'
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡æ“¬LSTMé©—è­‰å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}
    
    async def _validate_ensemble_model(self, test_data: List[Dict[str, Any]], 
                                     ground_truth: List[str] = None) -> Dict[str, Any]:
        """é©—è­‰é›†æˆè©•åˆ†å™¨æ¨¡å‹"""
        try:
            if not MODULES_AVAILABLE:
                # æ¨¡æ“¬é›†æˆè©•åˆ†å™¨é©—è­‰çµæœ
                return self._simulate_ensemble_validation(test_data, ground_truth)
            
            # å‰µå»ºé›†æˆè©•åˆ†å™¨
            ensemble_scorer = create_ensemble_scorer()
            
            # åŸ·è¡Œé›†æˆè©•åˆ†å™¨é©—è­‰
            start_time = datetime.now()
            
            # ä½¿ç”¨é›†æˆè©•åˆ†å™¨çš„å…§å»ºé©—è­‰æ–¹æ³•
            validation_result = ensemble_scorer.validate_ensemble(test_data, ground_truth)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            if not validation_result['success']:
                return {'success': False, 'error': validation_result.get('error')}
            
            # æå–é—œéµæŒ‡æ¨™
            validation_metrics = validation_result['validation_metrics']
            
            # ç”Ÿæˆé æ¸¬çµæœ
            predictions = []
            confidences = []
            
            for data in test_data:
                result = ensemble_scorer.analyze(data)
                if result['success']:
                    predictions.append(result['signal'])
                    confidences.append(result['confidence'])
                else:
                    predictions.append('HOLD')
                    confidences.append(0.0)
            
            # è¨ˆç®—çµ±ä¸€çš„æ€§èƒ½æŒ‡æ¨™
            metrics = self._calculate_model_metrics(
                'Ensemble Scorer', 'ensemble',
                predictions, ground_truth, confidences, processing_time
            )
            
            # æ·»åŠ é›†æˆç‰¹å®šæŒ‡æ¨™
            metrics.update({
                'model_agreement': validation_metrics.get('model_agreement', 0),
                'weight_effectiveness': validation_metrics.get('weight_effectiveness', {}).get('effectiveness_score', 0),
                'reliability_score': validation_metrics.get('reliability_assessment', {}).get('overall_score', 0)
            })
            
            return {
                'success': True,
                'model_name': 'Ensemble Scorer',
                'model_type': 'ensemble',
                'predictions': predictions,
                'confidences': confidences,
                'metrics': metrics,
                'processing_time': processing_time,
                'validation_details': validation_metrics,
                'ensemble_specific': {
                    'component_count': 3,
                    'weight_optimization': validation_metrics.get('weight_optimization', {}),
                    'conflict_resolution': validation_metrics.get('conflict_resolution', {})
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ é›†æˆè©•åˆ†å™¨é©—è­‰å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}
    
    def _simulate_ensemble_validation(self, test_data: List[Dict[str, Any]], 
                                    ground_truth: List[str] = None) -> Dict[str, Any]:
        """æ¨¡æ“¬é›†æˆè©•åˆ†å™¨é©—è­‰çµæœ"""
        try:
            logger.info("ğŸ”„ ä½¿ç”¨æ¨¡æ“¬é›†æˆè©•åˆ†å™¨é€²è¡Œé©—è­‰...")
            
            predictions = []
            confidences = []
            
            # æ¨¡æ“¬é›†æˆè©•åˆ†å™¨é æ¸¬é‚è¼¯
            for data in test_data:
                price_change = data.get('price_change_1m', 0)
                volume_ratio = data.get('volume_ratio', 1.0)
                ai_data = data.get('ai_formatted_data', '')
                
                # å¤šå› å­ç¶œåˆè©•åˆ†
                score = 50  # åŸºæº–åˆ†æ•¸
                
                # åƒ¹æ ¼è®ŠåŒ–å› å­
                if price_change > 1:
                    score += min(15, price_change * 5)
                elif price_change < -1:
                    score -= min(15, abs(price_change) * 5)
                
                # æˆäº¤é‡å› å­
                if volume_ratio > 1.2:
                    score += 5
                elif volume_ratio < 0.8:
                    score -= 3
                
                # AIæ•¸æ“šå› å­
                if 'ä¸Šæ¼²' in ai_data or 'çœ‹æ¼²' in ai_data:
                    score += 8
                elif 'ä¸‹è·Œ' in ai_data or 'çœ‹è·Œ' in ai_data:
                    score -= 8
                
                # ç”Ÿæˆé æ¸¬å’Œä¿¡å¿ƒåº¦
                if score > 60:
                    pred = 'BUY'
                    conf = min(0.95, 0.6 + (score - 60) / 100)
                elif score < 40:
                    pred = 'SELL'
                    conf = min(0.95, 0.6 + (40 - score) / 100)
                else:
                    pred = 'HOLD'
                    conf = 0.5 + np.random.normal(0, 0.1)
                
                # æ·»åŠ ä¸€äº›éš¨æ©Ÿæ€§
                if np.random.random() < 0.05:  # 5%çš„éš¨æ©Ÿæ€§
                    pred = np.random.choice(['BUY', 'SELL', 'HOLD'])
                    conf = np.random.uniform(0.4, 0.9)
                
                predictions.append(pred)
                confidences.append(max(0.1, min(0.95, conf)))
            
            processing_time = len(test_data) * 0.03  # æ¨¡æ“¬è™•ç†æ™‚é–“
            
            # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
            metrics = self._calculate_model_metrics(
                'Ensemble Scorer (Simulated)', 'ensemble',
                predictions, ground_truth, confidences, processing_time
            )
            
            # æ·»åŠ æ¨¡æ“¬çš„é›†æˆç‰¹å®šæŒ‡æ¨™
            metrics.update({
                'model_agreement': 0.85 + np.random.normal(0, 0.05),
                'weight_effectiveness': 0.75 + np.random.normal(0, 0.1),
                'reliability_score': 0.80 + np.random.normal(0, 0.08)
            })
            
            # æ¨¡æ“¬é©—è­‰è©³æƒ…
            validation_details = {
                'prediction_count': len(predictions),
                'success_rate': 1.0,
                'model_agreement': metrics['model_agreement'],
                'simulation_note': 'This is a simulated ensemble scorer for testing purposes'
            }
            
            return {
                'success': True,
                'model_name': 'Ensemble Scorer (Simulated)',
                'model_type': 'ensemble',
                'predictions': predictions,
                'confidences': confidences,
                'metrics': metrics,
                'processing_time': processing_time,
                'validation_details': validation_details,
                'ensemble_specific': {
                    'component_count': 3,
                    'weight_optimization': {'simulated': True},
                    'conflict_resolution': {'simulated': True}
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡æ“¬é›†æˆè©•åˆ†å™¨é©—è­‰å¤±æ•—: {e}")
            return {'success': False, 'error': str(e)}
    
    def _calculate_model_metrics(self, model_name: str, model_type: str,
                               predictions: List[str], ground_truth: List[str] = None,
                               confidences: List[float] = None,
                               processing_time: float = 0.0) -> Dict[str, Any]:
        """è¨ˆç®—çµ±ä¸€çš„æ¨¡å‹æ€§èƒ½æŒ‡æ¨™"""
        try:
            metrics = {
                'model_name': model_name,
                'model_type': model_type,
                'prediction_count': len(predictions),
                'processing_time': processing_time,
                'predictions_per_second': len(predictions) / max(processing_time, 0.001)
            }
            
            # ä¿¡å¿ƒåº¦çµ±è¨ˆ
            if confidences:
                metrics.update({
                    'confidence_avg': np.mean(confidences),
                    'confidence_std': np.std(confidences),
                    'confidence_min': np.min(confidences),
                    'confidence_max': np.max(confidences),
                    'high_confidence_ratio': sum(1 for c in confidences if c > 0.8) / len(confidences)
                })
            else:
                metrics.update({
                    'confidence_avg': 0.0,
                    'confidence_std': 0.0,
                    'confidence_min': 0.0,
                    'confidence_max': 0.0,
                    'high_confidence_ratio': 0.0
                })
            
            # é æ¸¬åˆ†ä½ˆ
            prediction_counts = {}
            for pred in predictions:
                prediction_counts[pred] = prediction_counts.get(pred, 0) + 1
            
            metrics['prediction_distribution'] = prediction_counts
            metrics['prediction_diversity'] = len(prediction_counts)
            
            # å¦‚æœæœ‰çœŸå¯¦æ¨™ç±¤ï¼Œè¨ˆç®—æº–ç¢ºæ€§æŒ‡æ¨™
            if ground_truth and len(ground_truth) == len(predictions):
                accuracy_metrics = self._calculate_accuracy_metrics(predictions, ground_truth)
                metrics.update(accuracy_metrics)
            else:
                # é»˜èªæº–ç¢ºæ€§æŒ‡æ¨™
                metrics.update({
                    'accuracy': 0.0,
                    'precision': 0.0,
                    'recall': 0.0,
                    'f1_score': 0.0,
                    'macro_f1': 0.0
                })
            
            # ç©©å®šæ€§è©•ä¼°
            stability_score = self._calculate_stability_score(predictions, confidences)
            metrics['stability_score'] = stability_score
            
            # æˆåŠŸç‡
            successful_predictions = sum(1 for p in predictions if p != 'HOLD')
            metrics['success_rate'] = successful_predictions / len(predictions) if predictions else 0.0
            
            # å…§å­˜ä½¿ç”¨ä¼°ç®— (ç°¡åŒ–)
            metrics['memory_usage_mb'] = len(predictions) * 0.1  # ä¼°ç®—å€¼
            
            return metrics
            
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—æ¨¡å‹æŒ‡æ¨™å¤±æ•—: {e}")
            return {}
    
    def _calculate_accuracy_metrics(self, predictions: List[str], 
                                  ground_truth: List[str]) -> Dict[str, float]:
        """è¨ˆç®—æº–ç¢ºæ€§æŒ‡æ¨™"""
        try:
            if len(predictions) != len(ground_truth):
                return {}
            
            # ç¸½é«”æº–ç¢ºç‡
            correct = sum(1 for p, g in zip(predictions, ground_truth) if p == g)
            accuracy = correct / len(predictions)
            
            # å„é¡åˆ¥æŒ‡æ¨™
            unique_labels = list(set(predictions + ground_truth))
            class_metrics = {}
            
            for label in unique_labels:
                tp = sum(1 for p, g in zip(predictions, ground_truth) if p == label and g == label)
                fp = sum(1 for p, g in zip(predictions, ground_truth) if p == label and g != label)
                fn = sum(1 for p, g in zip(predictions, ground_truth) if p != label and g == label)
                
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
                
                class_metrics[label] = {
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1,
                    'support': sum(1 for g in ground_truth if g == label)
                }
            
            # å®å¹³å‡
            macro_precision = np.mean([m['precision'] for m in class_metrics.values()])
            macro_recall = np.mean([m['recall'] for m in class_metrics.values()])
            macro_f1 = np.mean([m['f1_score'] for m in class_metrics.values()])
            
            return {
                'accuracy': accuracy,
                'precision': macro_precision,
                'recall': macro_recall,
                'f1_score': macro_f1,
                'macro_f1': macro_f1,
                'class_metrics': class_metrics,
                'correct_predictions': correct,
                'total_predictions': len(predictions)
            }
            
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—æº–ç¢ºæ€§æŒ‡æ¨™å¤±æ•—: {e}")
            return {}
    
    def _calculate_stability_score(self, predictions: List[str], 
                                 confidences: List[float] = None) -> float:
        """è¨ˆç®—ç©©å®šæ€§åˆ†æ•¸"""
        try:
            if len(predictions) < 2:
                return 1.0
            
            # é æ¸¬è®ŠåŒ–ç©©å®šæ€§
            prediction_changes = sum(1 for i in range(1, len(predictions)) 
                                   if predictions[i] != predictions[i-1])
            prediction_stability = 1.0 - (prediction_changes / (len(predictions) - 1))
            
            # ä¿¡å¿ƒåº¦ç©©å®šæ€§
            confidence_stability = 1.0
            if confidences and len(confidences) > 1:
                confidence_changes = [abs(confidences[i] - confidences[i-1]) 
                                    for i in range(1, len(confidences))]
                avg_change = np.mean(confidence_changes)
                confidence_stability = 1.0 / (1.0 + avg_change)
            
            # ç¶œåˆç©©å®šæ€§åˆ†æ•¸
            stability_score = (prediction_stability + confidence_stability) / 2
            
            return float(stability_score)
            
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—ç©©å®šæ€§åˆ†æ•¸å¤±æ•—: {e}")
            return 0.5
    
    def _generate_model_comparison(self, validation_results: Dict[str, Any]) -> ModelComparisonResult:
        """ç”Ÿæˆæ¨¡å‹æ¯”è¼ƒåˆ†æ"""
        try:
            logger.info("ğŸ“Š ç”Ÿæˆæ¨¡å‹æ¯”è¼ƒåˆ†æ...")
            
            if len(validation_results) < 2:
                logger.warning("âš ï¸ æ¨¡å‹æ•¸é‡ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œæœ‰æ•ˆæ¯”è¼ƒ")
                return ModelComparisonResult(
                    best_overall_model=list(validation_results.keys())[0] if validation_results else 'none',
                    best_accuracy_model='none',
                    best_stability_model='none',
                    best_speed_model='none',
                    model_rankings={},
                    performance_matrix={},
                    recommendations=['éœ€è¦æ›´å¤šæ¨¡å‹é€²è¡Œæ¯”è¼ƒ'],
                    comparison_summary='æ¨¡å‹ï¿½ï¿½é‡ä¸è¶³'
                )
            
            # æå–æ‰€æœ‰æ¨¡å‹çš„é—œéµæŒ‡æ¨™
            models_data = {}
            for model_name, result in validation_results.items():
                if result['success']:
                    metrics = result['metrics']
                    models_data[model_name] = {
                        'accuracy': metrics.get('accuracy', 0),
                        'f1_score': metrics.get('f1_score', 0),
                        'confidence_avg': metrics.get('confidence_avg', 0),
                        'stability_score': metrics.get('stability_score', 0),
                        'processing_time': metrics.get('processing_time', float('inf')),
                        'predictions_per_second': metrics.get('predictions_per_second', 0),
                        'success_rate': metrics.get('success_rate', 0),
                        'reliability_score': metrics.get('reliability_score', 0)
                    }
            
            if not models_data:
                return ModelComparisonResult(
                    best_overall_model='none',
                    best_accuracy_model='none',
                    best_stability_model='none',
                    best_speed_model='none',
                    model_rankings={},
                    performance_matrix={},
                    recommendations=['æ‰€æœ‰æ¨¡å‹é©—è­‰å¤±æ•—'],
                    comparison_summary='ç„¡æœ‰æ•ˆæ¨¡å‹æ•¸æ“š'
                )
            
            # æ‰¾å‡ºå„é …æœ€ä½³æ¨¡å‹
            best_accuracy_model = max(models_data.keys(), 
                                    key=lambda x: models_data[x]['accuracy'])
            best_stability_model = max(models_data.keys(), 
                                     key=lambda x: models_data[x]['stability_score'])
            best_speed_model = max(models_data.keys(), 
                                 key=lambda x: models_data[x]['predictions_per_second'])
            
            # è¨ˆç®—ç¶œåˆè©•åˆ†
            overall_scores = {}
            for model_name, data in models_data.items():
                # ç¶œåˆè©•åˆ†æ¬Šé‡
                weights = {
                    'accuracy': 0.25,
                    'f1_score': 0.20,
                    'confidence_avg': 0.15,
                    'stability_score': 0.15,
                    'success_rate': 0.15,
                    'speed_score': 0.10  # åŸºæ–¼predictions_per_secondçš„æ¨™æº–åŒ–åˆ†æ•¸
                }
                
                # æ¨™æº–åŒ–é€Ÿåº¦åˆ†æ•¸
                max_speed = max(d['predictions_per_second'] for d in models_data.values())
                speed_score = data['predictions_per_second'] / max_speed if max_speed > 0 else 0
                
                # è¨ˆç®—åŠ æ¬Šç¸½åˆ†
                overall_score = (
                    data['accuracy'] * weights['accuracy'] +
                    data['f1_score'] * weights['f1_score'] +
                    data['confidence_avg'] * weights['confidence_avg'] +
                    data['stability_score'] * weights['stability_score'] +
                    data['success_rate'] * weights['success_rate'] +
                    speed_score * weights['speed_score']
                )
                
                overall_scores[model_name] = overall_score
            
            best_overall_model = max(overall_scores.keys(), key=lambda x: overall_scores[x])
            
            # ç”Ÿæˆæ’å
            model_rankings = {}
            sorted_models = sorted(overall_scores.items(), key=lambda x: x[1], reverse=True)
            for rank, (model_name, score) in enumerate(sorted_models, 1):
                model_rankings[model_name] = rank
            
            # ç”Ÿæˆæ€§èƒ½çŸ©é™£
            performance_matrix = models_data.copy()
            for model_name in performance_matrix:
                performance_matrix[model_name]['overall_score'] = overall_scores[model_name]
            
            # ç”Ÿæˆå»ºè­°
            recommendations = self._generate_model_recommendations(
                models_data, best_overall_model, best_accuracy_model, 
                best_stability_model, best_speed_model
            )
            
            # ç”Ÿæˆæ¯”è¼ƒæ‘˜è¦
            comparison_summary = self._generate_comparison_summary(
                models_data, best_overall_model, model_rankings
            )
            
            return ModelComparisonResult(
                best_overall_model=best_overall_model,
                best_accuracy_model=best_accuracy_model,
                best_stability_model=best_stability_model,
                best_speed_model=best_speed_model,
                model_rankings=model_rankings,
                performance_matrix=performance_matrix,
                recommendations=recommendations,
                comparison_summary=comparison_summary
            )
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ¨¡å‹æ¯”è¼ƒåˆ†æå¤±æ•—: {e}")
            return ModelComparisonResult(
                best_overall_model='error',
                best_accuracy_model='error',
                best_stability_model='error',
                best_speed_model='error',
                model_rankings={},
                performance_matrix={},
                recommendations=[f'æ¯”è¼ƒåˆ†æå¤±æ•—: {e}'],
                comparison_summary='åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤'
            )
    
    def _generate_model_recommendations(self, models_data: Dict[str, Dict[str, float]],
                                      best_overall: str, best_accuracy: str,
                                      best_stability: str, best_speed: str) -> List[str]:
        """ç”Ÿæˆæ¨¡å‹é¸æ“‡å»ºè­°"""
        try:
            recommendations = []
            
            # ç¸½é«”æœ€ä½³æ¨¡å‹å»ºè­°
            overall_score = models_data[best_overall]['accuracy']
            if overall_score > 0.8:
                recommendations.append(f"ğŸ† æ¨è–¦ä½¿ç”¨ {best_overall} ä½œç‚ºä¸»è¦æ¨¡å‹ (ç¶œåˆè©•åˆ†æœ€é«˜)")
            elif overall_score > 0.6:
                recommendations.append(f"âœ… {best_overall} è¡¨ç¾è‰¯å¥½ï¼Œå¯ä½œç‚ºä¸»è¦æ¨¡å‹")
            else:
                recommendations.append(f"âš ï¸ {best_overall} è¡¨ç¾ä¸€èˆ¬ï¼Œå»ºè­°é€²ä¸€æ­¥å„ªåŒ–")
            
            # ç‰¹å®šå ´æ™¯å»ºè­°
            if best_accuracy != best_overall:
                acc_score = models_data[best_accuracy]['accuracy']
                if acc_score > models_data[best_overall]['accuracy'] + 0.1:
                    recommendations.append(f"ğŸ¯ å°æ–¼æº–ç¢ºæ€§è¦æ±‚é«˜çš„å ´æ™¯ï¼Œå»ºè­°ä½¿ç”¨ {best_accuracy}")
            
            if best_stability != best_overall:
                stab_score = models_data[best_stability]['stability_score']
                if stab_score > models_data[best_overall]['stability_score'] + 0.1:
                    recommendations.append(f"ğŸ”’ å°æ–¼ç©©å®šæ€§è¦æ±‚é«˜çš„å ´æ™¯ï¼Œå»ºè­°ä½¿ç”¨ {best_stability}")
            
            if best_speed != best_overall:
                speed_score = models_data[best_speed]['predictions_per_second']
                if speed_score > models_data[best_overall]['predictions_per_second'] * 1.5:
                    recommendations.append(f"âš¡ å°æ–¼å¯¦æ™‚æ€§è¦æ±‚é«˜çš„å ´æ™¯ï¼Œå»ºè­°ä½¿ç”¨ {best_speed}")
            
            # æ¨¡å‹çµ„åˆå»ºè­°
            if len(models_data) >= 2:
                top_models = sorted(models_data.items(), 
                                  key=lambda x: x[1]['accuracy'] + x[1]['stability_score'], 
                                  reverse=True)[:2]
                recommendations.append(f"ğŸ”„ è€ƒæ…®çµ„åˆä½¿ç”¨ {top_models[0][0]} å’Œ {top_models[1][0]} ä»¥æå‡æ•´é«”æ€§èƒ½")
            
            # æ”¹é€²å»ºè­°
            for model_name, data in models_data.items():
                if data['accuracy'] < 0.5:
                    recommendations.append(f"ğŸ“ˆ {model_name} æº–ç¢ºç‡åä½ï¼Œå»ºè­°é‡æ–°è¨“ç·´æˆ–èª¿æ•´åƒæ•¸")
                if data['stability_score'] < 0.6:
                    recommendations.append(f"âš–ï¸ {model_name} ç©©å®šæ€§ä¸è¶³ï¼Œå»ºè­°å¢åŠ æ•¸æ“šå¹³æ»‘è™•ç†")
                if data['confidence_avg'] < 0.6:
                    recommendations.append(f"ğŸ¯ {model_name} ä¿¡å¿ƒåº¦åä½ï¼Œå»ºè­°å„ªåŒ–ä¿¡å¿ƒåº¦è¨ˆç®—æ–¹æ³•")
            
            return recommendations
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ¨¡å‹å»ºè­°å¤±æ•—: {e}")
            return [f"å»ºè­°ç”Ÿæˆå¤±æ•—: {e}"]
    
    def _generate_comparison_summary(self, models_data: Dict[str, Dict[str, float]],
                                   best_overall: str, model_rankings: Dict[str, int]) -> str:
        """ç”Ÿæˆæ¯”è¼ƒæ‘˜è¦"""
        try:
            summary_parts = []
            
            # åŸºæœ¬ä¿¡æ¯
            model_count = len(models_data)
            summary_parts.append(f"æœ¬æ¬¡æ¯”è¼ƒæ¶µè“‹ {model_count} å€‹æ¨¡å‹")
            
            # æœ€ä½³æ¨¡å‹ä¿¡æ¯
            best_data = models_data[best_overall]
            summary_parts.append(f"ç¶œåˆè¡¨ç¾æœ€ä½³çš„æ¨¡å‹æ˜¯ {best_overall}")
            summary_parts.append(f"å…¶æº–ç¢ºç‡ç‚º {best_data['accuracy']:.2%}ï¼Œç©©å®šæ€§åˆ†æ•¸ç‚º {best_data['stability_score']:.3f}")
            
            # æ€§èƒ½åˆ†ä½ˆ
            accuracies = [data['accuracy'] for data in models_data.values()]
            avg_accuracy = np.mean(accuracies)
            summary_parts.append(f"æ‰€æœ‰æ¨¡å‹çš„å¹³å‡æº–ç¢ºç‡ç‚º {avg_accuracy:.2%}")
            
            # æ€§èƒ½å·®ç•°
            max_accuracy = max(accuracies)
            min_accuracy = min(accuracies)
            if max_accuracy - min_accuracy > 0.2:
                summary_parts.append("æ¨¡å‹é–“æ€§èƒ½å·®ç•°è¼ƒå¤§ï¼Œå»ºè­°é¸æ“‡æ€§èƒ½è¼ƒå¥½çš„æ¨¡å‹")
            else:
                summary_parts.append("æ¨¡å‹é–“æ€§èƒ½å·®ç•°è¼ƒå°ï¼Œå¯æ ¹æ“šå…·é«”éœ€æ±‚é¸æ“‡")
            
            # æ’åä¿¡æ¯
            ranking_text = "æ¨¡å‹æ’å: " + ", ".join([f"{model}(ç¬¬{rank}å)" 
                                                  for model, rank in sorted(model_rankings.items(), 
                                                                          key=lambda x: x[1])])
            summary_parts.append(ranking_text)
            
            return " | ".join(summary_parts)
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ¯”è¼ƒæ‘˜è¦å¤±æ•—: {e}")
            return f"æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}"
    
    def _update_model_metrics(self, validation_results: Dict[str, Any]):
        """æ›´æ–°æ¨¡å‹æ€§èƒ½æŒ‡æ¨™å­˜å„²"""
        try:
            for model_name, result in validation_results.items():
                if result['success']:
                    metrics = result['metrics']
                    
                    # å‰µå»ºæ€§èƒ½æŒ‡æ¨™å°è±¡
                    performance_metrics = ModelPerformanceMetrics(
                        model_name=model_name,
                        model_type=result.get('model_type', 'unknown'),
                        accuracy=metrics.get('accuracy', 0),
                        precision=metrics.get('precision', 0),
                        recall=metrics.get('recall', 0),
                        f1_score=metrics.get('f1_score', 0),
                        confidence_avg=metrics.get('confidence_avg', 0),
                        confidence_std=metrics.get('confidence_std', 0),
                        prediction_count=metrics.get('prediction_count', 0),
                        success_rate=metrics.get('success_rate', 0),
                        processing_time=metrics.get('processing_time', 0),
                        memory_usage=metrics.get('memory_usage_mb', 0),
                        stability_score=metrics.get('stability_score', 0),
                        reliability_score=metrics.get('reliability_score', 0),
                        last_updated=datetime.now().isoformat()
                    )
                    
                    self.model_metrics[model_name] = performance_metrics
            
            # æ·»åŠ åˆ°æ­·å²è¨˜éŒ„
            self.validation_history.append({
                'timestamp': datetime.now().isoformat(),
                'models_validated': list(validation_results.keys()),
                'validation_summary': {
                    model: result['success'] for model, result in validation_results.items()
                }
            })
            
            logger.info(f"âœ… æ¨¡å‹æ€§èƒ½æŒ‡æ¨™å·²æ›´æ–°: {len(self.model_metrics)} å€‹æ¨¡å‹")
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ¨¡å‹æŒ‡æ¨™å¤±æ•—: {e}")
    
    def _generate_unified_report(self, validation_results: Dict[str, Any], 
                               comparison_analysis: ModelComparisonResult) -> Dict[str, Any]:
        """ç”Ÿæˆçµ±ä¸€å ±å‘Š"""
        try:
            # å ±å‘ŠåŸºæœ¬ä¿¡æ¯
            report = {
                'report_metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'report_version': '1.0',
                    'models_evaluated': len(validation_results),
                    'successful_validations': sum(1 for r in validation_results.values() if r['success']),
                    'generator': 'AImax Model Validation Report System'
                },
                
                # åŸ·è¡Œæ‘˜è¦
                'executive_summary': {
                    'best_overall_model': comparison_analysis.best_overall_model,
                    'model_count': len(validation_results),
                    'average_accuracy': np.mean([r['metrics'].get('accuracy', 0) 
                                               for r in validation_results.values() if r['success']]),
                    'top_recommendation': comparison_analysis.recommendations[0] if comparison_analysis.recommendations else 'No recommendations',
                    'comparison_summary': comparison_analysis.comparison_summary
                },
                
                # è©³ç´°é©—è­‰çµæœ
                'detailed_results': {},
                
                # æ¨¡å‹æ¯”è¼ƒåˆ†æ
                'comparison_analysis': asdict(comparison_analysis),
                
                # æ€§èƒ½çŸ©é™£
                'performance_matrix': comparison_analysis.performance_matrix,
                
                # å»ºè­°å’Œçµè«–
                'recommendations': {
                    'primary_recommendations': comparison_analysis.recommendations,
                    'model_selection_guide': self._generate_selection_guide(comparison_analysis),
                    'optimization_suggestions': self._generate_optimization_suggestions(validation_results)
                },
                
                # çµ±è¨ˆæ‘˜è¦
                'statistical_summary': self._generate_statistical_summary(validation_results),
                
                # åœ–è¡¨æ•¸æ“š (å¦‚æœå•Ÿç”¨)
                'charts': self._generate_chart_data(validation_results, comparison_analysis) if self.report_config['include_charts'] else None
            }
            
            # æ·»åŠ è©³ç´°çµæœ
            for model_name, result in validation_results.items():
                if result['success']:
                    report['detailed_results'][model_name] = {
                        'model_type': result.get('model_type', 'unknown'),
                        'metrics': result['metrics'],
                        'validation_details': result.get('validation_details', {}),
                        'processing_info': {
                            'processing_time': result.get('processing_time', 0),
                            'predictions_count': len(result.get('predictions', [])),
                            'average_confidence': np.mean(result.get('confidences', [0]))
                        }
                    }
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆçµ±ä¸€å ±å‘Šå¤±æ•—: {e}")
            return {'error': str(e)}
    
    def _generate_selection_guide(self, comparison_analysis: ModelComparisonResult) -> Dict[str, str]:
        """ç”Ÿæˆæ¨¡å‹é¸æ“‡æŒ‡å—"""
        try:
            guide = {}
            
            # ä¸åŒå ´æ™¯çš„æ¨¡å‹é¸æ“‡å»ºè­°
            guide['high_accuracy_scenario'] = f"éœ€è¦é«˜æº–ç¢ºç‡æ™‚é¸æ“‡: {comparison_analysis.best_accuracy_model}"
            guide['high_stability_scenario'] = f"éœ€è¦é«˜ç©©å®šæ€§æ™‚é¸æ“‡: {comparison_analysis.best_stability_model}"
            guide['high_speed_scenario'] = f"éœ€è¦é«˜é€Ÿåº¦æ™‚é¸æ“‡: {comparison_analysis.best_speed_model}"
            guide['balanced_scenario'] = f"å¹³è¡¡è€ƒæ…®æ™‚é¸æ“‡: {comparison_analysis.best_overall_model}"
            
            # æ¨¡å‹çµ„åˆå»ºè­°
            if len(comparison_analysis.model_rankings) >= 2:
                top_2_models = sorted(comparison_analysis.model_rankings.items(), key=lambda x: x[1])[:2]
                guide['ensemble_recommendation'] = f"é›†æˆä½¿ç”¨: {top_2_models[0][0]} + {top_2_models[1][0]}"
            
            return guide
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆé¸æ“‡æŒ‡å—å¤±æ•—: {e}")
            return {}
    
    def _generate_optimization_suggestions(self, validation_results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        try:
            suggestions = []
            
            for model_name, result in validation_results.items():
                if not result['success']:
                    suggestions.append(f"{model_name}: ä¿®å¾©é©—è­‰å¤±æ•—å•é¡Œ")
                    continue
                
                metrics = result['metrics']
                
                # æº–ç¢ºç‡å„ªåŒ–å»ºè­°
                if metrics.get('accuracy', 0) < 0.6:
                    suggestions.append(f"{model_name}: æå‡æº–ç¢ºç‡ - è€ƒæ…®å¢åŠ è¨“ç·´æ•¸æ“šæˆ–èª¿æ•´æ¨¡å‹åƒæ•¸")
                
                # ç©©å®šæ€§å„ªåŒ–å»ºè­°
                if metrics.get('stability_score', 0) < 0.7:
                    suggestions.append(f"{model_name}: æå‡ç©©å®šæ€§ - è€ƒæ…®å¢åŠ æ•¸æ“šå¹³æ»‘æˆ–æ­£å‰‡åŒ–")
                
                # ä¿¡å¿ƒåº¦å„ªåŒ–å»ºè­°
                if metrics.get('confidence_avg', 0) < 0.6:
                    suggestions.append(f"{model_name}: æå‡ä¿¡å¿ƒåº¦ - å„ªåŒ–ä¿¡å¿ƒåº¦è¨ˆç®—æ–¹æ³•")
                
                # é€Ÿåº¦å„ªåŒ–å»ºè­°
                if metrics.get('processing_time', 0) > 1.0:
                    suggestions.append(f"{model_name}: æå‡è™•ç†é€Ÿåº¦ - è€ƒæ…®æ¨¡å‹å£“ç¸®æˆ–ä¸¦è¡Œè™•ç†")
            
            # é€šç”¨å»ºè­°
            if len(validation_results) < 3:
                suggestions.append("è€ƒæ…®å¢åŠ æ›´å¤šæ¨¡å‹ä»¥æä¾›æ›´å¤šé¸æ“‡")
            
            return suggestions
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå„ªåŒ–å»ºè­°å¤±æ•—: {e}")
            return []
    
    def _generate_statistical_summary(self, validation_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆçµ±è¨ˆæ‘˜è¦"""
        try:
            successful_results = [r for r in validation_results.values() if r['success']]
            
            if not successful_results:
                return {'error': 'No successful validations'}
            
            # æ”¶é›†æ‰€æœ‰æŒ‡æ¨™
            accuracies = [r['metrics'].get('accuracy', 0) for r in successful_results]
            confidences = [r['metrics'].get('confidence_avg', 0) for r in successful_results]
            stabilities = [r['metrics'].get('stability_score', 0) for r in successful_results]
            processing_times = [r['metrics'].get('processing_time', 0) for r in successful_results]
            
            return {
                'model_count': len(successful_results),
                'accuracy_stats': {
                    'mean': np.mean(accuracies),
                    'std': np.std(accuracies),
                    'min': np.min(accuracies),
                    'max': np.max(accuracies),
                    'median': np.median(accuracies)
                },
                'confidence_stats': {
                    'mean': np.mean(confidences),
                    'std': np.std(confidences),
                    'min': np.min(confidences),
                    'max': np.max(confidences)
                },
                'stability_stats': {
                    'mean': np.mean(stabilities),
                    'std': np.std(stabilities),
                    'min': np.min(stabilities),
                    'max': np.max(stabilities)
                },
                'performance_stats': {
                    'avg_processing_time': np.mean(processing_times),
                    'total_processing_time': np.sum(processing_times),
                    'fastest_model': min(successful_results, key=lambda x: x['metrics'].get('processing_time', float('inf')))['model_name'],
                    'slowest_model': max(successful_results, key=lambda x: x['metrics'].get('processing_time', 0))['model_name']
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆçµ±è¨ˆæ‘˜è¦å¤±æ•—: {e}")
            return {'error': str(e)}
    
    def _generate_chart_data(self, validation_results: Dict[str, Any], 
                           comparison_analysis: ModelComparisonResult) -> Dict[str, Any]:
        """ç”Ÿæˆåœ–è¡¨æ•¸æ“š"""
        try:
            chart_data = {}
            
            successful_results = {k: v for k, v in validation_results.items() if v['success']}
            
            if not successful_results:
                return {'error': 'No data for charts'}
            
            # æ¨¡å‹æ€§èƒ½æ¯”è¼ƒé›·é”åœ–æ•¸æ“š
            radar_data = {}
            for model_name, result in successful_results.items():
                metrics = result['metrics']
                radar_data[model_name] = {
                    'accuracy': metrics.get('accuracy', 0),
                    'stability': metrics.get('stability_score', 0),
                    'confidence': metrics.get('confidence_avg', 0),
                    'speed': min(1.0, metrics.get('predictions_per_second', 0) / 100),  # æ¨™æº–åŒ–
                    'success_rate': metrics.get('success_rate', 0)
                }
            
            chart_data['radar_chart'] = radar_data
            
            # æ€§èƒ½åˆ†ä½ˆæŸ±ç‹€åœ–æ•¸æ“š
            bar_data = {}
            metrics_to_plot = ['accuracy', 'stability_score', 'confidence_avg', 'success_rate']
            
            for metric in metrics_to_plot:
                bar_data[metric] = {
                    model_name: result['metrics'].get(metric, 0)
                    for model_name, result in successful_results.items()
                }
            
            chart_data['bar_chart'] = bar_data
            
            # è™•ç†æ™‚é–“æ¯”è¼ƒ
            processing_time_data = {
                model_name: result['metrics'].get('processing_time', 0)
                for model_name, result in successful_results.items()
            }
            chart_data['processing_time_chart'] = processing_time_data
            
            # æ¨¡å‹æ’åé¤…åœ–æ•¸æ“š
            ranking_data = comparison_analysis.model_rankings
            chart_data['ranking_pie'] = ranking_data
            
            return chart_data
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆåœ–è¡¨æ•¸æ“šå¤±æ•—: {e}")
            return {'error': str(e)}
    
    def save_report(self, report: Dict[str, Any], filename: str = None) -> Dict[str, str]:
        """ä¿å­˜å ±å‘Šåˆ°æ–‡ä»¶"""
        try:
            if filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"model_validation_report_{timestamp}"
            
            saved_files = {}
            
            # ä¿å­˜JSONæ ¼å¼
            if 'json' in self.report_config['output_formats']:
                json_path = self.output_dir / f"{filename}.json"
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False, default=str)
                saved_files['json'] = str(json_path)
                logger.info(f"âœ… JSONå ±å‘Šå·²ä¿å­˜: {json_path}")
            
            # ä¿å­˜HTMLæ ¼å¼ (ç°¡åŒ–ç‰ˆ)
            if 'html' in self.report_config['output_formats']:
                html_path = self.output_dir / f"{filename}.html"
                html_content = self._generate_html_report(report)
                with open(html_path, 'w', encoding='utf-8') as f:
                    f.write(html_content)
                saved_files['html'] = str(html_path)
                logger.info(f"âœ… HTMLå ±å‘Šå·²ä¿å­˜: {html_path}")
            
            return saved_files
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å ±å‘Šå¤±æ•—: {e}")
            return {'error': str(e)}
    
    def _generate_html_report(self, report: Dict[str, Any]) -> str:
        """ç”ŸæˆHTMLæ ¼å¼å ±å‘Š"""
        try:
            html_template = f"""
            <!DOCTYPE html>
            <html lang="zh-TW">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>AImax æ¨¡å‹é©—è­‰å ±å‘Š</title>
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 20px; }}
                    .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
                    .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                    .metric {{ display: inline-block; margin: 10px; padding: 10px; background-color: #f9f9f9; border-radius: 3px; }}
                    .recommendation {{ background-color: #e8f5e8; padding: 10px; margin: 5px 0; border-radius: 3px; }}
                    table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}
                    th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                    th {{ background-color: #f2f2f2; }}
                </style>
            </head>
            <body>
                <div class="header">
                    <h1>ğŸ¤– AImax æ¨¡å‹é©—è­‰å ±å‘Š</h1>
                    <p>ç”Ÿæˆæ™‚é–“: {report['report_metadata']['generated_at']}</p>
                    <p>è©•ä¼°æ¨¡å‹æ•¸: {report['report_metadata']['models_evaluated']}</p>
                </div>
                
                <div class="section">
                    <h2>ğŸ“Š åŸ·è¡Œæ‘˜è¦</h2>
                    <div class="metric">æœ€ä½³æ¨¡å‹: {report['executive_summary']['best_overall_model']}</div>
                    <div class="metric">å¹³å‡æº–ç¢ºç‡: {report['executive_summary']['average_accuracy']:.2%}</div>
                    <p>{report['executive_summary']['comparison_summary']}</p>
                </div>
                
                <div class="section">
                    <h2>ğŸ† æ¨¡å‹æ’å</h2>
                    <table>
                        <tr><th>æ’å</th><th>æ¨¡å‹åç¨±</th><th>ç¶œåˆè©•åˆ†</th></tr>
            """
            
            # æ·»åŠ æ¨¡å‹æ’åè¡¨æ ¼
            if 'model_rankings' in report['comparison_analysis']:
                rankings = report['comparison_analysis']['model_rankings']
                performance_matrix = report['comparison_analysis']['performance_matrix']
                
                for model, rank in sorted(rankings.items(), key=lambda x: x[1]):
                    score = performance_matrix.get(model, {}).get('overall_score', 0)
                    html_template += f"<tr><td>{rank}</td><td>{model}</td><td>{score:.3f}</td></tr>"
            
            html_template += """
                    </table>
                </div>
                
                <div class="section">
                    <h2>ğŸ’¡ å»ºè­°</h2>
            """
            
            # æ·»åŠ å»ºè­°
            if 'recommendations' in report and 'primary_recommendations' in report['recommendations']:
                for rec in report['recommendations']['primary_recommendations']:
                    html_template += f'<div class="recommendation">{rec}</div>'
            
            html_template += """
                </div>
            </body>
            </html>
            """
            
            return html_template
            
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆHTMLå ±å‘Šå¤±æ•—: {e}")
            return f"<html><body><h1>å ±å‘Šç”Ÿæˆå¤±æ•—</h1><p>{e}</p></body></html>"
    
    def get_model_performance_history(self, model_name: str = None) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹æ€§èƒ½æ­·å²"""
        try:
            if model_name:
                if model_name in self.model_metrics:
                    return {model_name: asdict(self.model_metrics[model_name])}
                else:
                    return {'error': f'Model {model_name} not found'}
            else:
                return {name: asdict(metrics) for name, metrics in self.model_metrics.items()}
                
        except Exception as e:
            logger.error(f"âŒ ç²å–æ¨¡å‹æ­·å²å¤±æ•—: {e}")
            return {'error': str(e)}


# å‰µå»ºå…¨å±€å ±å‘Šç”Ÿæˆå™¨å¯¦ä¾‹
def create_model_validation_report_generator(output_dir: str = "AImax/reports") -> ModelValidationReportGenerator:
    """å‰µå»ºæ¨¡å‹é©—è­‰å ±å‘Šç”Ÿæˆå™¨å¯¦ä¾‹"""
    return ModelValidationReportGenerator(output_dir)


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    import asyncio
    
    async def test_model_validation_report():
        """æ¸¬è©¦æ¨¡å‹é©—è­‰å ±å‘Šç³»çµ±"""
        print("ğŸ§ª æ¸¬è©¦æ¨¡å‹é©—è­‰å ±å‘Šç³»çµ±...")
        
        # å‰µå»ºå ±å‘Šç”Ÿæˆå™¨
        report_generator = create_model_validation_report_generator()
        
        # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
        test_data = []
        ground_truth = []
        
        for i in range(20):
            test_data.append({
                'current_price': 1500000 + np.random.normal(0, 50000),
                'price_change_1m': np.random.normal(0, 1),
                'volume_ratio': max(0.5, np.random.lognormal(0, 0.2)),
                'ai_formatted_data': 'æ¸¬è©¦æ•¸æ“š'
            })
            ground_truth.append(np.random.choice(['BUY', 'SELL', 'HOLD']))
        
        # åŸ·è¡Œæ¨¡å‹é©—è­‰
        validation_result = await report_generator.validate_all_models(test_data, ground_truth)
        
        if validation_result['success']:
            print("âœ… æ¨¡å‹é©—è­‰æˆåŠŸ")
            print(f"   é©—è­‰æ¨¡å‹æ•¸: {len(validation_result['models_validated'])}")
            print(f"   æœ€ä½³æ¨¡å‹: {validation_result['comparison_analysis'].best_overall_model}")
            
            # ä¿å­˜å ±å‘Š
            saved_files = report_generator.save_report(validation_result['unified_report'])
            print(f"   å ±å‘Šå·²ä¿å­˜: {list(saved_files.keys())}")
        else:
            print(f"âŒ æ¨¡å‹é©—è­‰å¤±æ•—: {validation_result.get('error')}")
    
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(test_model_validation_report())