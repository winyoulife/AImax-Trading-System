#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AImax å­˜å„²ç©ºé–“ç›£æ§å’Œå„ªåŒ– - ä»»å‹™10å¯¦ç¾
ç›£æ§å­˜å„²ç©ºé–“ä½¿ç”¨ï¼Œè‡ªå‹•æ¸…ç†ä¸å¿…è¦æ–‡ä»¶ï¼Œå„ªåŒ–å­˜å„²æ•ˆç‡
"""

import sys
import os
import json
import time
import logging
import shutil
import gzip
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
from dataclasses import dataclass, field
import threading

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(str(Path(__file__).parent.parent.parent))

logger = logging.getLogger(__name__)

@dataclass
class DirectoryInfo:
    """ç›®éŒ„ä¿¡æ¯"""
    path: Path
    size_mb: float
    file_count: int
    subdirectory_count: int
    last_modified: datetime
    file_types: Dict[str, int] = field(default_factory=dict)

@dataclass
class CleanupResult:
    """æ¸…ç†çµæœ"""
    cleaned_files: int
    freed_space_mb: float
    cleaned_directories: List[str]
    errors: List[str] = field(default_factory=list)

class StorageOptimizer:
    """å­˜å„²ç©ºé–“å„ªåŒ–å™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        self.storage_stats: List[Dict[str, Any]] = []
        
        # æ¸…ç†é…ç½®
        self.cleanup_config = {
            'log_files': {
                'extensions': ['.log', '.out', '.err'],
                'max_age_days': 7,
                'compress_after_days': 1,
                'max_size_mb': 100
            },
            'temp_files': {
                'directories': ['temp', 'tmp', '__pycache__', '.pytest_cache'],
                'extensions': ['.tmp', '.temp', '.cache'],
                'max_age_hours': 24
            },
            'backup_files': {
                'extensions': ['.bak', '.backup', '.old'],
                'max_age_days': 30,
                'max_count': 5
            },
            'report_files': {
                'extensions': ['.json', '.txt', '.html'],
                'directories': ['reports'],
                'max_age_days': 30,
                'max_count': 100
            },
            'data_files': {
                'extensions': ['.csv', '.db'],
                'directories': ['data'],
                'max_age_days': 90,
                'compress_after_days': 30
            }
        }
        
        # å­˜å„²ç›£æ§é…ç½®
        self.monitoring_config = {
            'scan_interval_hours': 6,
            'alert_thresholds': {
                'total_size_gb': 1.0,      # é …ç›®ç¸½å¤§å°è¶…é1GBè­¦å‘Š
                'single_dir_mb': 100,      # å–®å€‹ç›®éŒ„è¶…é100MBè­¦å‘Š
                'file_count': 10000,       # æ–‡ä»¶æ•¸é‡è¶…é10000è­¦å‘Š
                'log_size_mb': 50          # æ—¥èªŒæ–‡ä»¶è¶…é50MBè­¦å‘Š
            },
            'exclude_patterns': [
                '.git',
                'node_modules',
                '.venv',
                'venv',
                '__pycache__',
                '.pytest_cache'
            ]
        }
        
        # ç›£æ§ç‹€æ…‹
        self.monitoring_active = False
        self.monitoring_thread = None
    
    def start_monitoring(self):
        """é–‹å§‹å­˜å„²ç›£æ§"""
        if self.monitoring_active:
            logger.warning("âš ï¸ å­˜å„²ç›£æ§å·²åœ¨é‹è¡Œ")
            return
        
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitoring_thread.start()
        
        logger.info(f"ğŸ’¾ å­˜å„²ç›£æ§å·²å•Ÿå‹•ï¼Œæƒæé–“éš”: {self.monitoring_config['scan_interval_hours']}å°æ™‚")
    
    def stop_monitoring(self):
        """åœæ­¢å­˜å„²ç›£æ§"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=10)
        
        logger.info("ğŸ›‘ å­˜å„²ç›£æ§å·²åœæ­¢")
    
    def _monitoring_loop(self):
        """ç›£æ§å¾ªç’°"""
        while self.monitoring_active:
            try:
                # æƒæå­˜å„²ä½¿ç”¨æƒ…æ³
                storage_info = self.scan_storage_usage()
                self.storage_stats.append(storage_info)
                
                # æª¢æŸ¥è­¦å‘Šé–¾å€¼
                self._check_storage_alerts(storage_info)
                
                # è‡ªå‹•æ¸…ç†
                self._auto_cleanup()
                
                # æ¸…ç†èˆŠçµ±è¨ˆæ•¸æ“š
                self._cleanup_old_stats()
                
                # ç­‰å¾…ä¸‹æ¬¡æƒæ
                time.sleep(self.monitoring_config['scan_interval_hours'] * 3600)
                
            except Exception as e:
                logger.error(f"âŒ å­˜å„²ç›£æ§å¾ªç’°éŒ¯èª¤: {e}")
                time.sleep(3600)  # å‡ºéŒ¯æ™‚ç­‰å¾…1å°æ™‚
    
    def scan_storage_usage(self) -> Dict[str, Any]:
        """æƒæå­˜å„²ä½¿ç”¨æƒ…æ³"""
        logger.info("ğŸ” æƒæå­˜å„²ä½¿ç”¨æƒ…æ³...")
        
        scan_start = time.time()
        directory_info = {}
        total_size = 0
        total_files = 0
        
        try:
            # æƒæä¸»è¦ç›®éŒ„
            main_directories = [
                'src', 'scripts', 'tests', 'data', 'logs', 'reports', 
                'static', 'templates', '.github', 'docs'
            ]
            
            for dir_name in main_directories:
                dir_path = self.project_root / dir_name
                if dir_path.exists():
                    dir_info = self._analyze_directory(dir_path)
                    directory_info[dir_name] = dir_info
                    total_size += dir_info.size_mb
                    total_files += dir_info.file_count
            
            # æƒææ ¹ç›®éŒ„æ–‡ä»¶
            root_files = self._analyze_directory(self.project_root, recursive=False)
            directory_info['root_files'] = root_files
            total_size += root_files.size_mb
            total_files += root_files.file_count
            
            scan_time = time.time() - scan_start
            
            storage_info = {
                'timestamp': datetime.now(),
                'total_size_mb': total_size,
                'total_size_gb': total_size / 1024,
                'total_files': total_files,
                'directory_info': directory_info,
                'scan_time_seconds': scan_time,
                'largest_directories': self._get_largest_directories(directory_info),
                'file_type_distribution': self._get_file_type_distribution(directory_info)
            }
            
            logger.info(f"âœ… å­˜å„²æƒæå®Œæˆ: {total_size:.2f} MB, {total_files} å€‹æ–‡ä»¶")
            
            return storage_info
            
        except Exception as e:
            logger.error(f"âŒ å­˜å„²æƒæå¤±æ•—: {e}")
            return {
                'timestamp': datetime.now(),
                'error': str(e),
                'total_size_mb': 0,
                'total_files': 0
            }
    
    def _analyze_directory(self, directory: Path, recursive: bool = True) -> DirectoryInfo:
        """åˆ†æç›®éŒ„ä¿¡æ¯"""
        try:
            total_size = 0
            file_count = 0
            subdirectory_count = 0
            file_types = {}
            last_modified = datetime.fromtimestamp(0)
            
            if recursive:
                # éæ­¸æƒæ
                for item in directory.rglob('*'):
                    if item.is_file():
                        try:
                            file_size = item.stat().st_size
                            total_size += file_size
                            file_count += 1
                            
                            # çµ±è¨ˆæ–‡ä»¶é¡å‹
                            extension = item.suffix.lower()
                            file_types[extension] = file_types.get(extension, 0) + 1
                            
                            # æ›´æ–°æœ€å¾Œä¿®æ”¹æ™‚é–“
                            file_mtime = datetime.fromtimestamp(item.stat().st_mtime)
                            if file_mtime > last_modified:
                                last_modified = file_mtime
                                
                        except (OSError, FileNotFoundError):
                            continue
                    elif item.is_dir():
                        subdirectory_count += 1
            else:
                # åªæƒæç•¶å‰ç›®éŒ„
                for item in directory.iterdir():
                    if item.is_file():
                        try:
                            file_size = item.stat().st_size
                            total_size += file_size
                            file_count += 1
                            
                            extension = item.suffix.lower()
                            file_types[extension] = file_types.get(extension, 0) + 1
                            
                            file_mtime = datetime.fromtimestamp(item.stat().st_mtime)
                            if file_mtime > last_modified:
                                last_modified = file_mtime
                                
                        except (OSError, FileNotFoundError):
                            continue
                    elif item.is_dir():
                        subdirectory_count += 1
            
            return DirectoryInfo(
                path=directory,
                size_mb=total_size / (1024 * 1024),
                file_count=file_count,
                subdirectory_count=subdirectory_count,
                last_modified=last_modified,
                file_types=file_types
            )
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æç›®éŒ„å¤±æ•— {directory}: {e}")
            return DirectoryInfo(
                path=directory,
                size_mb=0,
                file_count=0,
                subdirectory_count=0,
                last_modified=datetime.fromtimestamp(0)
            )
    
    def _get_largest_directories(self, directory_info: Dict[str, DirectoryInfo]) -> List[Tuple[str, float]]:
        """ç²å–æœ€å¤§çš„ç›®éŒ„"""
        dir_sizes = [
            (name, info.size_mb) 
            for name, info in directory_info.items()
            if isinstance(info, DirectoryInfo)
        ]
        
        return sorted(dir_sizes, key=lambda x: x[1], reverse=True)[:10]
    
    def _get_file_type_distribution(self, directory_info: Dict[str, DirectoryInfo]) -> Dict[str, int]:
        """ç²å–æ–‡ä»¶é¡å‹åˆ†å¸ƒ"""
        file_types = {}
        
        for info in directory_info.values():
            if isinstance(info, DirectoryInfo):
                for ext, count in info.file_types.items():
                    file_types[ext] = file_types.get(ext, 0) + count
        
        return dict(sorted(file_types.items(), key=lambda x: x[1], reverse=True))
    
    def _check_storage_alerts(self, storage_info: Dict[str, Any]):
        """æª¢æŸ¥å­˜å„²è­¦å‘Š"""
        alerts = []
        thresholds = self.monitoring_config['alert_thresholds']
        
        # æª¢æŸ¥ç¸½å¤§å°
        if storage_info['total_size_gb'] > thresholds['total_size_gb']:
            alerts.append(f"ğŸš¨ é …ç›®ç¸½å¤§å°éå¤§: {storage_info['total_size_gb']:.2f} GB")
        
        # æª¢æŸ¥æ–‡ä»¶æ•¸é‡
        if storage_info['total_files'] > thresholds['file_count']:
            alerts.append(f"âš ï¸ æ–‡ä»¶æ•¸é‡éå¤š: {storage_info['total_files']:,} å€‹æ–‡ä»¶")
        
        # æª¢æŸ¥å–®å€‹ç›®éŒ„å¤§å°
        for dir_name, size_mb in storage_info.get('largest_directories', []):
            if size_mb > thresholds['single_dir_mb']:
                alerts.append(f"âš ï¸ ç›®éŒ„ '{dir_name}' éå¤§: {size_mb:.2f} MB")
        
        # æª¢æŸ¥æ—¥èªŒæ–‡ä»¶å¤§å°
        if 'logs' in storage_info.get('directory_info', {}):
            logs_info = storage_info['directory_info']['logs']
            if isinstance(logs_info, DirectoryInfo) and logs_info.size_mb > thresholds['log_size_mb']:
                alerts.append(f"âš ï¸ æ—¥èªŒæ–‡ä»¶éå¤§: {logs_info.size_mb:.2f} MB")
        
        # è¨˜éŒ„è­¦å‘Š
        for alert in alerts:
            logger.warning(alert)
    
    def _auto_cleanup(self):
        """è‡ªå‹•æ¸…ç†"""
        try:
            logger.info("ğŸ§¹ é–‹å§‹è‡ªå‹•æ¸…ç†...")
            
            # æ¸…ç†æ—¥èªŒæ–‡ä»¶
            log_result = self.cleanup_log_files()
            if log_result.freed_space_mb > 0:
                logger.info(f"ğŸ“„ æ¸…ç†æ—¥èªŒæ–‡ä»¶: é‡‹æ”¾ {log_result.freed_space_mb:.2f} MB")
            
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            temp_result = self.cleanup_temporary_files()
            if temp_result.freed_space_mb > 0:
                logger.info(f"ğŸ—‚ï¸ æ¸…ç†è‡¨æ™‚æ–‡ä»¶: é‡‹æ”¾ {temp_result.freed_space_mb:.2f} MB")
            
            # æ¸…ç†èˆŠå ±å‘Š
            report_result = self.cleanup_old_reports()
            if report_result.freed_space_mb > 0:
                logger.info(f"ğŸ“Š æ¸…ç†èˆŠå ±å‘Š: é‡‹æ”¾ {report_result.freed_space_mb:.2f} MB")
            
        except Exception as e:
            logger.error(f"âŒ è‡ªå‹•æ¸…ç†å¤±æ•—: {e}")
    
    def cleanup_log_files(self) -> CleanupResult:
        """æ¸…ç†æ—¥èªŒæ–‡ä»¶"""
        result = CleanupResult(0, 0.0, [])
        
        try:
            logs_dir = self.project_root / "logs"
            if not logs_dir.exists():
                return result
            
            config = self.cleanup_config['log_files']
            cutoff_date = datetime.now() - timedelta(days=config['max_age_days'])
            compress_date = datetime.now() - timedelta(days=config['compress_after_days'])
            
            for log_file in logs_dir.rglob("*"):
                if not log_file.is_file():
                    continue
                
                try:
                    file_mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
                    file_size = log_file.stat().st_size
                    
                    # åˆªé™¤éèˆŠçš„æ–‡ä»¶
                    if file_mtime < cutoff_date:
                        freed_mb = file_size / (1024 * 1024)
                        log_file.unlink()
                        result.cleaned_files += 1
                        result.freed_space_mb += freed_mb
                        
                    # å£“ç¸®è¼ƒèˆŠçš„æ–‡ä»¶
                    elif (file_mtime < compress_date and 
                          log_file.suffix in config['extensions'] and
                          not log_file.name.endswith('.gz')):
                        
                        compressed_file = log_file.with_suffix(log_file.suffix + '.gz')
                        
                        with open(log_file, 'rb') as f_in:
                            with gzip.open(compressed_file, 'wb') as f_out:
                                shutil.copyfileobj(f_in, f_out)
                        
                        # åˆªé™¤åŸæ–‡ä»¶
                        original_size = file_size / (1024 * 1024)
                        compressed_size = compressed_file.stat().st_size / (1024 * 1024)
                        
                        log_file.unlink()
                        result.cleaned_files += 1
                        result.freed_space_mb += (original_size - compressed_size)
                        
                except (OSError, FileNotFoundError) as e:
                    result.errors.append(f"è™•ç†æ—¥èªŒæ–‡ä»¶å¤±æ•— {log_file}: {e}")
            
            if result.cleaned_files > 0:
                result.cleaned_directories.append(str(logs_dir))
            
        except Exception as e:
            result.errors.append(f"æ¸…ç†æ—¥èªŒæ–‡ä»¶å¤±æ•—: {e}")
        
        return result
    
    def cleanup_temporary_files(self) -> CleanupResult:
        """æ¸…ç†è‡¨æ™‚æ–‡ä»¶"""
        result = CleanupResult(0, 0.0, [])
        
        try:
            config = self.cleanup_config['temp_files']
            cutoff_time = datetime.now() - timedelta(hours=config['max_age_hours'])
            
            # æ¸…ç†æŒ‡å®šçš„è‡¨æ™‚ç›®éŒ„
            for dir_name in config['directories']:
                temp_dir = self.project_root / dir_name
                if temp_dir.exists():
                    initial_size = self._get_directory_size_mb(temp_dir)
                    
                    if dir_name in ['__pycache__', '.pytest_cache']:
                        # å®Œå…¨åˆªé™¤ç·©å­˜ç›®éŒ„
                        shutil.rmtree(temp_dir)
                        result.cleaned_files += 1
                        result.freed_space_mb += initial_size
                        result.cleaned_directories.append(str(temp_dir))
                    else:
                        # æ¸…ç†èˆŠæ–‡ä»¶
                        for temp_file in temp_dir.rglob("*"):
                            if temp_file.is_file():
                                try:
                                    file_mtime = datetime.fromtimestamp(temp_file.stat().st_mtime)
                                    if file_mtime < cutoff_time:
                                        file_size = temp_file.stat().st_size / (1024 * 1024)
                                        temp_file.unlink()
                                        result.cleaned_files += 1
                                        result.freed_space_mb += file_size
                                except (OSError, FileNotFoundError):
                                    continue
                        
                        if result.cleaned_files > 0:
                            result.cleaned_directories.append(str(temp_dir))
            
            # æ¸…ç†æŒ‡å®šæ“´å±•åçš„è‡¨æ™‚æ–‡ä»¶
            for temp_file in self.project_root.rglob("*"):
                if (temp_file.is_file() and 
                    temp_file.suffix in config['extensions']):
                    try:
                        file_mtime = datetime.fromtimestamp(temp_file.stat().st_mtime)
                        if file_mtime < cutoff_time:
                            file_size = temp_file.stat().st_size / (1024 * 1024)
                            temp_file.unlink()
                            result.cleaned_files += 1
                            result.freed_space_mb += file_size
                    except (OSError, FileNotFoundError):
                        continue
            
        except Exception as e:
            result.errors.append(f"æ¸…ç†è‡¨æ™‚æ–‡ä»¶å¤±æ•—: {e}")
        
        return result
    
    def cleanup_old_reports(self) -> CleanupResult:
        """æ¸…ç†èˆŠå ±å‘Šæ–‡ä»¶"""
        result = CleanupResult(0, 0.0, [])
        
        try:
            reports_dir = self.project_root / "reports"
            if not reports_dir.exists():
                return result
            
            config = self.cleanup_config['report_files']
            cutoff_date = datetime.now() - timedelta(days=config['max_age_days'])
            
            # ç²å–æ‰€æœ‰å ±å‘Šæ–‡ä»¶ä¸¦æŒ‰æ™‚é–“æ’åº
            report_files = []
            for report_file in reports_dir.rglob("*"):
                if (report_file.is_file() and 
                    report_file.suffix in config['extensions']):
                    try:
                        file_mtime = datetime.fromtimestamp(report_file.stat().st_mtime)
                        file_size = report_file.stat().st_size
                        report_files.append((report_file, file_mtime, file_size))
                    except (OSError, FileNotFoundError):
                        continue
            
            # æŒ‰æ™‚é–“æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            report_files.sort(key=lambda x: x[1], reverse=True)
            
            # ä¿ç•™æœ€æ–°çš„æ–‡ä»¶ï¼Œåˆªé™¤éèˆŠæˆ–éå¤šçš„æ–‡ä»¶
            for i, (report_file, file_mtime, file_size) in enumerate(report_files):
                should_delete = False
                
                # è¶…éæœ€å¤§æ•¸é‡é™åˆ¶
                if i >= config['max_count']:
                    should_delete = True
                
                # è¶…éæ™‚é–“é™åˆ¶
                elif file_mtime < cutoff_date:
                    should_delete = True
                
                if should_delete:
                    try:
                        freed_mb = file_size / (1024 * 1024)
                        report_file.unlink()
                        result.cleaned_files += 1
                        result.freed_space_mb += freed_mb
                    except (OSError, FileNotFoundError):
                        continue
            
            if result.cleaned_files > 0:
                result.cleaned_directories.append(str(reports_dir))
            
        except Exception as e:
            result.errors.append(f"æ¸…ç†å ±å‘Šæ–‡ä»¶å¤±æ•—: {e}")
        
        return result
    
    def _get_directory_size_mb(self, directory: Path) -> float:
        """ç²å–ç›®éŒ„å¤§å°ï¼ˆMBï¼‰"""
        try:
            total_size = 0
            for file_path in directory.rglob('*'):
                if file_path.is_file():
                    try:
                        total_size += file_path.stat().st_size
                    except (OSError, FileNotFoundError):
                        continue
            return total_size / (1024 * 1024)
        except Exception:
            return 0.0
    
    def _cleanup_old_stats(self):
        """æ¸…ç†èˆŠçµ±è¨ˆæ•¸æ“š"""
        cutoff_date = datetime.now() - timedelta(days=30)
        self.storage_stats = [
            stat for stat in self.storage_stats
            if stat.get('timestamp', datetime.now()) > cutoff_date
        ]
    
    def get_storage_statistics(self) -> Dict[str, Any]:
        """ç²å–å­˜å„²çµ±è¨ˆ"""
        if not self.storage_stats:
            # å¦‚æœæ²’æœ‰çµ±è¨ˆæ•¸æ“šï¼Œç«‹å³æƒæä¸€æ¬¡
            current_stats = self.scan_storage_usage()
            return current_stats
        
        latest_stats = self.storage_stats[-1]
        
        # è¨ˆç®—è¶¨å‹¢
        if len(self.storage_stats) > 1:
            previous_stats = self.storage_stats[-2]
            size_trend = latest_stats['total_size_mb'] - previous_stats['total_size_mb']
            files_trend = latest_stats['total_files'] - previous_stats['total_files']
        else:
            size_trend = 0
            files_trend = 0
        
        return {
            'current': latest_stats,
            'trends': {
                'size_change_mb': size_trend,
                'files_change': files_trend
            },
            'history_points': len(self.storage_stats)
        }
    
    def generate_storage_report(self) -> str:
        """ç”Ÿæˆå­˜å„²å ±å‘Š"""
        stats = self.get_storage_statistics()
        current = stats['current']
        
        report = f"""
ğŸ’¾ AImax å­˜å„²ç©ºé–“ç›£æ§å ±å‘Š
{'='*60}

ğŸ“Š ç•¶å‰å­˜å„²ä½¿ç”¨:
   ç¸½å¤§å°: {current['total_size_mb']:.2f} MB ({current['total_size_gb']:.2f} GB)
   ç¸½æ–‡ä»¶æ•¸: {current['total_files']:,}
   æƒææ™‚é–“: {current.get('scan_time_seconds', 0):.2f} ç§’
"""
        
        # æ·»åŠ è¶¨å‹¢ä¿¡æ¯
        if 'trends' in stats:
            trends = stats['trends']
            size_trend_str = f"+{trends['size_change_mb']:.2f}" if trends['size_change_mb'] >= 0 else f"{trends['size_change_mb']:.2f}"
            files_trend_str = f"+{trends['files_change']}" if trends['files_change'] >= 0 else f"{trends['files_change']}"
            
            report += f"""
ğŸ“ˆ è®ŠåŒ–è¶¨å‹¢:
   å¤§å°è®ŠåŒ–: {size_trend_str} MB
   æ–‡ä»¶æ•¸è®ŠåŒ–: {files_trend_str}
"""
        
        # æ·»åŠ æœ€å¤§ç›®éŒ„ä¿¡æ¯
        if 'largest_directories' in current:
            report += f"\nğŸ“ æœ€å¤§ç›®éŒ„ (å‰5å):\n"
            for dir_name, size_mb in current['largest_directories'][:5]:
                report += f"   {dir_name}: {size_mb:.2f} MB\n"
        
        # æ·»åŠ æ–‡ä»¶é¡å‹åˆ†å¸ƒ
        if 'file_type_distribution' in current:
            report += f"\nğŸ“„ æ–‡ä»¶é¡å‹åˆ†å¸ƒ (å‰10å):\n"
            for ext, count in list(current['file_type_distribution'].items())[:10]:
                ext_display = ext if ext else '(ç„¡æ“´å±•å)'
                report += f"   {ext_display}: {count} å€‹æ–‡ä»¶\n"
        
        report += f"\n{'='*60}\n"
        report += f"å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        
        return report
    
    def save_storage_data(self) -> Path:
        """ä¿å­˜å­˜å„²æ•¸æ“š"""
        reports_dir = self.project_root / "reports"
        reports_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜è©³ç´°æ•¸æ“š
        data_file = reports_dir / f"storage_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        storage_data = {
            'timestamp': datetime.now().isoformat(),
            'statistics': self.get_storage_statistics(),
            'cleanup_config': self.cleanup_config,
            'monitoring_config': self.monitoring_config
        }
        
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(storage_data, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜å ±å‘Š
        report_file = reports_dir / f"storage_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(self.generate_storage_report())
        
        logger.info(f"ğŸ“„ å­˜å„²æ•¸æ“šå·²ä¿å­˜: {data_file}")
        logger.info(f"ğŸ“„ å­˜å„²å ±å‘Šå·²ä¿å­˜: {report_file}")
        
        return data_file

# å…¨å±€å­˜å„²å„ªåŒ–å™¨å¯¦ä¾‹
storage_optimizer = StorageOptimizer()

# ä¾¿æ·å‡½æ•¸
def start_storage_monitoring():
    """å•Ÿå‹•å­˜å„²ç›£æ§"""
    storage_optimizer.start_monitoring()

def stop_storage_monitoring():
    """åœæ­¢å­˜å„²ç›£æ§"""
    storage_optimizer.stop_monitoring()

def get_storage_stats() -> Dict[str, Any]:
    """ç²å–å­˜å„²çµ±è¨ˆ"""
    return storage_optimizer.get_storage_statistics()

def cleanup_storage() -> Dict[str, Any]:
    """æ¸…ç†å­˜å„²ç©ºé–“"""
    log_result = storage_optimizer.cleanup_log_files()
    temp_result = storage_optimizer.cleanup_temporary_files()
    report_result = storage_optimizer.cleanup_old_reports()
    
    return {
        'log_cleanup': log_result,
        'temp_cleanup': temp_result,
        'report_cleanup': report_result,
        'total_freed_mb': (log_result.freed_space_mb + 
                          temp_result.freed_space_mb + 
                          report_result.freed_space_mb)
    }