#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç³»çµ±æ€§èƒ½å„ªåŒ–å™¨ - ä»»å‹™8.2å¯¦ç¾
å„ªåŒ–å¤šç·šç¨‹ä¸¦ç™¼è™•ç†æ€§èƒ½å’Œç³»çµ±è³‡æºç®¡ç†
"""

import sys
import os
import asyncio
import threading
import multiprocessing
import psutil
import gc
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Callable
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from queue import Queue, PriorityQueue
import weakref
import json
from collections import defaultdict, deque
try:
    import resource
    RESOURCE_AVAILABLE = True
except ImportError:
    RESOURCE_AVAILABLE = False

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹"""
    timestamp: datetime = field(default_factory=datetime.now)
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    memory_available: float = 0.0
    thread_count: int = 0
    process_count: int = 0
    io_read_bytes: int = 0
    io_write_bytes: int = 0
    network_sent: int = 0
    network_recv: int = 0
    gc_collections: Dict[int, int] = field(default_factory=dict)
    response_times: List[float] = field(default_factory=list)
    throughput: float = 0.0
    error_rate: float = 0.0

@dataclass
class ResourceLimits:
    """è³‡æºé™åˆ¶é…ç½®"""
    max_cpu_usage: float = 80.0  # æœ€å¤§CPUä½¿ç”¨ç‡ (%)
    max_memory_usage: float = 70.0  # æœ€å¤§å…§å­˜ä½¿ç”¨ç‡ (%)
    max_threads: int = 50  # æœ€å¤§ç·šç¨‹æ•¸
    max_processes: int = 8  # æœ€å¤§é€²ç¨‹æ•¸
    max_queue_size: int = 1000  # æœ€å¤§éšŠåˆ—å¤§å°
    gc_threshold: float = 60.0  # åƒåœ¾å›æ”¶è§¸ç™¼é–¾å€¼ (%)
    api_rate_limit: int = 100  # APIèª¿ç”¨é »ç‡é™åˆ¶ (æ¬¡/åˆ†é˜)

class ThreadPoolManager:
    """ç·šç¨‹æ± ç®¡ç†å™¨"""
    
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or min(32, (os.cpu_count() or 1) + 4)
        self.pools: Dict[str, ThreadPoolExecutor] = {}
        self.pool_stats: Dict[str, Dict[str, Any]] = {}
        self.lock = threading.Lock()
        
    def get_pool(self, pool_name: str, max_workers: int = None) -> ThreadPoolExecutor:
        """ç²å–æˆ–å‰µå»ºç·šç¨‹æ± """
        with self.lock:
            if pool_name not in self.pools:
                workers = max_workers or self.max_workers
                self.pools[pool_name] = ThreadPoolExecutor(
                    max_workers=workers,
                    thread_name_prefix=f"{pool_name}_"
                )
                self.pool_stats[pool_name] = {
                    'created_at': datetime.now(),
                    'max_workers': workers,
                    'submitted_tasks': 0,
                    'completed_tasks': 0,
                    'failed_tasks': 0,
                    'avg_execution_time': 0.0
                }
                logger.info(f"âœ… å‰µå»ºç·šç¨‹æ±  {pool_name}: {workers} workers")
            
            return self.pools[pool_name]
    
    def submit_task(self, pool_name: str, func: Callable, *args, **kwargs):
        """æäº¤ä»»å‹™åˆ°æŒ‡å®šç·šç¨‹æ± """
        pool = self.get_pool(pool_name)
        
        def wrapped_func(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                execution_time = time.time() - start_time
                
                # æ›´æ–°çµ±è¨ˆ
                with self.lock:
                    stats = self.pool_stats[pool_name]
                    stats['completed_tasks'] += 1
                    stats['avg_execution_time'] = (
                        (stats['avg_execution_time'] * (stats['completed_tasks'] - 1) + execution_time) /
                        stats['completed_tasks']
                    )
                
                return result
                
            except Exception as e:
                with self.lock:
                    self.pool_stats[pool_name]['failed_tasks'] += 1
                logger.error(f"âŒ ç·šç¨‹æ±  {pool_name} ä»»å‹™åŸ·è¡Œå¤±æ•—: {e}")
                raise
        
        with self.lock:
            self.pool_stats[pool_name]['submitted_tasks'] += 1
        
        return pool.submit(wrapped_func, *args, **kwargs)
    
    def get_pool_stats(self) -> Dict[str, Dict[str, Any]]:
        """ç²å–ç·šç¨‹æ± çµ±è¨ˆä¿¡æ¯"""
        with self.lock:
            return dict(self.pool_stats)
    
    def shutdown_all(self, wait: bool = True):
        """é—œé–‰æ‰€æœ‰ç·šç¨‹æ± """
        with self.lock:
            for pool_name, pool in self.pools.items():
                logger.info(f"ğŸ”„ é—œé–‰ç·šç¨‹æ±  {pool_name}")
                pool.shutdown(wait=wait)
            self.pools.clear()
            self.pool_stats.clear()

class MemoryManager:
    """å…§å­˜ç®¡ç†å™¨"""
    
    def __init__(self, gc_threshold: float = 60.0):
        self.gc_threshold = gc_threshold
        self.memory_stats = deque(maxlen=100)  # ä¿ç•™æœ€è¿‘100æ¬¡è¨˜éŒ„
        self.weak_refs: List[weakref.ref] = []
        self.cache_registry: Dict[str, Any] = {}
        
    def monitor_memory(self) -> Dict[str, Any]:
        """ç›£æ§å…§å­˜ä½¿ç”¨æƒ…æ³"""
        try:
            # ç²å–ç³»çµ±å…§å­˜ä¿¡æ¯
            memory = psutil.virtual_memory()
            process = psutil.Process()
            process_memory = process.memory_info()
            
            memory_info = {
                'timestamp': datetime.now(),
                'system_total': memory.total,
                'system_available': memory.available,
                'system_used': memory.used,
                'system_percent': memory.percent,
                'process_rss': process_memory.rss,
                'process_vms': process_memory.vms,
                'process_percent': process.memory_percent(),
                'gc_stats': {
                    gen: gc.get_count()[gen] for gen in range(3)
                }
            }
            
            self.memory_stats.append(memory_info)
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦åƒåœ¾å›æ”¶
            if memory.percent > self.gc_threshold:
                self.force_garbage_collection()
            
            return memory_info
            
        except Exception as e:
            logger.error(f"âŒ å…§å­˜ç›£æ§å¤±æ•—: {e}")
            return {}
    
    def force_garbage_collection(self) -> Dict[str, int]:
        """å¼·åˆ¶åƒåœ¾å›æ”¶"""
        try:
            logger.info("ğŸ—‘ï¸ åŸ·è¡Œå¼·åˆ¶åƒåœ¾å›æ”¶")
            
            # æ¸…ç†å¼±å¼•ç”¨
            self.cleanup_weak_refs()
            
            # åŸ·è¡Œåƒåœ¾å›æ”¶
            collected = {}
            for generation in range(3):
                collected[f'gen_{generation}'] = gc.collect(generation)
            
            # æ¸…ç†ç·©å­˜
            self.cleanup_caches()
            
            logger.info(f"âœ… åƒåœ¾å›æ”¶å®Œæˆ: {collected}")
            return collected
            
        except Exception as e:
            logger.error(f"âŒ åƒåœ¾å›æ”¶å¤±æ•—: {e}")
            return {}
    
    def cleanup_weak_refs(self):
        """æ¸…ç†å¼±å¼•ç”¨"""
        try:
            alive_refs = []
            for ref in self.weak_refs:
                if ref() is not None:
                    alive_refs.append(ref)
            
            cleaned = len(self.weak_refs) - len(alive_refs)
            self.weak_refs = alive_refs
            
            if cleaned > 0:
                logger.info(f"ğŸ§¹ æ¸…ç†äº† {cleaned} å€‹å¼±å¼•ç”¨")
                
        except Exception as e:
            logger.error(f"âŒ å¼±å¼•ç”¨æ¸…ç†å¤±æ•—: {e}")
    
    def cleanup_caches(self):
        """æ¸…ç†ç·©å­˜"""
        try:
            cleaned_count = 0
            for cache_name in list(self.cache_registry.keys()):
                cache = self.cache_registry[cache_name]
                if hasattr(cache, 'clear'):
                    cache.clear()
                    cleaned_count += 1
                elif isinstance(cache, dict):
                    cache.clear()
                    cleaned_count += 1
            
            if cleaned_count > 0:
                logger.info(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} å€‹ç·©å­˜")
                
        except Exception as e:
            logger.error(f"âŒ ç·©å­˜æ¸…ç†å¤±æ•—: {e}")
    
    def register_cache(self, name: str, cache_obj: Any):
        """è¨»å†Šç·©å­˜å°è±¡"""
        self.cache_registry[name] = cache_obj
        logger.info(f"ğŸ“ è¨»å†Šç·©å­˜: {name}")
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ç²å–å…§å­˜çµ±è¨ˆä¿¡æ¯"""
        if not self.memory_stats:
            return {}
        
        recent_stats = list(self.memory_stats)[-10:]  # æœ€è¿‘10æ¬¡è¨˜éŒ„
        
        return {
            'current': recent_stats[-1] if recent_stats else {},
            'avg_system_percent': sum(s['system_percent'] for s in recent_stats) / len(recent_stats),
            'avg_process_percent': sum(s['process_percent'] for s in recent_stats) / len(recent_stats),
            'peak_system_percent': max(s['system_percent'] for s in recent_stats),
            'peak_process_percent': max(s['process_percent'] for s in recent_stats),
            'total_records': len(self.memory_stats)
        }

class APIRateLimiter:
    """APIé »ç‡é™åˆ¶å™¨"""
    
    def __init__(self, rate_limit: int = 100, time_window: int = 60):
        self.rate_limit = rate_limit  # æ¯åˆ†é˜æœ€å¤§è«‹æ±‚æ•¸
        self.time_window = time_window  # æ™‚é–“çª—å£ï¼ˆç§’ï¼‰
        self.requests: Dict[str, deque] = defaultdict(lambda: deque())
        self.lock = threading.Lock()
        
    def can_make_request(self, api_key: str = "default") -> bool:
        """æª¢æŸ¥æ˜¯å¦å¯ä»¥ç™¼èµ·è«‹æ±‚"""
        with self.lock:
            now = time.time()
            requests = self.requests[api_key]
            
            # æ¸…ç†éæœŸè«‹æ±‚
            while requests and requests[0] < now - self.time_window:
                requests.popleft()
            
            # æª¢æŸ¥æ˜¯å¦è¶…éé™åˆ¶
            return len(requests) < self.rate_limit
    
    def make_request(self, api_key: str = "default") -> bool:
        """è¨˜éŒ„è«‹æ±‚"""
        with self.lock:
            if self.can_make_request(api_key):
                self.requests[api_key].append(time.time())
                return True
            return False
    
    def wait_for_slot(self, api_key: str = "default", timeout: float = 30.0) -> bool:
        """ç­‰å¾…å¯ç”¨çš„è«‹æ±‚æ§½ä½"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            if self.can_make_request(api_key):
                return self.make_request(api_key)
            
            # è¨ˆç®—ç­‰å¾…æ™‚é–“
            with self.lock:
                requests = self.requests[api_key]
                if requests:
                    oldest_request = requests[0]
                    wait_time = max(0, oldest_request + self.time_window - time.time())
                    time.sleep(min(wait_time, 1.0))
                else:
                    time.sleep(0.1)
        
        return False
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–é™æµçµ±è¨ˆ"""
        with self.lock:
            stats = {}
            for api_key, requests in self.requests.items():
                now = time.time()
                recent_requests = [r for r in requests if r > now - self.time_window]
                
                stats[api_key] = {
                    'current_requests': len(recent_requests),
                    'rate_limit': self.rate_limit,
                    'utilization': len(recent_requests) / self.rate_limit,
                    'time_to_reset': max(0, recent_requests[0] + self.time_window - now) if recent_requests else 0
                }
            
            return stats

class ResourceMonitor:
    """è³‡æºç›£æ§å™¨"""
    
    def __init__(self, limits: ResourceLimits = None):
        self.limits = limits or ResourceLimits()
        self.metrics_history = deque(maxlen=1000)
        self.alerts: List[Dict[str, Any]] = []
        self.monitoring = False
        self.monitor_thread = None
        
    def start_monitoring(self, interval: float = 5.0):
        """é–‹å§‹è³‡æºç›£æ§"""
        if self.monitoring:
            return
        
        self.monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            args=(interval,),
            daemon=True
        )
        self.monitor_thread.start()
        logger.info(f"ğŸ” é–‹å§‹è³‡æºç›£æ§ (é–“éš”: {interval}ç§’)")
    
    def stop_monitoring(self):
        """åœæ­¢è³‡æºç›£æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5.0)
        logger.info("â¹ï¸ åœæ­¢è³‡æºç›£æ§")
    
    def _monitor_loop(self, interval: float):
        """ç›£æ§å¾ªç’°"""
        while self.monitoring:
            try:
                metrics = self.collect_metrics()
                self.metrics_history.append(metrics)
                self.check_resource_limits(metrics)
                time.sleep(interval)
                
            except Exception as e:
                logger.error(f"âŒ è³‡æºç›£æ§éŒ¯èª¤: {e}")
                time.sleep(interval)
    
    def collect_metrics(self) -> PerformanceMetrics:
        """æ”¶é›†æ€§èƒ½æŒ‡æ¨™"""
        try:
            # ç³»çµ±è³‡æº
            cpu_percent = psutil.cpu_percent(interval=0.1)
            memory = psutil.virtual_memory()
            
            # é€²ç¨‹ä¿¡æ¯
            process = psutil.Process()
            process_info = process.as_dict([
                'num_threads', 'memory_info', 'io_counters', 'connections'
            ])
            
            # ç¶²çµ¡ä¿¡æ¯
            net_io = psutil.net_io_counters()
            
            # åƒåœ¾å›æ”¶çµ±è¨ˆ
            gc_stats = {i: gc.get_count()[i] for i in range(3)}
            
            metrics = PerformanceMetrics(
                cpu_usage=cpu_percent,
                memory_usage=memory.percent,
                memory_available=memory.available,
                thread_count=process_info.get('num_threads', 0),
                process_count=len(psutil.pids()),
                io_read_bytes=process_info.get('io_counters', {}).get('read_bytes', 0),
                io_write_bytes=process_info.get('io_counters', {}).get('write_bytes', 0),
                network_sent=net_io.bytes_sent if net_io else 0,
                network_recv=net_io.bytes_recv if net_io else 0,
                gc_collections=gc_stats
            )
            
            return metrics
            
        except Exception as e:
            logger.error(f"âŒ æŒ‡æ¨™æ”¶é›†å¤±æ•—: {e}")
            return PerformanceMetrics()
    
    def check_resource_limits(self, metrics: PerformanceMetrics):
        """æª¢æŸ¥è³‡æºé™åˆ¶"""
        alerts = []
        
        # CPUä½¿ç”¨ç‡æª¢æŸ¥
        if metrics.cpu_usage > self.limits.max_cpu_usage:
            alerts.append({
                'type': 'cpu_high',
                'message': f'CPUä½¿ç”¨ç‡éé«˜: {metrics.cpu_usage:.1f}% > {self.limits.max_cpu_usage}%',
                'severity': 'warning',
                'timestamp': metrics.timestamp
            })
        
        # å…§å­˜ä½¿ç”¨ç‡æª¢æŸ¥
        if metrics.memory_usage > self.limits.max_memory_usage:
            alerts.append({
                'type': 'memory_high',
                'message': f'å…§å­˜ä½¿ç”¨ç‡éé«˜: {metrics.memory_usage:.1f}% > {self.limits.max_memory_usage}%',
                'severity': 'warning',
                'timestamp': metrics.timestamp
            })
        
        # ç·šç¨‹æ•¸æª¢æŸ¥
        if metrics.thread_count > self.limits.max_threads:
            alerts.append({
                'type': 'thread_high',
                'message': f'ç·šç¨‹æ•¸éå¤š: {metrics.thread_count} > {self.limits.max_threads}',
                'severity': 'error',
                'timestamp': metrics.timestamp
            })
        
        # è¨˜éŒ„è­¦å ±
        for alert in alerts:
            self.alerts.append(alert)
            logger.warning(f"âš ï¸ è³‡æºè­¦å ±: {alert['message']}")
        
        # ä¿æŒè­¦å ±åˆ—è¡¨å¤§å°
        if len(self.alerts) > 100:
            self.alerts = self.alerts[-50:]
    
    def get_current_metrics(self) -> Optional[PerformanceMetrics]:
        """ç²å–ç•¶å‰æŒ‡æ¨™"""
        return self.metrics_history[-1] if self.metrics_history else None
    
    def get_metrics_summary(self) -> Dict[str, Any]:
        """ç²å–æŒ‡æ¨™æ‘˜è¦"""
        if not self.metrics_history:
            return {}
        
        recent_metrics = list(self.metrics_history)[-60:]  # æœ€è¿‘60æ¬¡è¨˜éŒ„
        
        return {
            'avg_cpu_usage': sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics),
            'avg_memory_usage': sum(m.memory_usage for m in recent_metrics) / len(recent_metrics),
            'avg_thread_count': sum(m.thread_count for m in recent_metrics) / len(recent_metrics),
            'peak_cpu_usage': max(m.cpu_usage for m in recent_metrics),
            'peak_memory_usage': max(m.memory_usage for m in recent_metrics),
            'peak_thread_count': max(m.thread_count for m in recent_metrics),
            'total_alerts': len(self.alerts),
            'recent_alerts': len([a for a in self.alerts if a['timestamp'] > datetime.now() - timedelta(minutes=10)])
        }

class SystemPerformanceOptimizer:
    """ç³»çµ±æ€§èƒ½å„ªåŒ–å™¨ä¸»é¡"""
    
    def __init__(self, limits: ResourceLimits = None):
        self.limits = limits or ResourceLimits()
        self.thread_pool_manager = ThreadPoolManager()
        self.memory_manager = MemoryManager(self.limits.gc_threshold)
        self.api_rate_limiter = APIRateLimiter(self.limits.api_rate_limit)
        self.resource_monitor = ResourceMonitor(self.limits)
        
        self.optimization_stats = {
            'start_time': datetime.now(),
            'optimizations_applied': 0,
            'performance_improvements': [],
            'resource_savings': {}
        }
        
        logger.info("ğŸš€ ç³»çµ±æ€§èƒ½å„ªåŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def start_optimization(self):
        """é–‹å§‹æ€§èƒ½å„ªåŒ–"""
        try:
            logger.info("ğŸ”§ é–‹å§‹ç³»çµ±æ€§èƒ½å„ªåŒ–")
            
            # å•Ÿå‹•è³‡æºç›£æ§
            self.resource_monitor.start_monitoring(interval=5.0)
            
            # æ‡‰ç”¨åˆå§‹å„ªåŒ–
            self.apply_initial_optimizations()
            
            # è¨»å†Šç·©å­˜
            self.register_system_caches()
            
            logger.info("âœ… ç³»çµ±æ€§èƒ½å„ªåŒ–å•Ÿå‹•å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½å„ªåŒ–å•Ÿå‹•å¤±æ•—: {e}")
    
    def stop_optimization(self):
        """åœæ­¢æ€§èƒ½å„ªåŒ–"""
        try:
            logger.info("â¹ï¸ åœæ­¢ç³»çµ±æ€§èƒ½å„ªåŒ–")
            
            # åœæ­¢è³‡æºç›£æ§
            self.resource_monitor.stop_monitoring()
            
            # é—œé–‰ç·šç¨‹æ± 
            self.thread_pool_manager.shutdown_all()
            
            # æœ€çµ‚åƒåœ¾å›æ”¶
            self.memory_manager.force_garbage_collection()
            
            logger.info("âœ… ç³»çµ±æ€§èƒ½å„ªåŒ–åœæ­¢å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½å„ªåŒ–åœæ­¢å¤±æ•—: {e}")
    
    def apply_initial_optimizations(self):
        """æ‡‰ç”¨åˆå§‹å„ªåŒ–"""
        try:
            optimizations = []
            
            # 1. è¨­ç½®åƒåœ¾å›æ”¶é–¾å€¼
            original_thresholds = gc.get_threshold()
            new_thresholds = (700, 10, 10)  # æ›´ç©æ¥µçš„åƒåœ¾å›æ”¶
            gc.set_threshold(*new_thresholds)
            optimizations.append(f"åƒåœ¾å›æ”¶é–¾å€¼: {original_thresholds} â†’ {new_thresholds}")
            
            # 2. è¨­ç½®è³‡æºé™åˆ¶ (åƒ…åœ¨æ”¯æŒçš„å¹³å°ä¸Š)
            if RESOURCE_AVAILABLE:
                try:
                    # è¨­ç½®æœ€å¤§æ–‡ä»¶æè¿°ç¬¦æ•¸
                    soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
                    new_soft = min(hard, 4096)
                    resource.setrlimit(resource.RLIMIT_NOFILE, (new_soft, hard))
                    optimizations.append(f"æ–‡ä»¶æè¿°ç¬¦é™åˆ¶: {soft} â†’ {new_soft}")
                except:
                    optimizations.append("è³‡æºé™åˆ¶è¨­ç½®è·³é (å¹³å°ä¸æ”¯æŒ)")
            else:
                optimizations.append("è³‡æºé™åˆ¶è¨­ç½®è·³é (Windowså¹³å°)")
            
            # 3. å„ªåŒ–ç·šç¨‹æ± é…ç½®
            cpu_count = os.cpu_count() or 1
            optimal_threads = min(32, cpu_count * 2)
            self.thread_pool_manager.max_workers = optimal_threads
            optimizations.append(f"ç·šç¨‹æ± å¤§å°: {optimal_threads}")
            
            # 4. å•Ÿç”¨å…§å­˜ç›£æ§
            self.memory_manager.monitor_memory()
            optimizations.append("å…§å­˜ç›£æ§å·²å•Ÿç”¨")
            
            self.optimization_stats['optimizations_applied'] = len(optimizations)
            self.optimization_stats['performance_improvements'] = optimizations
            
            logger.info(f"âœ… æ‡‰ç”¨äº† {len(optimizations)} é …åˆå§‹å„ªåŒ–")
            for opt in optimizations:
                logger.info(f"   â€¢ {opt}")
                
        except Exception as e:
            logger.error(f"âŒ åˆå§‹å„ªåŒ–æ‡‰ç”¨å¤±æ•—: {e}")
    
    def register_system_caches(self):
        """è¨»å†Šç³»çµ±ç·©å­˜"""
        try:
            # é€™è£¡å¯ä»¥è¨»å†Šå„ç¨®ç³»çµ±ç·©å­˜
            # ä¾‹å¦‚ï¼šæ•¸æ“šç·©å­˜ã€AIæ¨¡å‹ç·©å­˜ã€APIéŸ¿æ‡‰ç·©å­˜ç­‰
            
            caches = [
                "market_data_cache",
                "ai_prediction_cache", 
                "technical_indicator_cache",
                "api_response_cache"
            ]
            
            for cache_name in caches:
                # å‰µå»ºæ¨¡æ“¬ç·©å­˜å°è±¡
                cache_obj = {}
                self.memory_manager.register_cache(cache_name, cache_obj)
            
            logger.info(f"âœ… è¨»å†Šäº† {len(caches)} å€‹ç³»çµ±ç·©å­˜")
            
        except Exception as e:
            logger.error(f"âŒ ç³»çµ±ç·©å­˜è¨»å†Šå¤±æ•—: {e}")
    
    def optimize_for_trading_pairs(self, pair_count: int) -> Dict[str, Any]:
        """é‡å°äº¤æ˜“å°æ•¸é‡å„ªåŒ–ç³»çµ±"""
        try:
            logger.info(f"ğŸ¯ é‡å° {pair_count} å€‹äº¤æ˜“å°å„ªåŒ–ç³»çµ±")
            
            optimizations = {}
            
            # 1. å‹•æ…‹èª¿æ•´ç·šç¨‹æ± å¤§å°
            optimal_threads = min(50, max(8, pair_count * 2))
            self.thread_pool_manager.max_workers = optimal_threads
            optimizations['thread_pool_size'] = optimal_threads
            
            # 2. èª¿æ•´APIé™æµ
            optimal_rate_limit = min(200, max(50, pair_count * 20))
            self.api_rate_limiter.rate_limit = optimal_rate_limit
            optimizations['api_rate_limit'] = optimal_rate_limit
            
            # 3. èª¿æ•´å…§å­˜ç®¡ç†
            if pair_count > 10:
                # æ›´é »ç¹çš„åƒåœ¾å›æ”¶
                gc.set_threshold(500, 8, 8)
                optimizations['gc_threshold'] = 'aggressive'
            else:
                # æ¨™æº–åƒåœ¾å›æ”¶
                gc.set_threshold(700, 10, 10)
                optimizations['gc_threshold'] = 'standard'
            
            # 4. èª¿æ•´è³‡æºé™åˆ¶
            self.limits.max_threads = optimal_threads + 10
            self.limits.max_memory_usage = min(80.0, 50.0 + pair_count * 2)
            optimizations['memory_limit'] = self.limits.max_memory_usage
            
            logger.info(f"âœ… äº¤æ˜“å°å„ªåŒ–å®Œæˆ: {optimizations}")
            return optimizations
            
        except Exception as e:
            logger.error(f"âŒ äº¤æ˜“å°å„ªåŒ–å¤±æ•—: {e}")
            return {}
    
    def get_optimization_report(self) -> Dict[str, Any]:
        """ç²å–å„ªåŒ–å ±å‘Š"""
        try:
            current_metrics = self.resource_monitor.get_current_metrics()
            metrics_summary = self.resource_monitor.get_metrics_summary()
            memory_stats = self.memory_manager.get_memory_stats()
            thread_pool_stats = self.thread_pool_manager.get_pool_stats()
            api_stats = self.api_rate_limiter.get_stats()
            
            report = {
                'optimization_info': {
                    'start_time': self.optimization_stats['start_time'].isoformat(),
                    'running_time': (datetime.now() - self.optimization_stats['start_time']).total_seconds(),
                    'optimizations_applied': self.optimization_stats['optimizations_applied'],
                    'improvements': self.optimization_stats['performance_improvements']
                },
                'current_performance': {
                    'cpu_usage': current_metrics.cpu_usage if current_metrics else 0,
                    'memory_usage': current_metrics.memory_usage if current_metrics else 0,
                    'thread_count': current_metrics.thread_count if current_metrics else 0,
                    'timestamp': current_metrics.timestamp.isoformat() if current_metrics else None
                },
                'performance_summary': metrics_summary,
                'memory_management': memory_stats,
                'thread_pools': thread_pool_stats,
                'api_rate_limiting': api_stats,
                'resource_limits': {
                    'max_cpu_usage': self.limits.max_cpu_usage,
                    'max_memory_usage': self.limits.max_memory_usage,
                    'max_threads': self.limits.max_threads,
                    'api_rate_limit': self.limits.api_rate_limit
                },
                'alerts': self.resource_monitor.alerts[-10:] if self.resource_monitor.alerts else []
            }
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ å„ªåŒ–å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
            return {'error': str(e)}

# å…¨å±€å„ªåŒ–å™¨å¯¦ä¾‹
_global_optimizer: Optional[SystemPerformanceOptimizer] = None

def get_performance_optimizer() -> SystemPerformanceOptimizer:
    """ç²å–å…¨å±€æ€§èƒ½å„ªåŒ–å™¨å¯¦ä¾‹"""
    global _global_optimizer
    if _global_optimizer is None:
        _global_optimizer = SystemPerformanceOptimizer()
    return _global_optimizer

def start_system_optimization():
    """å•Ÿå‹•ç³»çµ±å„ªåŒ–"""
    optimizer = get_performance_optimizer()
    optimizer.start_optimization()
    return optimizer

def stop_system_optimization():
    """åœæ­¢ç³»çµ±å„ªåŒ–"""
    global _global_optimizer
    if _global_optimizer:
        _global_optimizer.stop_optimization()
        _global_optimizer = None

# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # æ¸¬è©¦æ€§èƒ½å„ªåŒ–å™¨
    print("ğŸ§ª æ¸¬è©¦ç³»çµ±æ€§èƒ½å„ªåŒ–å™¨...")
    
    optimizer = SystemPerformanceOptimizer()
    
    try:
        # å•Ÿå‹•å„ªåŒ–
        optimizer.start_optimization()
        
        # é‡å°5å€‹äº¤æ˜“å°å„ªåŒ–
        pair_optimizations = optimizer.optimize_for_trading_pairs(5)
        print(f"äº¤æ˜“å°å„ªåŒ–: {pair_optimizations}")
        
        # ç­‰å¾…ä¸€æ®µæ™‚é–“æ”¶é›†æŒ‡æ¨™
        time.sleep(10)
        
        # ç²å–å„ªåŒ–å ±å‘Š
        report = optimizer.get_optimization_report()
        print(f"å„ªåŒ–å ±å‘Š: {json.dumps(report, indent=2, default=str)}")
        
    finally:
        # åœæ­¢å„ªåŒ–
        optimizer.stop_optimization()
    
    print("âœ… æ€§èƒ½å„ªåŒ–å™¨æ¸¬è©¦å®Œæˆ")