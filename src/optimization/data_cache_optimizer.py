#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AImax æ•¸æ“šç·©å­˜å„ªåŒ–å™¨ - å¯¦ç¾æ•¸æ“šç·©å­˜å’Œä¸¦è¡Œè™•ç†
"""

import asyncio
import logging
import time
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import hashlib
from collections import OrderedDict
import sqlite3
import pickle

logger = logging.getLogger(__name__)

@dataclass
class CacheEntry:
    """ç·©å­˜æ¢ç›®"""
    key: str
    data: Any
    timestamp: datetime
    access_count: int
    size_bytes: int

class DataCacheOptimizer:
    """æ•¸æ“šç·©å­˜å„ªåŒ–å™¨ - å°ˆç‚ºAImaxç³»çµ±è¨­è¨ˆ"""
    
    def __init__(self, config_path: str = "AImax/config/cache.json"):
        """
        åˆå§‹åŒ–æ•¸æ“šç·©å­˜å„ªåŒ–å™¨
        
        Args:
            config_path: ç·©å­˜é…ç½®æ–‡ä»¶è·¯å¾‘
        """
        self.config = self._load_config(config_path)
        
        # å…§å­˜ç·©å­˜
        self.memory_cache = OrderedDict()
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'total_size': 0
        }
        
        # æŒä¹…åŒ–ç·©å­˜
        self.db_path = self.config.get('db_path', 'AImax/data/cache/data_cache.db')
        self._init_persistent_cache()
        
        # ä¸¦è¡Œè™•ç†é…ç½®
        self.thread_pool = ThreadPoolExecutor(
            max_workers=self.config.get('max_workers', 4)
        )
        
        # ç·©å­˜æ¸…ç†ç·šç¨‹
        self.cleanup_thread = None
        self.cleanup_active = False
        
        logger.info("ğŸ¯ æ•¸æ“šç·©å­˜å„ªåŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """è¼‰å…¥ç·©å­˜é…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"âœ… è¼‰å…¥ç·©å­˜é…ç½®: {config_path}")
            return config
        except Exception as e:
            logger.warning(f"âš ï¸ è¼‰å…¥é…ç½®å¤±æ•—ï¼Œä½¿ç”¨é»˜èªé…ç½®: {e}")
            return {
                'max_memory_cache_size': 100,  # æœ€å¤§ç·©å­˜æ¢ç›®æ•¸
                'max_memory_size_mb': 500,     # æœ€å¤§å…§å­˜ä½¿ç”¨MB
                'cache_ttl_seconds': 3600,     # ç·©å­˜éæœŸæ™‚é–“1å°æ™‚
                'cleanup_interval_seconds': 300,  # æ¸…ç†é–“éš”5åˆ†é˜
                'max_workers': 4,              # ä¸¦è¡Œå·¥ä½œç·šç¨‹æ•¸
                'enable_persistent_cache': True,
                'db_path': 'AImax/data/cache/data_cache.db'
            }
    
    def _init_persistent_cache(self):
        """åˆå§‹åŒ–æŒä¹…åŒ–ç·©å­˜"""
        try:
            if not self.config.get('enable_persistent_cache', True):
                return
            
            # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            import os
            os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
            
            # å‰µå»ºæ•¸æ“šåº«è¡¨
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('''
                    CREATE TABLE IF NOT EXISTS cache_entries (
                        key TEXT PRIMARY KEY,
                        data BLOB,
                        timestamp TEXT,
                        access_count INTEGER,
                        size_bytes INTEGER
                    )
                ''')
                conn.commit()
            
            logger.info(f"âœ… æŒä¹…åŒ–ç·©å­˜åˆå§‹åŒ–å®Œæˆ: {self.db_path}")
            
        except Exception as e:
            logger.error(f"âŒ æŒä¹…åŒ–ç·©å­˜åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def start_cleanup_service(self):
        """å•Ÿå‹•ç·©å­˜æ¸…ç†æœå‹™"""
        try:
            if self.cleanup_active:
                logger.warning("âš ï¸ ç·©å­˜æ¸…ç†æœå‹™å·²åœ¨é‹è¡Œ")
                return
            
            self.cleanup_active = True
            self.cleanup_thread = threading.Thread(
                target=self._cleanup_loop,
                daemon=True
            )
            self.cleanup_thread.start()
            
            logger.info("ğŸ§¹ ç·©å­˜æ¸…ç†æœå‹™å·²å•Ÿå‹•")
            
        except Exception as e:
            logger.error(f"âŒ å•Ÿå‹•ç·©å­˜æ¸…ç†æœå‹™å¤±æ•—: {e}")
    
    def stop_cleanup_service(self):
        """åœæ­¢ç·©å­˜æ¸…ç†æœå‹™"""
        try:
            self.cleanup_active = False
            if self.cleanup_thread:
                self.cleanup_thread.join(timeout=5.0)
            
            logger.info("ğŸ§¹ ç·©å­˜æ¸…ç†æœå‹™å·²åœæ­¢")
            
        except Exception as e:
            logger.error(f"âŒ åœæ­¢ç·©å­˜æ¸…ç†æœå‹™å¤±æ•—: {e}")
    
    def _cleanup_loop(self):
        """ç·©å­˜æ¸…ç†å¾ªç’°"""
        try:
            interval = self.config.get('cleanup_interval_seconds', 300)
            
            while self.cleanup_active:
                self._cleanup_expired_cache()
                self._cleanup_oversized_cache()
                time.sleep(interval)
                
        except Exception as e:
            logger.error(f"âŒ ç·©å­˜æ¸…ç†å¾ªç’°ç•°å¸¸: {e}")
    
    def _cleanup_expired_cache(self):
        """æ¸…ç†éæœŸç·©å­˜"""
        try:
            ttl_seconds = self.config.get('cache_ttl_seconds', 3600)
            cutoff_time = datetime.now() - timedelta(seconds=ttl_seconds)
            
            # æ¸…ç†å…§å­˜ç·©å­˜
            expired_keys = []
            for key, entry in self.memory_cache.items():
                if entry.timestamp < cutoff_time:
                    expired_keys.append(key)
            
            for key in expired_keys:
                del self.memory_cache[key]
                self.cache_stats['evictions'] += 1
            
            if expired_keys:
                logger.info(f"ğŸ§¹ æ¸…ç†éæœŸå…§å­˜ç·©å­˜: {len(expired_keys)} é …")
            
            # æ¸…ç†æŒä¹…åŒ–ç·©å­˜
            if self.config.get('enable_persistent_cache', True):
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.execute(
                        'DELETE FROM cache_entries WHERE timestamp < ?',
                        (cutoff_time.isoformat(),)
                    )
                    deleted_count = cursor.rowcount
                    conn.commit()
                    
                    if deleted_count > 0:
                        logger.info(f"ğŸ§¹ æ¸…ç†éæœŸæŒä¹…åŒ–ç·©å­˜: {deleted_count} é …")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†éæœŸç·©å­˜å¤±æ•—: {e}")
    
    def _cleanup_oversized_cache(self):
        """æ¸…ç†è¶…å¤§ç·©å­˜"""
        try:
            max_size = self.config.get('max_memory_cache_size', 100)
            
            # å¦‚æœç·©å­˜è¶…éé™åˆ¶ï¼Œç§»é™¤æœ€å°‘ä½¿ç”¨çš„é …ç›®
            while len(self.memory_cache) > max_size:
                # OrderedDictçš„ç¬¬ä¸€å€‹é …ç›®æ˜¯æœ€èˆŠçš„
                oldest_key = next(iter(self.memory_cache))
                del self.memory_cache[oldest_key]
                self.cache_stats['evictions'] += 1
            
            # æª¢æŸ¥å…§å­˜ä½¿ç”¨
            total_size_mb = sum(entry.size_bytes for entry in self.memory_cache.values()) / 1024 / 1024
            max_memory_mb = self.config.get('max_memory_size_mb', 500)
            
            if total_size_mb > max_memory_mb:
                # ç§»é™¤å¤§å‹ç·©å­˜é …
                sorted_entries = sorted(
                    self.memory_cache.items(),
                    key=lambda x: x[1].size_bytes,
                    reverse=True
                )
                
                removed_count = 0
                for key, entry in sorted_entries:
                    if total_size_mb <= max_memory_mb:
                        break
                    
                    total_size_mb -= entry.size_bytes / 1024 / 1024
                    del self.memory_cache[key]
                    self.cache_stats['evictions'] += 1
                    removed_count += 1
                
                if removed_count > 0:
                    logger.info(f"ğŸ§¹ æ¸…ç†å¤§å‹ç·©å­˜é …: {removed_count} é …")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†è¶…å¤§ç·©å­˜å¤±æ•—: {e}")
    
    def get_cache_key(self, data_type: str, params: Dict[str, Any]) -> str:
        """ç”Ÿæˆç·©å­˜éµ"""
        try:
            # å‰µå»ºç©©å®šçš„åƒæ•¸å­—ç¬¦ä¸²
            param_str = json.dumps(params, sort_keys=True)
            
            # ç”Ÿæˆå“ˆå¸Œ
            hash_obj = hashlib.md5(f"{data_type}:{param_str}".encode())
            return f"cache_{data_type}_{hash_obj.hexdigest()[:16]}"
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç·©å­˜éµå¤±æ•—: {e}")
            return f"cache_{data_type}_{int(time.time())}"
    
    async def get_cached_data(self, cache_key: str) -> Optional[Any]:
        """ç²å–ç·©å­˜æ•¸æ“š"""
        try:
            # é¦–å…ˆæª¢æŸ¥å…§å­˜ç·©å­˜
            if cache_key in self.memory_cache:
                entry = self.memory_cache[cache_key]
                
                # æª¢æŸ¥æ˜¯å¦éæœŸ
                ttl_seconds = self.config.get('cache_ttl_seconds', 3600)
                if datetime.now() - entry.timestamp < timedelta(seconds=ttl_seconds):
                    # æ›´æ–°è¨ªå•çµ±è¨ˆ
                    entry.access_count += 1
                    # ç§»å‹•åˆ°æœ«å°¾ï¼ˆLRUï¼‰
                    self.memory_cache.move_to_end(cache_key)
                    self.cache_stats['hits'] += 1
                    
                    logger.debug(f"ğŸ¯ å…§å­˜ç·©å­˜å‘½ä¸­: {cache_key}")
                    return entry.data
                else:
                    # éæœŸï¼Œç§»é™¤
                    del self.memory_cache[cache_key]
            
            # æª¢æŸ¥æŒä¹…åŒ–ç·©å­˜
            if self.config.get('enable_persistent_cache', True):
                persistent_data = await self._get_persistent_cache(cache_key)
                if persistent_data is not None:
                    # åŠ è¼‰åˆ°å…§å­˜ç·©å­˜
                    await self.set_cached_data(cache_key, persistent_data)
                    self.cache_stats['hits'] += 1
                    
                    logger.debug(f"ğŸ¯ æŒä¹…åŒ–ç·©å­˜å‘½ä¸­: {cache_key}")
                    return persistent_data
            
            # ç·©å­˜æœªå‘½ä¸­
            self.cache_stats['misses'] += 1
            logger.debug(f"âŒ ç·©å­˜æœªå‘½ä¸­: {cache_key}")
            return None
            
        except Exception as e:
            logger.error(f"âŒ ç²å–ç·©å­˜æ•¸æ“šå¤±æ•—: {e}")
            self.cache_stats['misses'] += 1
            return None
    
    async def set_cached_data(self, cache_key: str, data: Any):
        """è¨­ç½®ç·©å­˜æ•¸æ“š"""
        try:
            # è¨ˆç®—æ•¸æ“šå¤§å°
            data_bytes = pickle.dumps(data)
            size_bytes = len(data_bytes)
            
            # å‰µå»ºç·©å­˜æ¢ç›®
            entry = CacheEntry(
                key=cache_key,
                data=data,
                timestamp=datetime.now(),
                access_count=1,
                size_bytes=size_bytes
            )
            
            # å­˜å„²åˆ°å…§å­˜ç·©å­˜
            self.memory_cache[cache_key] = entry
            self.memory_cache.move_to_end(cache_key)  # ç§»å‹•åˆ°æœ«å°¾
            
            # æ›´æ–°çµ±è¨ˆ
            self.cache_stats['total_size'] += size_bytes
            
            # å­˜å„²åˆ°æŒä¹…åŒ–ç·©å­˜
            if self.config.get('enable_persistent_cache', True):
                await self._set_persistent_cache(cache_key, data, entry)
            
            logger.debug(f"ğŸ’¾ ç·©å­˜æ•¸æ“šå·²å­˜å„²: {cache_key} ({size_bytes} bytes)")
            
        except Exception as e:
            logger.error(f"âŒ è¨­ç½®ç·©å­˜æ•¸æ“šå¤±æ•—: {e}")
    
    async def _get_persistent_cache(self, cache_key: str) -> Optional[Any]:
        """å¾æŒä¹…åŒ–ç·©å­˜ç²å–æ•¸æ“š"""
        try:
            def get_from_db():
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.execute(
                        'SELECT data, timestamp FROM cache_entries WHERE key = ?',
                        (cache_key,)
                    )
                    row = cursor.fetchone()
                    
                    if row:
                        data_blob, timestamp_str = row
                        timestamp = datetime.fromisoformat(timestamp_str)
                        
                        # æª¢æŸ¥æ˜¯å¦éæœŸ
                        ttl_seconds = self.config.get('cache_ttl_seconds', 3600)
                        if datetime.now() - timestamp < timedelta(seconds=ttl_seconds):
                            return pickle.loads(data_blob)
                        else:
                            # éæœŸï¼Œåˆªé™¤
                            conn.execute('DELETE FROM cache_entries WHERE key = ?', (cache_key,))
                            conn.commit()
                    
                    return None
            
            # åœ¨ç·šç¨‹æ± ä¸­åŸ·è¡Œæ•¸æ“šåº«æ“ä½œ
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(self.thread_pool, get_from_db)
            
        except Exception as e:
            logger.error(f"âŒ å¾æŒä¹…åŒ–ç·©å­˜ç²å–æ•¸æ“šå¤±æ•—: {e}")
            return None
    
    async def _set_persistent_cache(self, cache_key: str, data: Any, entry: CacheEntry):
        """è¨­ç½®æŒä¹…åŒ–ç·©å­˜"""
        try:
            def set_to_db():
                data_blob = pickle.dumps(data)
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute('''
                        INSERT OR REPLACE INTO cache_entries 
                        (key, data, timestamp, access_count, size_bytes)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (
                        cache_key,
                        data_blob,
                        entry.timestamp.isoformat(),
                        entry.access_count,
                        entry.size_bytes
                    ))
                    conn.commit()
            
            # åœ¨ç·šç¨‹æ± ä¸­åŸ·è¡Œæ•¸æ“šåº«æ“ä½œ
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(self.thread_pool, set_to_db)
            
        except Exception as e:
            logger.error(f"âŒ è¨­ç½®æŒä¹…åŒ–ç·©å­˜å¤±æ•—: {e}")
    
    async def parallel_data_fetch(self, fetch_tasks: List[Callable]) -> List[Any]:
        """ä¸¦è¡Œæ•¸æ“šç²å–"""
        try:
            logger.info(f"âš¡ é–‹å§‹ä¸¦è¡Œæ•¸æ“šç²å–: {len(fetch_tasks)} å€‹ä»»å‹™")
            
            # å‰µå»ºç•°æ­¥ä»»å‹™
            tasks = []
            for i, fetch_func in enumerate(fetch_tasks):
                task = asyncio.create_task(self._execute_fetch_task(fetch_func, i))
                tasks.append(task)
            
            # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰ä»»å‹™
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # è™•ç†çµæœ
            successful_results = []
            failed_count = 0
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.warning(f"âš ï¸ ä»»å‹™ {i} å¤±æ•—: {result}")
                    failed_count += 1
                    successful_results.append(None)
                else:
                    successful_results.append(result)
            
            success_rate = (len(results) - failed_count) / len(results) if results else 0
            logger.info(f"âœ… ä¸¦è¡Œæ•¸æ“šç²å–å®Œæˆ: æˆåŠŸç‡ {success_rate:.1%}")
            
            return successful_results
            
        except Exception as e:
            logger.error(f"âŒ ä¸¦è¡Œæ•¸æ“šç²å–å¤±æ•—: {e}")
            return []
    
    async def _execute_fetch_task(self, fetch_func: Callable, task_id: int) -> Any:
        """åŸ·è¡Œå–®å€‹ç²å–ä»»å‹™"""
        try:
            # å¦‚æœæ˜¯å”ç¨‹å‡½æ•¸
            if asyncio.iscoroutinefunction(fetch_func):
                return await fetch_func()
            else:
                # åœ¨ç·šç¨‹æ± ä¸­åŸ·è¡ŒåŒæ­¥å‡½æ•¸
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(self.thread_pool, fetch_func)
                
        except Exception as e:
            logger.error(f"âŒ åŸ·è¡Œç²å–ä»»å‹™ {task_id} å¤±æ•—: {e}")
            raise
    
    async def cached_data_fetch(self, data_type: str, params: Dict[str, Any], 
                               fetch_func: Callable) -> Any:
        """å¸¶ç·©å­˜çš„æ•¸æ“šç²å–"""
        try:
            # ç”Ÿæˆç·©å­˜éµ
            cache_key = self.get_cache_key(data_type, params)
            
            # å˜—è©¦å¾ç·©å­˜ç²å–
            cached_data = await self.get_cached_data(cache_key)
            if cached_data is not None:
                logger.debug(f"ğŸ¯ ä½¿ç”¨ç·©å­˜æ•¸æ“š: {data_type}")
                return cached_data
            
            # ç·©å­˜æœªå‘½ä¸­ï¼Œç²å–æ–°æ•¸æ“š
            logger.debug(f"ğŸ“¡ ç²å–æ–°æ•¸æ“š: {data_type}")
            
            if asyncio.iscoroutinefunction(fetch_func):
                new_data = await fetch_func()
            else:
                loop = asyncio.get_event_loop()
                new_data = await loop.run_in_executor(self.thread_pool, fetch_func)
            
            # å­˜å„²åˆ°ç·©å­˜
            await self.set_cached_data(cache_key, new_data)
            
            return new_data
            
        except Exception as e:
            logger.error(f"âŒ å¸¶ç·©å­˜çš„æ•¸æ“šç²å–å¤±æ•—: {e}")
            raise
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ç²å–ç·©å­˜çµ±è¨ˆä¿¡æ¯"""
        try:
            total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
            hit_rate = self.cache_stats['hits'] / max(1, total_requests)
            
            # è¨ˆç®—å…§å­˜ä½¿ç”¨
            memory_usage_mb = sum(entry.size_bytes for entry in self.memory_cache.values()) / 1024 / 1024
            
            return {
                'cache_hits': self.cache_stats['hits'],
                'cache_misses': self.cache_stats['misses'],
                'hit_rate': hit_rate,
                'evictions': self.cache_stats['evictions'],
                'memory_cache_size': len(self.memory_cache),
                'memory_usage_mb': memory_usage_mb,
                'max_memory_cache_size': self.config.get('max_memory_cache_size', 100),
                'max_memory_size_mb': self.config.get('max_memory_size_mb', 500),
                'cleanup_active': self.cleanup_active,
                'persistent_cache_enabled': self.config.get('enable_persistent_cache', True)
            }
            
        except Exception as e:
            logger.error(f"âŒ ç²å–ç·©å­˜çµ±è¨ˆå¤±æ•—: {e}")
            return {'error': str(e)}
    
    def clear_cache(self, cache_type: str = 'all'):
        """æ¸…ç©ºç·©å­˜"""
        try:
            if cache_type in ['all', 'memory']:
                cleared_count = len(self.memory_cache)
                self.memory_cache.clear()
                self.cache_stats['evictions'] += cleared_count
                logger.info(f"ğŸ§¹ æ¸…ç©ºå…§å­˜ç·©å­˜: {cleared_count} é …")
            
            if cache_type in ['all', 'persistent'] and self.config.get('enable_persistent_cache', True):
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.execute('DELETE FROM cache_entries')
                    deleted_count = cursor.rowcount
                    conn.commit()
                    logger.info(f"ğŸ§¹ æ¸…ç©ºæŒä¹…åŒ–ç·©å­˜: {deleted_count} é …")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºç·©å­˜å¤±æ•—: {e}")
    
    def __del__(self):
        """æ¸…ç†è³‡æº"""
        try:
            self.stop_cleanup_service()
            if hasattr(self, 'thread_pool'):
                self.thread_pool.shutdown(wait=False)
        except:
            pass


def create_data_cache_optimizer() -> DataCacheOptimizer:
    """å‰µå»ºæ•¸æ“šç·©å­˜å„ªåŒ–å™¨å¯¦ä¾‹"""
    return DataCacheOptimizer()


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    import asyncio
    
    async def test_data_cache_optimizer():
        """æ¸¬è©¦æ•¸æ“šç·©å­˜å„ªåŒ–å™¨"""
        print("ğŸ§ª æ¸¬è©¦æ•¸æ“šç·©å­˜å„ªåŒ–å™¨...")
        
        optimizer = create_data_cache_optimizer()
        optimizer.start_cleanup_service()
        
        # æ¸¬è©¦ç·©å­˜åŠŸèƒ½
        async def mock_data_fetch():
            await asyncio.sleep(0.1)  # æ¨¡æ“¬ç¶²çµ¡å»¶é²
            return {'data': 'test_data', 'timestamp': datetime.now()}
        
        # ç¬¬ä¸€æ¬¡ç²å–ï¼ˆç·©å­˜æœªå‘½ä¸­ï¼‰
        start_time = time.time()
        data1 = await optimizer.cached_data_fetch(
            'test_data', {'param': 'value'}, mock_data_fetch
        )
        first_time = time.time() - start_time
        
        # ç¬¬äºŒæ¬¡ç²å–ï¼ˆç·©å­˜å‘½ä¸­ï¼‰
        start_time = time.time()
        data2 = await optimizer.cached_data_fetch(
            'test_data', {'param': 'value'}, mock_data_fetch
        )
        second_time = time.time() - start_time
        
        # æ¸¬è©¦ä¸¦è¡Œç²å–
        fetch_tasks = [mock_data_fetch for _ in range(3)]
        start_time = time.time()
        parallel_results = await optimizer.parallel_data_fetch(fetch_tasks)
        parallel_time = time.time() - start_time
        
        # ç²å–çµ±è¨ˆä¿¡æ¯
        stats = optimizer.get_cache_stats()
        
        print(f"âœ… ç·©å­˜æ¸¬è©¦çµæœ:")
        print(f"   ç¬¬ä¸€æ¬¡ç²å–æ™‚é–“: {first_time:.3f}s")
        print(f"   ç¬¬äºŒæ¬¡ç²å–æ™‚é–“: {second_time:.3f}s")
        print(f"   ç·©å­˜åŠ é€Ÿæ¯”: {first_time/second_time:.1f}x")
        print(f"   ä¸¦è¡Œç²å–æ™‚é–“: {parallel_time:.3f}s")
        print(f"   ç·©å­˜å‘½ä¸­ç‡: {stats['hit_rate']:.1%}")
        print(f"   å…§å­˜ç·©å­˜å¤§å°: {stats['memory_cache_size']}")
        
        optimizer.stop_cleanup_service()
        print("âœ… æ•¸æ“šç·©å­˜å„ªåŒ–å™¨æ¸¬è©¦å®Œæˆ!")
    
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(test_data_cache_optimizer())