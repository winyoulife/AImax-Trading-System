#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸¦è¡ŒAIç®¡ç†å™¨ - å„ªåŒ–AIæ¨ç†é€Ÿåº¦çš„ä¸¦è¡Œè™•ç†ç‰ˆæœ¬
"""

import asyncio
import json
import logging
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

import ollama

logger = logging.getLogger(__name__)

@dataclass
class ParallelAIResponse:
    """ä¸¦è¡ŒAIå›æ‡‰æ•¸æ“šçµæ§‹"""
    ai_role: str
    model_name: str
    response: str
    confidence: float
    processing_time: float
    timestamp: datetime
    success: bool
    error_message: Optional[str] = None
    parallel_execution: bool = True

@dataclass
class OptimizedDecision:
    """å„ªåŒ–å¾Œçš„å”ä½œæ±ºç­–çµæœ"""
    final_decision: str  # BUY, SELL, HOLD
    confidence: float
    consensus_level: float
    ai_responses: List[ParallelAIResponse]
    reasoning: str
    risk_level: str
    timestamp: datetime
    total_processing_time: float
    parallel_efficiency: float  # ä¸¦è¡Œæ•ˆç‡æ¯”ç‡

class ParallelAIManager:
    """ä¸¦è¡ŒAIç®¡ç†å™¨ - å„ªåŒ–ç‰ˆæœ¬"""
    
    def __init__(self, config_path: str = "config/ai_models_qwen7b.json"):
        """
        åˆå§‹åŒ–ä¸¦è¡ŒAIç®¡ç†å™¨
        
        Args:
            config_path: AIæ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾‘
        """
        self.config_path = Path(config_path)
        self.config = self._load_config()
        
        # å‰µå»ºå¤šå€‹Ollamaå®¢æˆ¶ç«¯å¯¦ä¾‹ä»¥æ”¯æŒä¸¦è¡Œ
        self.ollama_clients = [ollama.Client() for _ in range(3)]
        
        # AIæ¨¡å‹é…ç½®
        self.models = self.config["ai_models"]
        self.collaboration_settings = self.config["collaboration_settings"]
        
        # æ€§èƒ½çµ±è¨ˆ
        self.performance_stats = {
            "total_decisions": 0,
            "successful_decisions": 0,
            "parallel_decisions": 0,
            "average_processing_time": 0.0,
            "average_parallel_efficiency": 0.0,
            "speed_improvement": 0.0,
            "ai_availability": {
                "market_scanner": True,
                "deep_analyst": True,
                "decision_maker": True
            }
        }
        
        # ç·šç¨‹æ± åŸ·è¡Œå™¨
        self.executor = ThreadPoolExecutor(max_workers=3)
        
        logger.info("ğŸš€ ä¸¦è¡ŒAIç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self) -> Dict[str, Any]:
        """è¼‰å…¥AIæ¨¡å‹é…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"âœ… è¼‰å…¥AIé…ç½®: {self.config_path}")
            return config
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥AIé…ç½®å¤±æ•—: {e}")
            raise
    
    async def analyze_market_parallel(self, market_data: Dict[str, Any]) -> OptimizedDecision:
        """
        ä¸¦è¡Œåˆ†æå¸‚å ´æ•¸æ“š - å„ªåŒ–ç‰ˆæœ¬
        
        Args:
            market_data: å¸‚å ´æ•¸æ“š
            
        Returns:
            OptimizedDecision: å„ªåŒ–å¾Œçš„å”ä½œæ±ºç­–çµæœ
        """
        start_time = time.time()
        
        try:
            logger.info("ğŸš€ é–‹å§‹ä¸¦è¡ŒAIå”ä½œå¸‚å ´åˆ†æ")
            
            # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨ä¸¦è¡Œè™•ç†
            if self.collaboration_settings.get("enable_parallel_processing", True):
                # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰AIåˆ†æ
                ai_responses = await self._run_parallel_analysis(market_data)
                parallel_execution = True
            else:
                # é †åºåŸ·è¡Œï¼ˆå›é€€æ¨¡å¼ï¼‰
                ai_responses = await self._run_sequential_analysis(market_data)
                parallel_execution = False
            
            # ç”Ÿæˆæœ€çµ‚å”ä½œæ±ºç­–
            collaborative_decision = self._synthesize_optimized_decision(ai_responses, start_time)
            
            # è¨ˆç®—ä¸¦è¡Œæ•ˆç‡
            if parallel_execution:
                collaborative_decision.parallel_efficiency = self._calculate_parallel_efficiency(ai_responses)
            else:
                collaborative_decision.parallel_efficiency = 1.0
            
            # æ›´æ–°æ€§èƒ½çµ±è¨ˆ
            processing_time = time.time() - start_time
            self._update_performance_stats(collaborative_decision, processing_time, parallel_execution)
            
            logger.info(f"âœ… ä¸¦è¡ŒAIåˆ†æå®Œæˆ: {collaborative_decision.final_decision} (è€—æ™‚: {processing_time:.2f}s)")
            return collaborative_decision
            
        except Exception as e:
            logger.error(f"âŒ ä¸¦è¡ŒAIåˆ†æå¤±æ•—: {e}")
            return self._create_fallback_decision(str(e), time.time() - start_time)
    
    async def _run_parallel_analysis(self, market_data: Dict[str, Any]) -> List[ParallelAIResponse]:
        """ä¸¦è¡ŒåŸ·è¡ŒAIåˆ†æ"""
        try:
            # å‰µå»ºä¸¦è¡Œä»»å‹™
            tasks = []
            
            # ä»»å‹™1: å¸‚å ´æƒæå“¡
            scanner_task = asyncio.create_task(
                self._run_ai_model_async(
                    "market_scanner", 
                    self._build_scanner_prompt(market_data),
                    0  # ä½¿ç”¨ç¬¬ä¸€å€‹å®¢æˆ¶ç«¯
                )
            )
            tasks.append(scanner_task)
            
            # ä»»å‹™2: æ·±åº¦åˆ†æå¸«ï¼ˆå¯ä»¥ä¸¦è¡Œï¼Œå› ç‚ºä¸ä¾è³´æƒæå“¡çµæœï¼‰
            analyst_task = asyncio.create_task(
                self._run_ai_model_async(
                    "deep_analyst", 
                    self._build_analyst_prompt_independent(market_data),
                    1  # ä½¿ç”¨ç¬¬äºŒå€‹å®¢æˆ¶ç«¯
                )
            )
            tasks.append(analyst_task)
            
            # ç­‰å¾…å‰å…©å€‹ä»»å‹™å®Œæˆ
            scanner_response, analyst_response = await asyncio.gather(*tasks)
            
            # ä»»å‹™3: æœ€çµ‚æ±ºç­–è€…ï¼ˆéœ€è¦å‰å…©å€‹çµæœï¼‰
            decision_task = asyncio.create_task(
                self._run_ai_model_async(
                    "decision_maker",
                    self._build_decision_prompt(market_data, scanner_response, analyst_response),
                    2  # ä½¿ç”¨ç¬¬ä¸‰å€‹å®¢æˆ¶ç«¯
                )
            )
            
            decision_response = await decision_task
            
            return [scanner_response, analyst_response, decision_response]
            
        except Exception as e:
            logger.error(f"âŒ ä¸¦è¡Œåˆ†æåŸ·è¡Œå¤±æ•—: {e}")
            raise
    
    async def _run_sequential_analysis(self, market_data: Dict[str, Any]) -> List[ParallelAIResponse]:
        """é †åºåŸ·è¡ŒAIåˆ†æï¼ˆå›é€€æ¨¡å¼ï¼‰"""
        try:
            responses = []
            
            # é †åºåŸ·è¡Œ
            scanner_response = await self._run_ai_model_async(
                "market_scanner", 
                self._build_scanner_prompt(market_data),
                0
            )
            responses.append(scanner_response)
            
            analyst_response = await self._run_ai_model_async(
                "deep_analyst", 
                self._build_analyst_prompt(market_data, scanner_response),
                0
            )
            responses.append(analyst_response)
            
            decision_response = await self._run_ai_model_async(
                "decision_maker",
                self._build_decision_prompt(market_data, scanner_response, analyst_response),
                0
            )
            responses.append(decision_response)
            
            # æ¨™è¨˜ç‚ºéä¸¦è¡ŒåŸ·è¡Œ
            for response in responses:
                response.parallel_execution = False
            
            return responses
            
        except Exception as e:
            logger.error(f"âŒ é †åºåˆ†æåŸ·è¡Œå¤±æ•—: {e}")
            raise
    
    async def _run_ai_model_async(self, model_role: str, prompt: str, client_index: int) -> ParallelAIResponse:
        """ç•°æ­¥é‹è¡ŒAIæ¨¡å‹"""
        start_time = time.time()
        
        try:
            model_config = self.models[model_role]
            client = self.ollama_clients[client_index]
            
            # åœ¨ç·šç¨‹æ± ä¸­åŸ·è¡ŒAIèª¿ç”¨ä»¥é¿å…é˜»å¡
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                self.executor,
                self._call_ai_model_sync,
                client,
                model_config["model_name"],
                model_config["system_prompt"],
                prompt,
                model_config["max_tokens"],
                model_config["temperature"]
            )
            
            processing_time = time.time() - start_time
            
            return ParallelAIResponse(
                ai_role=model_role,
                model_name=model_config["model_name"],
                response=response,
                confidence=self._extract_confidence(response),
                processing_time=processing_time,
                timestamp=datetime.now(),
                success=True,
                parallel_execution=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ AIæ¨¡å‹åŸ·è¡Œå¤±æ•— ({model_role}): {e}")
            
            return ParallelAIResponse(
                ai_role=model_role,
                model_name=model_config["model_name"],
                response="",
                confidence=0.0,
                processing_time=processing_time,
                timestamp=datetime.now(),
                success=False,
                error_message=str(e),
                parallel_execution=True
            )
    
    def _call_ai_model_sync(self, client: ollama.Client, model_name: str, 
                           system_prompt: str, user_prompt: str, 
                           max_tokens: int, temperature: float) -> str:
        """åŒæ­¥èª¿ç”¨AIæ¨¡å‹ï¼ˆåœ¨ç·šç¨‹æ± ä¸­åŸ·è¡Œï¼‰"""
        try:
            response = client.chat(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                options={
                    "num_predict": max_tokens,
                    "temperature": temperature,
                    "top_p": 0.9
                }
            )
            
            return response['message']['content']
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥AIæ¨¡å‹èª¿ç”¨å¤±æ•— ({model_name}): {e}")
            raise
    
    def _build_scanner_prompt(self, market_data: Dict[str, Any]) -> str:
        """æ§‹å»ºå¸‚å ´æƒæå“¡æç¤ºè©"""
        if 'ai_formatted_data' in market_data:
            formatted_data = market_data['ai_formatted_data']
        else:
            formatted_data = f"""
å¸‚å ´æ•¸æ“šï¼š
- ç•¶å‰åƒ¹æ ¼: {market_data.get('current_price', 'N/A')} TWD
- 1åˆ†é˜è®ŠåŒ–: {market_data.get('price_change_1m', 'N/A')}%
- 5åˆ†é˜è®ŠåŒ–: {market_data.get('price_change_5m', 'N/A')}%
- æˆäº¤é‡æ¯”ç‡: {market_data.get('volume_ratio', 'N/A')}x
- RSI: {market_data.get('rsi', 'N/A')}
- MACD: {market_data.get('macd_trend', 'N/A')}
"""
        
        return f"""
ä½ æ˜¯å°ˆæ¥­çš„åŠ å¯†è²¨å¹£å¸‚å ´æƒæå“¡ï¼Œè² è²¬å¿«é€Ÿè­˜åˆ¥äº¤æ˜“æ©Ÿæœƒã€‚

{formatted_data}

è«‹åŸºæ–¼ä»¥ä¸Šæ•¸æ“šé€²è¡Œå¿«é€Ÿåˆ†æï¼š

1. å¸‚å ´è©•ä¼° (çœ‹æ¼²/çœ‹è·Œ/ä¸­æ€§)
2. é—œéµä¿¡è™Ÿè­˜åˆ¥ (åˆ—å‡º2-3å€‹æœ€é‡è¦çš„ä¿¡è™Ÿ)
3. æ©Ÿæœƒç­‰ç´š (é«˜/ä¸­/ä½)
4. ä¿¡å¿ƒåº¦ (0-100)
5. å»ºè­°æ“ä½œ (BUY/SELL/HOLD)

è¦æ±‚ï¼š
- ä¿æŒç°¡æ½”ï¼Œå°ˆæ³¨æ–¼æœ€é—œéµçš„ä¿¡è™Ÿ
- å„ªå…ˆè€ƒæ…®æŠ€è¡“æŒ‡æ¨™çµ„åˆ
- æ³¨æ„æˆäº¤é‡å’Œåƒ¹æ ¼çš„é…åˆ
- çµ¦å‡ºæ˜ç¢ºçš„æ•¸å­—ä¿¡å¿ƒåº¦

æ ¼å¼ï¼š
å¸‚å ´è©•ä¼°: [çœ‹æ¼²/çœ‹è·Œ/ä¸­æ€§]
é—œéµä¿¡è™Ÿ: [ä¿¡è™Ÿ1, ä¿¡è™Ÿ2, ä¿¡è™Ÿ3]
æ©Ÿæœƒç­‰ç´š: [é«˜/ä¸­/ä½]
å»ºè­°æ“ä½œ: [BUY/SELL/HOLD]
ä¿¡å¿ƒåº¦: [0-100æ•¸å­—]
"""
    
    def _build_analyst_prompt_independent(self, market_data: Dict[str, Any]) -> str:
        """æ§‹å»ºç¨ç«‹çš„æ·±åº¦åˆ†æå¸«æç¤ºè©ï¼ˆä¸ä¾è³´æƒæå“¡çµæœï¼‰"""
        return f"""
ä½ æ˜¯è³‡æ·±çš„åŠ å¯†è²¨å¹£æŠ€è¡“åˆ†æå¸«ï¼Œè«‹å°ç•¶å‰å¸‚å ´é€²è¡Œæ·±åº¦æŠ€è¡“åˆ†æï¼š

å¸‚å ´æ•¸æ“šï¼š
- ç•¶å‰åƒ¹æ ¼: {market_data.get('current_price', 'N/A')} TWD
- åƒ¹æ ¼è®ŠåŒ–: {market_data.get('price_changes', {})}
- æŠ€è¡“æŒ‡æ¨™: {market_data.get('technical_indicators', {})}
- æˆäº¤é‡åˆ†æ: {market_data.get('volume_analysis', {})}

è«‹æä¾›ï¼š
1. æŠ€è¡“æŒ‡æ¨™åˆ†æ
2. æ”¯æ’é˜»åŠ›ä½åˆ†æ
3. é¢¨éšªè©•ä¼°
4. è©³ç´°äº¤æ˜“å»ºè­°
5. ä¿¡å¿ƒåº¦ (0-100)

è«‹æä¾›å°ˆæ¥­çš„æŠ€è¡“åˆ†æå ±å‘Šã€‚
"""
    
    def _build_analyst_prompt(self, market_data: Dict[str, Any], scanner_response: ParallelAIResponse) -> str:
        """æ§‹å»ºæ·±åº¦åˆ†æå¸«æç¤ºè©ï¼ˆåŸºæ–¼æƒæå“¡çµæœï¼‰"""
        return f"""
åŸºæ–¼å¸‚å ´æƒæå“¡çš„åˆæ­¥åˆ†æï¼Œè«‹é€²è¡Œæ·±åº¦æŠ€è¡“åˆ†æï¼š

å¸‚å ´æƒæå“¡å ±å‘Šï¼š
{scanner_response.response if scanner_response.success else "æƒæå“¡åˆ†æå¤±æ•—"}

è©³ç´°å¸‚å ´æ•¸æ“šï¼š
- ç•¶å‰åƒ¹æ ¼: {market_data.get('current_price', 'N/A')} TWD
- åƒ¹æ ¼è®ŠåŒ–: {market_data.get('price_changes', {})}
- æŠ€è¡“æŒ‡æ¨™: {market_data.get('technical_indicators', {})}
- æˆäº¤é‡åˆ†æ: {market_data.get('volume_analysis', {})}

è«‹æä¾›ï¼š
1. æŠ€è¡“æŒ‡æ¨™åˆ†æ
2. æ”¯æ’é˜»åŠ›ä½åˆ†æ
3. é¢¨éšªè©•ä¼°
4. è©³ç´°äº¤æ˜“å»ºè­°
5. ä¿¡å¿ƒåº¦ (0-100)

è«‹æä¾›å°ˆæ¥­çš„æŠ€è¡“åˆ†æå ±å‘Šã€‚
"""
    
    def _build_decision_prompt(self, market_data: Dict[str, Any], 
                             scanner_response: ParallelAIResponse, 
                             analyst_response: ParallelAIResponse) -> str:
        """æ§‹å»ºæœ€çµ‚æ±ºç­–è€…æç¤ºè©"""
        return f"""
åŸºæ–¼å¸‚å ´æƒæå“¡å’Œæ·±åº¦åˆ†æå¸«çš„å ±å‘Šï¼Œè«‹åšå‡ºæœ€çµ‚äº¤æ˜“æ±ºç­–ï¼š

å¸‚å ´æƒæå“¡å ±å‘Šï¼š
{scanner_response.response if scanner_response.success else "æƒæå“¡åˆ†æå¤±æ•—"}

æ·±åº¦åˆ†æå¸«å ±å‘Šï¼š
{analyst_response.response if analyst_response.success else "åˆ†æå¸«åˆ†æå¤±æ•—"}

è«‹ç¶œåˆåˆ†æä¸¦æä¾›ï¼š
1. æœ€çµ‚æ±ºç­–: BUY/SELL/HOLD
2. æ±ºç­–ç†ç”±
3. å»ºè­°å€‰ä½å¤§å° (%)
4. æ­¢æé»
5. ç›®æ¨™åƒ¹ä½
6. ä¿¡å¿ƒåº¦ (0-100)

è«‹åšå‡ºæ˜ç¢ºçš„äº¤æ˜“æ±ºç­–ã€‚
"""
    
    def _extract_confidence(self, response: str) -> float:
        """å¾AIå›æ‡‰ä¸­æå–ä¿¡å¿ƒåº¦"""
        try:
            import re
            confidence_match = re.search(r'ä¿¡å¿ƒåº¦[ï¼š:]\s*(\d+)', response)
            if confidence_match:
                return float(confidence_match.group(1)) / 100.0
            
            if "å¼·çƒˆ" in response or "ç¢ºä¿¡" in response:
                return 0.8
            elif "å¯èƒ½" in response or "å»ºè­°" in response:
                return 0.6
            else:
                return 0.5
                
        except Exception:
            return 0.5
    
    def _synthesize_optimized_decision(self, ai_responses: List[ParallelAIResponse], 
                                     start_time: float) -> OptimizedDecision:
        """ç¶œåˆAIå›æ‡‰ç”Ÿæˆå„ªåŒ–æ±ºç­–"""
        try:
            # æå–æ±ºç­–
            decisions = []
            confidences = []
            
            for response in ai_responses:
                if response.success:
                    decision = self._extract_decision(response.response)
                    decisions.append(decision)
                    confidences.append(response.confidence)
            
            # è¨ˆç®—å…±è­˜æ°´å¹³
            consensus_level = self._calculate_consensus(decisions)
            
            # ç¢ºå®šæœ€çµ‚æ±ºç­–
            final_decision = self._determine_final_decision(decisions, confidences)
            
            # è¨ˆç®—æ•´é«”ä¿¡å¿ƒåº¦
            overall_confidence = sum(confidences) / len(confidences) if confidences else 0.0
            
            # ç”Ÿæˆæ¨ç†èªªæ˜
            reasoning = self._generate_reasoning(ai_responses, final_decision)
            
            # è©•ä¼°é¢¨éšªç­‰ç´š
            risk_level = self._assess_risk_level(ai_responses, overall_confidence)
            
            # è¨ˆç®—ç¸½è™•ç†æ™‚é–“
            total_processing_time = time.time() - start_time
            
            return OptimizedDecision(
                final_decision=final_decision,
                confidence=overall_confidence,
                consensus_level=consensus_level,
                ai_responses=ai_responses,
                reasoning=reasoning,
                risk_level=risk_level,
                timestamp=datetime.now(),
                total_processing_time=total_processing_time,
                parallel_efficiency=0.0  # å°‡åœ¨å¤–éƒ¨è¨ˆç®—
            )
            
        except Exception as e:
            logger.error(f"âŒ å„ªåŒ–æ±ºç­–ç¶œåˆå¤±æ•—: {e}")
            return self._create_fallback_decision(str(e), time.time() - start_time)
    
    def _calculate_parallel_efficiency(self, ai_responses: List[ParallelAIResponse]) -> float:
        """è¨ˆç®—ä¸¦è¡Œæ•ˆç‡"""
        try:
            if not ai_responses:
                return 1.0
            
            # è¨ˆç®—é †åºåŸ·è¡Œæ™‚é–“ï¼ˆæ‰€æœ‰AIè™•ç†æ™‚é–“ä¹‹å’Œï¼‰
            sequential_time = sum(response.processing_time for response in ai_responses)
            
            # è¨ˆç®—ä¸¦è¡ŒåŸ·è¡Œæ™‚é–“ï¼ˆæœ€é•·çš„AIè™•ç†æ™‚é–“ï¼‰
            parallel_time = max(response.processing_time for response in ai_responses)
            
            # è¨ˆç®—æ•ˆç‡æ¯”ç‡
            if parallel_time > 0:
                efficiency = sequential_time / parallel_time
                return min(efficiency, len(ai_responses))  # ç†è«–æœ€å¤§æ•ˆç‡æ˜¯AIæ•¸é‡
            else:
                return 1.0
                
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—ä¸¦è¡Œæ•ˆç‡å¤±æ•—: {e}")
            return 1.0
    
    def _extract_decision(self, response: str) -> str:
        """å¾AIå›æ‡‰ä¸­æå–æ±ºç­–"""
        response_upper = response.upper()
        
        if "BUY" in response_upper and "SELL" not in response_upper:
            return "BUY"
        elif "SELL" in response_upper and "BUY" not in response_upper:
            return "SELL"
        else:
            return "HOLD"
    
    def _calculate_consensus(self, decisions: List[str]) -> float:
        """è¨ˆç®—AIé–“çš„å…±è­˜æ°´å¹³"""
        if not decisions:
            return 0.0
        
        from collections import Counter
        decision_counts = Counter(decisions)
        most_common_count = decision_counts.most_common(1)[0][1]
        
        return most_common_count / len(decisions)
    
    def _determine_final_decision(self, decisions: List[str], confidences: List[float]) -> str:
        """ç¢ºå®šæœ€çµ‚æ±ºç­–"""
        if not decisions:
            return "HOLD"
        
        # åŠ æ¬ŠæŠ•ç¥¨
        decision_weights = {}
        for decision, confidence in zip(decisions, confidences):
            if decision not in decision_weights:
                decision_weights[decision] = 0
            decision_weights[decision] += confidence
        
        return max(decision_weights, key=decision_weights.get)
    
    def _generate_reasoning(self, ai_responses: List[ParallelAIResponse], final_decision: str) -> str:
        """ç”Ÿæˆæ±ºç­–æ¨ç†èªªæ˜"""
        reasoning_parts = []
        
        for response in ai_responses:
            if response.success:
                reasoning_parts.append(f"{response.ai_role}: {response.response[:100]}...")
        
        reasoning = f"æœ€çµ‚æ±ºç­–: {final_decision}\n\n" + "\n\n".join(reasoning_parts)
        return reasoning
    
    def _assess_risk_level(self, ai_responses: List[ParallelAIResponse], confidence: float) -> str:
        """è©•ä¼°é¢¨éšªç­‰ç´š"""
        if confidence >= 0.8:
            return "LOW"
        elif confidence >= 0.6:
            return "MEDIUM"
        else:
            return "HIGH"
    
    def _create_fallback_decision(self, error_message: str, processing_time: float) -> OptimizedDecision:
        """å‰µå»ºå‚™ç”¨æ±ºç­–"""
        return OptimizedDecision(
            final_decision="HOLD",
            confidence=0.0,
            consensus_level=0.0,
            ai_responses=[],
            reasoning=f"ç³»çµ±éŒ¯èª¤ï¼Œæ¡ç”¨ä¿å®ˆç­–ç•¥: {error_message}",
            risk_level="HIGH",
            timestamp=datetime.now(),
            total_processing_time=processing_time,
            parallel_efficiency=1.0
        )
    
    def _update_performance_stats(self, decision: OptimizedDecision, 
                                processing_time: float, parallel_execution: bool):
        """æ›´æ–°æ€§èƒ½çµ±è¨ˆ"""
        self.performance_stats["total_decisions"] += 1
        
        if decision.confidence > 0.5:
            self.performance_stats["successful_decisions"] += 1
        
        if parallel_execution:
            self.performance_stats["parallel_decisions"] += 1
            
            # æ›´æ–°å¹³å‡ä¸¦è¡Œæ•ˆç‡
            total_efficiency = (self.performance_stats["average_parallel_efficiency"] * 
                              (self.performance_stats["parallel_decisions"] - 1) + 
                              decision.parallel_efficiency)
            self.performance_stats["average_parallel_efficiency"] = total_efficiency / self.performance_stats["parallel_decisions"]
        
        # æ›´æ–°å¹³å‡è™•ç†æ™‚é–“
        total_time = (self.performance_stats["average_processing_time"] * 
                     (self.performance_stats["total_decisions"] - 1) + processing_time)
        self.performance_stats["average_processing_time"] = total_time / self.performance_stats["total_decisions"]
        
        # è¨ˆç®—é€Ÿåº¦æå‡
        if self.performance_stats["parallel_decisions"] > 0:
            parallel_avg = self.performance_stats["average_processing_time"]
            sequential_avg = parallel_avg * self.performance_stats["average_parallel_efficiency"]
            if parallel_avg > 0:
                self.performance_stats["speed_improvement"] = (sequential_avg - parallel_avg) / parallel_avg
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ç²å–æ€§èƒ½çµ±è¨ˆ"""
        return self.performance_stats.copy()
    
    def get_optimization_report(self) -> Dict[str, Any]:
        """ç²å–å„ªåŒ–å ±å‘Š"""
        stats = self.get_performance_stats()
        
        return {
            "optimization_summary": {
                "total_decisions": stats["total_decisions"],
                "parallel_decisions": stats["parallel_decisions"],
                "parallel_ratio": stats["parallel_decisions"] / max(stats["total_decisions"], 1),
                "average_processing_time": stats["average_processing_time"],
                "average_parallel_efficiency": stats["average_parallel_efficiency"],
                "speed_improvement": stats["speed_improvement"]
            },
            "performance_metrics": {
                "decisions_per_minute": 60 / max(stats["average_processing_time"], 1),
                "theoretical_max_efficiency": 3.0,  # 3å€‹AIä¸¦è¡Œ
                "actual_efficiency": stats["average_parallel_efficiency"],
                "efficiency_utilization": stats["average_parallel_efficiency"] / 3.0
            },
            "recommendations": self._generate_optimization_recommendations(stats)
        }
    
    def _generate_optimization_recommendations(self, stats: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        recommendations = []
        
        if stats["average_parallel_efficiency"] < 2.0:
            recommendations.append("è€ƒæ…®å„ªåŒ–AIæ¨¡å‹èª¿ç”¨çš„ä¸¦è¡Œåº¦")
        
        if stats["average_processing_time"] > 30:
            recommendations.append("è€ƒæ…®ä½¿ç”¨æ›´å¿«çš„AIæ¨¡å‹æˆ–æ¸›å°‘tokenæ•¸é‡")
        
        if stats["speed_improvement"] < 0.5:
            recommendations.append("ä¸¦è¡Œè™•ç†æ•ˆæœä¸æ˜é¡¯ï¼Œæª¢æŸ¥ç³»çµ±ç“¶é ¸")
        
        if not recommendations:
            recommendations.append("ç³»çµ±æ€§èƒ½è‰¯å¥½ï¼Œç¹¼çºŒä¿æŒç•¶å‰é…ç½®")
        
        return recommendations
    
    def __del__(self):
        """æ¸…ç†è³‡æº"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)


# å‰µå»ºå…¨å±€ä¸¦è¡ŒAIç®¡ç†å™¨å¯¦ä¾‹
def create_parallel_ai_manager(config_path: str = "config/ai_models_qwen7b.json") -> ParallelAIManager:
    """å‰µå»ºä¸¦è¡ŒAIç®¡ç†å™¨å¯¦ä¾‹"""
    return ParallelAIManager(config_path)


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    import asyncio
    
    async def test_parallel_ai():
        """æ¸¬è©¦ä¸¦è¡ŒAIåŠŸèƒ½"""
        print("ğŸ§ª æ¸¬è©¦ä¸¦è¡ŒAIç³»çµ±...")
        
        # å‰µå»ºä¸¦è¡ŒAIç®¡ç†å™¨
        ai_manager = create_parallel_ai_manager()
        
        # æ¨¡æ“¬å¸‚å ´æ•¸æ“š
        test_market_data = {
            "current_price": 1500000,
            "price_change_1m": 0.5,
            "price_change_5m": 1.2,
            "volume_ratio": 1.1,
            "volatility_level": "ä¸­",
            "technical_indicators": {
                "rsi": 65,
                "macd": "é‡‘å‰"
            }
        }
        
        # åŸ·è¡Œä¸¦è¡Œåˆ†æ
        start_time = time.time()
        decision = await ai_manager.analyze_market_parallel(test_market_data)
        end_time = time.time()
        
        print(f"âœ… ä¸¦è¡Œæ±ºç­–å®Œæˆ:")
        print(f"æœ€çµ‚æ±ºç­–: {decision.final_decision}")
        print(f"ä¿¡å¿ƒåº¦: {decision.confidence:.2f}")
        print(f"å…±è­˜æ°´å¹³: {decision.consensus_level:.2f}")
        print(f"é¢¨éšªç­‰ç´š: {decision.risk_level}")
        print(f"è™•ç†æ™‚é–“: {decision.total_processing_time:.2f}ç§’")
        print(f"ä¸¦è¡Œæ•ˆç‡: {decision.parallel_efficiency:.2f}x")
        
        # é¡¯ç¤ºå„ªåŒ–å ±å‘Š
        report = ai_manager.get_optimization_report()
        print(f"\nğŸ“Š å„ªåŒ–å ±å‘Š:")
        print(f"å¹³å‡è™•ç†æ™‚é–“: {report['optimization_summary']['average_processing_time']:.2f}ç§’")
        print(f"ä¸¦è¡Œæ•ˆç‡: {report['optimization_summary']['average_parallel_efficiency']:.2f}x")
        print(f"é€Ÿåº¦æå‡: {report['optimization_summary']['speed_improvement']:.1%}")
        print(f"å»ºè­°: {report['recommendations']}")
    
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(test_parallel_ai())